{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_PortfolioAllocation_NeurIPS_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVCcCalAknGn"
      },
      "source": [
        "## Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pT8a0fvhA_TW",
        "outputId": "c7abc977-bf49-4a96-e636-cbf8cbfecd87",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "Requirement already satisfied: shimmy>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-8vgytz1m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-8vgytz1m\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-doxkkjqb/elegantrl_8b49444c521c483fadb5937cff4e5448\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-doxkkjqb/elegantrl_8b49444c521c483fadb5937cff4e5448\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pandas_market_calendars\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNmvYN9YbU4B"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ntfTb0e2bU4C",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings & Backend Setup\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  \n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===========================\n",
        "# Enable Inline Plotting (Jupyter)\n",
        "# ===========================\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    RESULTS_DIR,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "# ===========================\n",
        "# Custom Library Path\n",
        "# ===========================\n",
        "sys.path.append(\"../FinRL-Library\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UwKwzRbU4l"
      },
      "source": [
        "## `process_csv_to_features(csv_path)`\n",
        "\n",
        "Processes financial data by adding technical indicators and rolling covariance matrices.\n",
        "\n",
        "### **Features**\n",
        "- Filters 5-day and 7-day tickers.\n",
        "- Applies technical indicators.\n",
        "- Computes rolling returns & covariance matrices (`lookback` window).\n",
        "- Merges features for portfolio modeling.\n",
        "\n",
        "### **Returns**\n",
        "- Processed DataFrame with indicators, returns, and covariance data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_to_features(csv_path, lookback=252):\n",
        "    # Step 1: Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    # Step 3: Filter tickers\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 4: Apply technical indicators\n",
        "    fe = FeatureEngineer(use_technical_indicator=True, use_turbulence=False, user_defined_feature=False)\n",
        "    df_5day_full = fe.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 5: Combine and clean\n",
        "    df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    df.index = range(len(df))\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df[df.groupby('date')['date'].transform('count') > 1]\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Prepare for covariance matrix computation\n",
        "    df = df.sort_values(['date', 'tic'], ignore_index=True)\n",
        "    df.index = df.date.factorize()[0]  # Re-index based on unique date\n",
        "\n",
        "    cov_list = []\n",
        "    return_list = []\n",
        "    unique_indices = df.index.unique()\n",
        "\n",
        "    for i in range(lookback, len(unique_indices)):\n",
        "        data_lookback = df.loc[i - lookback:i, :]\n",
        "        price_lookback = data_lookback.pivot_table(index='date', columns='tic', values='close')\n",
        "        return_lookback = price_lookback.pct_change().dropna()\n",
        "        return_list.append(return_lookback)\n",
        "        cov_list.append(return_lookback.cov().values)\n",
        "\n",
        "    # Step 7: Merge covariance matrix and return series back\n",
        "    df_cov = pd.DataFrame({\n",
        "        'date': df.date.unique()[lookback:], \n",
        "        'cov_list': cov_list, \n",
        "        'return_list': return_list\n",
        "    })\n",
        "    df = df.merge(df_cov, on='date')\n",
        "    df = df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing\n",
        "\n",
        "Apply `process_csv_to_features` to prepare datasets with technical indicators, returns, and covariance matrices.\n",
        "\n",
        "### **Datasets Processed**\n",
        "- `processed_0` : `2007-2025_no_crypto.csv`\n",
        "- `processed_1` : `2015-2025_crypto.csv`\n",
        "- `processed_2` : `2015-2025_no_crypto.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UooHj1OgbU4v"
      },
      "source": [
        "## `StockPortfolioEnv`\n",
        "\n",
        "A custom OpenAI Gym environment for portfolio optimization using reinforcement learning with covariance matrices and technical indicators.\n",
        "\n",
        "### **Key Features**\n",
        "- **State Space**: Combines rolling covariance matrix and technical indicators.\n",
        "- **Action Space**: Portfolio weights (normalized via softmax).\n",
        "- **Reward**: Updated portfolio value based on daily returns.\n",
        "- **Handles**: Portfolio tracking, action memory, and performance visualization.\n",
        "\n",
        "### **Core Methods**\n",
        "- `step(actions)`: Executes trading step, updates state, portfolio value, and computes reward.\n",
        "- `reset()`: Resets environment to initial state.\n",
        "- `render()`: Returns current state.\n",
        "- `save_asset_memory()`: Exports daily returns.\n",
        "- `save_action_memory()`: Exports portfolio weights (actions).\n",
        "- `get_sb_env()`: Wraps environment for Stable-Baselines3.\n",
        "\n",
        "### **Usage**\n",
        "Designed for training DRL agents on portfolio allocation tasks with risk-aware features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xlfE-VERbU40",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "\n",
        "class StockPortfolioEnv(gym.Env):\n",
        "    \"\"\"A single stock trading environment for OpenAI gym\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        df: DataFrame\n",
        "            input data\n",
        "        stock_dim : int\n",
        "            number of unique stocks\n",
        "        hmax : int\n",
        "            maximum number of shares to trade\n",
        "        initial_amount : int\n",
        "            start money\n",
        "        transaction_cost_pct: float\n",
        "            transaction cost percentage per trade\n",
        "        reward_scaling: float\n",
        "            scaling factor for reward, good for training\n",
        "        state_space: int\n",
        "            the dimension of input features\n",
        "        action_space: int\n",
        "            equals stock dimension\n",
        "        tech_indicator_list: list\n",
        "            a list of technical indicator names\n",
        "        turbulence_threshold: int\n",
        "            a threshold to control risk aversion\n",
        "        day: int\n",
        "            an increment number to control date\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    _sell_stock()\n",
        "        perform sell action based on the sign of the action\n",
        "    _buy_stock()\n",
        "        perform buy action based on the sign of the action\n",
        "    step()\n",
        "        at each step the agent will return actions, then\n",
        "        we will calculate the reward, and return the next observation.\n",
        "    reset()\n",
        "        reset the environment\n",
        "    render()\n",
        "        use render to return other functions\n",
        "    save_asset_memory()\n",
        "        return account value at each time step\n",
        "    save_action_memory()\n",
        "        return actions/positions at each time step\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self,\n",
        "                df,\n",
        "                stock_dim,\n",
        "                hmax,\n",
        "                initial_amount,\n",
        "                transaction_cost_pct,\n",
        "                reward_scaling,\n",
        "                state_space,\n",
        "                action_space,\n",
        "                tech_indicator_list,\n",
        "                turbulence_threshold=None,\n",
        "                lookback=252,\n",
        "                day = 0):\n",
        "        #super(StockEnv, self).__init__()\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.lookback=lookback\n",
        "        self.df = df\n",
        "        self.stock_dim = stock_dim\n",
        "        self.hmax = hmax\n",
        "        self.initial_amount = initial_amount\n",
        "        self.transaction_cost_pct =transaction_cost_pct\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "\n",
        "        # action_space normalization and shape is self.stock_dim\n",
        "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,))\n",
        "        # Shape = (34, 30)\n",
        "        # covariance matrix + technical indicators\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
        "\n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.terminal = False\n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
        "        self.portfolio_value = self.initial_amount\n",
        "\n",
        "        # memorize portfolio value each step\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        # memorize portfolio return each step\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "\n",
        "\n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            df = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df.columns = ['daily_return']\n",
        "            plt.plot(df.daily_return.cumsum(),'r')\n",
        "            plt.savefig('results/cumulative_reward.png')\n",
        "            plt.close()\n",
        "\n",
        "            plt.plot(self.portfolio_return_memory,'r')\n",
        "            plt.savefig('results/rewards.png')\n",
        "            plt.close()\n",
        "\n",
        "            print(\"=================================\")\n",
        "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
        "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
        "\n",
        "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df_daily_return.columns = ['daily_return']\n",
        "            if df_daily_return['daily_return'].std() !=0:\n",
        "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
        "                       df_daily_return['daily_return'].std()\n",
        "              print(\"Sharpe: \",sharpe)\n",
        "            print(\"=================================\")\n",
        "\n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            #print(\"Model actions: \",actions)\n",
        "            # actions are the portfolio weight\n",
        "            # normalize to sum of 1\n",
        "            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n",
        "            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n",
        "            #else:\n",
        "            #  norm_actions = actions\n",
        "            weights = self.softmax_normalization(actions)\n",
        "            #print(\"Normalized actions: \", weights)\n",
        "            self.actions_memory.append(weights)\n",
        "            last_day_memory = self.data\n",
        "\n",
        "            #load next state\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.covs = self.data['cov_list'].values[0]\n",
        "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "            #print(self.state)\n",
        "            # calcualte portfolio return\n",
        "            # individual stocks' return * weight\n",
        "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
        "            # update portfolio value\n",
        "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
        "            self.portfolio_value = new_portfolio_value\n",
        "\n",
        "            # save into memory\n",
        "            self.portfolio_return_memory.append(portfolio_return)\n",
        "            self.date_memory.append(self.data.date.unique()[0])\n",
        "            self.asset_memory.append(new_portfolio_value)\n",
        "\n",
        "            # the reward is the new portfolio value or end portfolo value\n",
        "            self.reward = new_portfolio_value\n",
        "            #print(\"Step reward: \", self.reward)\n",
        "            #self.reward = self.reward*self.reward_scaling\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        # load states\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.portfolio_value = self.initial_amount\n",
        "        #self.cost = 0\n",
        "        #self.trades = 0\n",
        "        self.terminal = False\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def softmax_normalization(self, actions):\n",
        "        numerator = np.exp(actions)\n",
        "        denominator = np.sum(np.exp(actions))\n",
        "        softmax_output = numerator/denominator\n",
        "        return softmax_output\n",
        "\n",
        "\n",
        "    def save_asset_memory(self):\n",
        "        date_list = self.date_memory\n",
        "        portfolio_return = self.portfolio_return_memory\n",
        "        #print(len(date_list))\n",
        "        #print(len(asset_list))\n",
        "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
        "        return df_account_value\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        # date and close price length must match actions length\n",
        "        date_list = self.date_memory\n",
        "        df_date = pd.DataFrame(date_list)\n",
        "        df_date.columns = ['date']\n",
        "\n",
        "        action_list = self.actions_memory\n",
        "        df_actions = pd.DataFrame(action_list)\n",
        "        df_actions.columns = self.data.tic.values\n",
        "        df_actions.index = df_date.date\n",
        "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
        "        return df_actions\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_sb_env(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ma6YpTlnuZ"
      },
      "source": [
        "## `train_and_predict_drl(...)`\n",
        "\n",
        "Trains, saves, reloads, and predicts using DRL models (A2C, PPO, DDPG, SAC, TD3) for portfolio management. Organizes outputs into structured folders.\n",
        "\n",
        "### **Key Features**\n",
        "- Supports five DRL models with predefined hyperparameters.\n",
        "- Automates training, saving, reloading, and prediction.\n",
        "- Saves daily returns and action history as CSV.\n",
        "- Organizes models and results into `<dataset>/<model_name>/` directories.\n",
        "\n",
        "### **Parameters**\n",
        "- `df`: Processed DataFrame.\n",
        "- `train_start_date`, `train_end_date`: Training period.\n",
        "- `trade_start_date`, `trade_end_date`: Trading period.\n",
        "- `model_name`: One of `['a2c', 'ppo', 'ddpg', 'sac', 'td3']`.\n",
        "- `output_return_csv`, `output_action_csv`: Filenames for outputs.\n",
        "- `total_timesteps`: Optional override for training steps.\n",
        "- `original_csv_path`: Dataset reference for folder structure.\n",
        "\n",
        "### **Returns**\n",
        "- `df_daily_return`: Predicted daily returns.\n",
        "- `df_actions`: Portfolio allocation actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_drl(df, \n",
        "                        train_start_date, train_end_date, \n",
        "                        trade_start_date, trade_end_date, \n",
        "                        model_name,\n",
        "                        output_return_csv,\n",
        "                        output_action_csv,\n",
        "                        total_timesteps=None,\n",
        "                        original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Train, save, reload, and predict using DRL models (A2C, PPO, DDPG, SAC, TD3).\n",
        "    Outputs are organized into subfolder: <base_csv_folder>/<model_name>\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "    from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "    from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "    import shutil\n",
        "    from finrl.main import check_and_make_directories\n",
        "    from finrl.config import (\n",
        "        RESULTS_DIR,\n",
        "        INDICATORS,\n",
        "    )\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "    # === Prepare folders ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]  # e.g., data.csv -> data\n",
        "    target_folder = os.path.join(base_name, model_name)  # e.g., data/a2c\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    # === Fixed Model Hyperparameters ===\n",
        "    model_configs = {\n",
        "        \"a2c\": {\n",
        "            \"params\": {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.005},\n",
        "            \"timesteps\": 30_000\n",
        "        },\n",
        "        \"ppo\": {\n",
        "            \"params\": {\"n_steps\": 2048, \"ent_coef\": 0.005, \"learning_rate\": 0.00001, \"batch_size\": 128},\n",
        "            \"timesteps\": 30_000\n",
        "        },\n",
        "        \"ddpg\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.005},\n",
        "            \"timesteps\": 30_000\n",
        "        },\n",
        "        \"sac\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 100000, \"learning_rate\": 0.005, \"learning_starts\": 100, \"ent_coef\": 0.1},\n",
        "            \"timesteps\": 30_000\n",
        "        },\n",
        "        \"td3\": {\n",
        "            \"params\": {\"batch_size\": 100, \"buffer_size\": 1_000_000, \"learning_rate\": 0.001},\n",
        "            \"timesteps\": 30_000\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # === Model Lookup ===\n",
        "    model_class_map = {\n",
        "        \"a2c\": A2C,\n",
        "        \"ppo\": PPO,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"sac\": SAC,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported. Choose from {list(model_configs.keys())}\")\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "    model_params = config[\"params\"]\n",
        "    timesteps = total_timesteps if total_timesteps else config[\"timesteps\"]\n",
        "\n",
        "    # === Data Split ===\n",
        "    train_data = data_split(df, train_start_date, train_end_date)\n",
        "    trade_data = data_split(df, trade_start_date, trade_end_date)\n",
        "\n",
        "    stock_dimension = len(train_data.tic.unique())\n",
        "    state_space = stock_dimension\n",
        "    print(f\"[INFO] Training {model_name.upper()} | Timesteps: {timesteps}\")\n",
        "\n",
        "    # === Environment Setup ===\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": 1_000_000,\n",
        "        \"transaction_cost_pct\": 0.001,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4\n",
        "    }\n",
        "\n",
        "    # === Train Model ===\n",
        "    e_train_gym = StockPortfolioEnv(df=train_data, **env_kwargs)\n",
        "    env_train, _ = e_train_gym.get_sb_env()\n",
        "\n",
        "    agent = DRLAgent(env=env_train)\n",
        "    model = agent.get_model(model_name=model_name, model_kwargs=model_params)\n",
        "    trained_model = agent.train_model(model=model, tb_log_name=model_name, total_timesteps=timesteps)\n",
        "\n",
        "    # === Save Model ===\n",
        "    model_save_path = os.path.join(target_folder, f\"{model_name}.zip\")\n",
        "    trained_model.save(model_save_path)\n",
        "    print(f\"[INFO] Model saved at {model_save_path}\")\n",
        "\n",
        "    # === Reload Model Automatically ===\n",
        "    ReloadedModelClass = model_class_map[model_name]\n",
        "    reloaded_model = ReloadedModelClass.load(model_save_path)\n",
        "    print(f\"[INFO] Model '{model_name.upper()}' reloaded from {model_save_path}\")\n",
        "\n",
        "    # === Predict ===\n",
        "    e_trade_gym = StockPortfolioEnv(df=trade_data, **env_kwargs)\n",
        "    df_daily_return, df_actions = DRLAgent.DRL_prediction(model=reloaded_model, environment=e_trade_gym)\n",
        "\n",
        "        # === Save Outputs into model folder ===\n",
        "    output_return_path = os.path.join(target_folder, output_return_csv)\n",
        "    output_action_path = os.path.join(target_folder, output_action_csv)\n",
        "\n",
        "    df_daily_return.to_csv(output_return_path, index=False)\n",
        "    df_actions.to_csv(output_action_path, index=False)\n",
        "\n",
        "    print(f\"[INFO] Outputs saved to {target_folder}:\")\n",
        "    print(f\" - Daily Return: {output_return_path}\")\n",
        "    print(f\" - Actions     : {output_action_path}\")\n",
        "\n",
        "    # === Move FinRL output directories if not empty ===\n",
        "    finrl_dirs = [RESULTS_DIR]\n",
        "    \n",
        "    for dir_path in finrl_dirs:\n",
        "        if os.path.exists(dir_path) and len(os.listdir(dir_path)) > 0:\n",
        "            new_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(new_path):\n",
        "                import shutil\n",
        "                shutil.rmtree(new_path)\n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[INFO] Skipped moving {dir_path} (empty or does not exist).\")\n",
        "\n",
        "    return df_daily_return, df_actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch DRL Model Training & Prediction\n",
        "\n",
        "Trains and evaluates multiple DRL models across different datasets using `train_and_predict_drl`.\n",
        "\n",
        "### **Workflow**\n",
        "- Iterates over 3 datasets and 5 DRL models (`A2C`, `PPO`, `DDPG`, `SAC`, `TD3`).\n",
        "- For each combination:\n",
        "  - Trains the model.\n",
        "  - Predicts portfolio performance.\n",
        "  - Saves daily returns and actions as CSV files in structured folders.\n",
        "\n",
        "### **Configurations**\n",
        "- **Datasets**:  \n",
        "  `2007-2025_no_crypto.csv`, `2015-2025_crypto.csv`, `2015-2025_no_crypto.csv`\n",
        "  \n",
        "- **Models**:  \n",
        "  `['a2c', 'ppo', 'ddpg', 'sac', 'td3']`\n",
        "\n",
        "- **Date Ranges**:  \n",
        "  Defined per dataset for training and trading periods.\n",
        "\n",
        "### **Outputs**\n",
        "- `<dataset>/<model_name>_daily_return.csv`\n",
        "- `<dataset>/<model_name>_actions.csv`\n",
        "- Organized model folders with results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Running A2C on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 30000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1291      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 3.1e+07   |\n",
            "|    reward             | 932084.06 |\n",
            "|    std                | 0.914     |\n",
            "|    value_loss         | 9.95e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1310      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 3.87e+07  |\n",
            "|    reward             | 1092569.5 |\n",
            "|    std                | 0.911     |\n",
            "|    value_loss         | 1.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1324      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 4.24e+07  |\n",
            "|    reward             | 1327868.9 |\n",
            "|    std                | 0.917     |\n",
            "|    value_loss         | 1.87e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1316      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 4.36e+07  |\n",
            "|    reward             | 1390585.1 |\n",
            "|    std                | 0.924     |\n",
            "|    value_loss         | 2.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1288      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 4.38e+07  |\n",
            "|    reward             | 1590158.0 |\n",
            "|    std                | 0.912     |\n",
            "|    value_loss         | 2.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1272      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 4.12e+07  |\n",
            "|    reward             | 1573568.0 |\n",
            "|    std                | 0.909     |\n",
            "|    value_loss         | 2.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1268      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 6.69e+07  |\n",
            "|    reward             | 2001264.5 |\n",
            "|    std                | 0.927     |\n",
            "|    value_loss         | 4.56e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1920393.5747758243\n",
            "Sharpe:  0.3539581281043172\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1153      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 2.01e+07  |\n",
            "|    reward             | 753421.7  |\n",
            "|    std                | 0.919     |\n",
            "|    value_loss         | 6.43e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1172      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 3.42e+07  |\n",
            "|    reward             | 1084613.6 |\n",
            "|    std                | 0.908     |\n",
            "|    value_loss         | 1.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1188      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 4.23e+07  |\n",
            "|    reward             | 1236808.9 |\n",
            "|    std                | 0.905     |\n",
            "|    value_loss         | 1.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1200      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 4.14e+07  |\n",
            "|    reward             | 1417182.1 |\n",
            "|    std                | 0.886     |\n",
            "|    value_loss         | 2.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1209      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.98     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 4.35e+07  |\n",
            "|    reward             | 1542259.5 |\n",
            "|    std                | 0.859     |\n",
            "|    value_loss         | 2.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1219      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.87     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 3.91e+07  |\n",
            "|    reward             | 1653872.9 |\n",
            "|    std                | 0.842     |\n",
            "|    value_loss         | 2.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1222      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.56     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 6.76e+07  |\n",
            "|    reward             | 2155024.2 |\n",
            "|    std                | 0.827     |\n",
            "|    value_loss         | 4.96e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2031928.4775563527\n",
            "Sharpe:  0.39474710336684526\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1191      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.49     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 2.42e+07  |\n",
            "|    reward             | 937725.44 |\n",
            "|    std                | 0.823     |\n",
            "|    value_loss         | 9.54e+12  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -9.31    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 2.66e+07 |\n",
            "|    reward             | 997766.4 |\n",
            "|    std                | 0.805    |\n",
            "|    value_loss         | 1.09e+13 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1199      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.97     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 3.05e+07  |\n",
            "|    reward             | 1238540.4 |\n",
            "|    std                | 0.776     |\n",
            "|    value_loss         | 1.67e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1204      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.63     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 3.79e+07  |\n",
            "|    reward             | 1576251.8 |\n",
            "|    std                | 0.745     |\n",
            "|    value_loss         | 2.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.29     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 4.45e+07  |\n",
            "|    reward             | 1619306.2 |\n",
            "|    std                | 0.719     |\n",
            "|    value_loss         | 2.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.37     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 4.55e+07  |\n",
            "|    reward             | 1920796.0 |\n",
            "|    std                | 0.727     |\n",
            "|    value_loss         | 3.93e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1218      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.03     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 5.6e+07   |\n",
            "|    reward             | 1933548.1 |\n",
            "|    std                | 0.705     |\n",
            "|    value_loss         | 4.18e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1214      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.83     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 4.78e+07  |\n",
            "|    reward             | 2354404.8 |\n",
            "|    std                | 0.685     |\n",
            "|    value_loss         | 5.75e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2262523.3765089205\n",
            "Sharpe:  0.41349328599481155\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1202      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.54     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 1.73e+07  |\n",
            "|    reward             | 820059.6  |\n",
            "|    std                | 0.663     |\n",
            "|    value_loss         | 6.82e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1206      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.34     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 2.27e+07  |\n",
            "|    reward             | 1206679.5 |\n",
            "|    std                | 0.646     |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.95     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 2.83e+07  |\n",
            "|    reward             | 1324648.0 |\n",
            "|    std                | 0.617     |\n",
            "|    value_loss         | 1.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.74     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 2.67e+07  |\n",
            "|    reward             | 1476798.8 |\n",
            "|    std                | 0.597     |\n",
            "|    value_loss         | 2.29e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.36     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 3.21e+07  |\n",
            "|    reward             | 1548109.1 |\n",
            "|    std                | 0.573     |\n",
            "|    value_loss         | 2.6e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.18     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 3.1e+07   |\n",
            "|    reward             | 1710829.8 |\n",
            "|    std                | 0.563     |\n",
            "|    value_loss         | 3.04e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.83     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 2.89e+07  |\n",
            "|    reward             | 2123479.0 |\n",
            "|    std                | 0.541     |\n",
            "|    value_loss         | 4.76e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1906391.5448904485\n",
            "Sharpe:  0.3493095978076631\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1209      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.49     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 1.39e+07  |\n",
            "|    reward             | 943323.8  |\n",
            "|    std                | 0.516     |\n",
            "|    value_loss         | 9.68e+12  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -5.15    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 1.22e+07 |\n",
            "|    reward             | 978731.5 |\n",
            "|    std                | 0.493    |\n",
            "|    value_loss         | 1.08e+13 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1206      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.66     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 1.35e+07  |\n",
            "|    reward             | 1180915.4 |\n",
            "|    std                | 0.468     |\n",
            "|    value_loss         | 1.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1204      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.49     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 2.56e+07  |\n",
            "|    reward             | 1379293.2 |\n",
            "|    std                | 0.463     |\n",
            "|    value_loss         | 2.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1205      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.5      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 1.73e+07  |\n",
            "|    reward             | 1488592.9 |\n",
            "|    std                | 0.466     |\n",
            "|    value_loss         | 2.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.4      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 2.04e+07  |\n",
            "|    reward             | 1638883.6 |\n",
            "|    std                | 0.465     |\n",
            "|    value_loss         | 2.85e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.23     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 2.27e+07  |\n",
            "|    reward             | 1747092.5 |\n",
            "|    std                | 0.463     |\n",
            "|    value_loss         | 3.11e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.19     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 2.3e+07   |\n",
            "|    reward             | 1895574.9 |\n",
            "|    std                | 0.469     |\n",
            "|    value_loss         | 3.9e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1936849.7586782037\n",
            "Sharpe:  0.36500043743037924\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.99     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 1.01e+07  |\n",
            "|    reward             | 848179.75 |\n",
            "|    std                | 0.459     |\n",
            "|    value_loss         | 7.62e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.92     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 1.25e+07  |\n",
            "|    reward             | 1097677.4 |\n",
            "|    std                | 0.464     |\n",
            "|    value_loss         | 1.17e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.54     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 1.62e+07  |\n",
            "|    reward             | 1264902.1 |\n",
            "|    std                | 0.442     |\n",
            "|    value_loss         | 1.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.24     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 1.51e+07  |\n",
            "|    reward             | 1362733.0 |\n",
            "|    std                | 0.436     |\n",
            "|    value_loss         | 2.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.1      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 1.84e+07  |\n",
            "|    reward             | 1567498.8 |\n",
            "|    std                | 0.436     |\n",
            "|    value_loss         | 2.66e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.77     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 1.01e+07  |\n",
            "|    reward             | 1706421.2 |\n",
            "|    std                | 0.419     |\n",
            "|    value_loss         | 3.13e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1213      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.26     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 1.56e+07  |\n",
            "|    reward             | 2197742.0 |\n",
            "|    std                | 0.394     |\n",
            "|    value_loss         | 5.16e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1988234.092290328\n",
            "Sharpe:  0.36529125336121726\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.76     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 5.13e+06  |\n",
            "|    reward             | 894716.9  |\n",
            "|    std                | 0.366     |\n",
            "|    value_loss         | 9.03e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.6      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 3.72e+06  |\n",
            "|    reward             | 1001576.3 |\n",
            "|    std                | 0.366     |\n",
            "|    value_loss         | 1.05e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1209      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.54     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 1.53e+07  |\n",
            "|    reward             | 1196608.2 |\n",
            "|    std                | 0.367     |\n",
            "|    value_loss         | 1.49e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.36     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 7.18e+06  |\n",
            "|    reward             | 1356728.6 |\n",
            "|    std                | 0.367     |\n",
            "|    value_loss         | 2.03e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1211      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.24     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 2.25e+06  |\n",
            "|    reward             | 1428661.9 |\n",
            "|    std                | 0.366     |\n",
            "|    value_loss         | 2.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1213      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.972    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 1.17e+07  |\n",
            "|    reward             | 1613190.8 |\n",
            "|    std                | 0.359     |\n",
            "|    value_loss         | 2.79e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1214      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.713    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 9.52e+05  |\n",
            "|    reward             | 1747817.2 |\n",
            "|    std                | 0.353     |\n",
            "|    value_loss         | 3.2e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.691    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 8.21e+05  |\n",
            "|    reward             | 1968780.4 |\n",
            "|    std                | 0.353     |\n",
            "|    value_loss         | 4.26e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1880634.3563307247\n",
            "Sharpe:  0.341760062506178\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.585    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 3.34e+05  |\n",
            "|    reward             | 907866.7  |\n",
            "|    std                | 0.356     |\n",
            "|    value_loss         | 8.58e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1211      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.315    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 1.6e+06   |\n",
            "|    reward             | 1062088.8 |\n",
            "|    std                | 0.35      |\n",
            "|    value_loss         | 1.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1213      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.0482    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 8.6e+06   |\n",
            "|    reward             | 1342135.8 |\n",
            "|    std                | 0.338     |\n",
            "|    value_loss         | 1.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1211      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0373   |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -2.91e+06 |\n",
            "|    reward             | 1441538.6 |\n",
            "|    std                | 0.341     |\n",
            "|    value_loss         | 2.21e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1213      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.067     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -5.8e+06  |\n",
            "|    reward             | 1674543.8 |\n",
            "|    std                | 0.347     |\n",
            "|    value_loss         | 3.02e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1214      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.318     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -6.13e+06 |\n",
            "|    reward             | 1829895.0 |\n",
            "|    std                | 0.344     |\n",
            "|    value_loss         | 3.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.323     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -3.77e+06 |\n",
            "|    reward             | 2340801.5 |\n",
            "|    std                | 0.356     |\n",
            "|    value_loss         | 5.97e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2124818.5339879077\n",
            "Sharpe:  0.3903390380296845\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1210      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.56      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -2.06e+06 |\n",
            "|    reward             | 726657.06 |\n",
            "|    std                | 0.347     |\n",
            "|    value_loss         | 5.55e+12  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2007-2025_no_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1142774.1643843683\n",
            "Sharpe:  0.6767325259729339\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/a2c:\n",
            " - Daily Return: 2007-2025_no_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 30000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2036      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 1         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1546225.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2070397.8600202666\n",
            "Sharpe:  0.3986685527707028\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1715      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 2         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.17e+14  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -4.14e-08 |\n",
            "|    reward               | 919735.25 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.47e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1684      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 4.24e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -2.02e-08 |\n",
            "|    reward               | 1589929.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 8.75e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1934120.6450815185\n",
            "Sharpe:  0.36705157033299257\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1633      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.43e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 9.77e-09  |\n",
            "|    reward               | 1060432.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.88e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1642      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.23e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -3.89e-08 |\n",
            "|    reward               | 1483128.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.8e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1788438.0550248034\n",
            "Sharpe:  0.33559259959610926\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1614      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.21e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -2.65e-08 |\n",
            "|    reward               | 1158674.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.53e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1592      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.63e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -5.15e-08 |\n",
            "|    reward               | 1785658.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.45e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1850285.724805024\n",
            "Sharpe:  0.3483030555742075\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1590      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 10        |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.92e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | 3.55e-08  |\n",
            "|    reward               | 1372086.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.76e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1562      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.67e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -3.88e-09 |\n",
            "|    reward               | 2178481.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.52e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2051390.75043969\n",
            "Sharpe:  0.39420722631007477\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1559      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.83e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | 2.94e-09  |\n",
            "|    reward               | 1234607.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.9e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1695764.4542022704\n",
            "Sharpe:  0.31003987659356574\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1553      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 14        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.01e+14  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -2.75e-08 |\n",
            "|    reward               | 743100.7  |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.07e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1557      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.35e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -1.77e-08 |\n",
            "|    reward               | 1423365.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.36e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1892065.9140876506\n",
            "Sharpe:  0.35727121086983443\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1552      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 3.58e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.22e+14  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -3.95e-08 |\n",
            "|    reward               | 1047514.3 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.3e+14   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1555      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 3.54e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -4.37e-08 |\n",
            "|    reward               | 1668691.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.32e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1890411.4121301132\n",
            "Sharpe:  0.3600373414279762\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 1547       |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 19         |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0        |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 2.7e+14    |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -2.1e-09   |\n",
            "|    reward               | 1021517.75 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 5.3e+14    |\n",
            "----------------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2007-2025_no_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1158083.4830237976\n",
            "Sharpe:  0.7565960482964126\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/ppo:\n",
            " - Daily Return: 2007-2025_no_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1815081.3189487674\n",
            "Sharpe:  0.34909152135621085\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 159       |\n",
            "|    time_elapsed    | 93        |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.17e+07 |\n",
            "|    critic_loss     | 5.59e+11  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1773023.5 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1773023.5569129412\n",
            "Sharpe:  0.3378809967674131\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 166       |\n",
            "|    time_elapsed    | 179       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.04e+08 |\n",
            "|    critic_loss     | 1.77e+12  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1773023.5 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2007-2025_no_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1117291.1899392775\n",
            "Sharpe:  0.6039277327817307\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/ddpg:\n",
            " - Daily Return: 2007-2025_no_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1823293.3332921662\n",
            "Sharpe:  0.33615318845517933\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 141       |\n",
            "|    time_elapsed    | 105       |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.03e+07 |\n",
            "|    critic_loss     | 5.61e+11  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1875628.4 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1875628.341985133\n",
            "Sharpe:  0.34768236972518474\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 140       |\n",
            "|    time_elapsed    | 212       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.05e+08 |\n",
            "|    critic_loss     | 1.55e+12  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1875628.4 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2007-2025_no_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1123329.4652446657\n",
            "Sharpe:  0.5987913828905709\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/sac:\n",
            " - Daily Return: 2007-2025_no_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1805963.108729668\n",
            "Sharpe:  0.3589450963599767\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 171       |\n",
            "|    time_elapsed    | 87        |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.12e+07 |\n",
            "|    critic_loss     | 3.44e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1957906.9 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1957906.8649698023\n",
            "Sharpe:  0.4030173819376054\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 171       |\n",
            "|    time_elapsed    | 174       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.48e+07 |\n",
            "|    critic_loss     | 6.7e+11   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1957906.9 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2007-2025_no_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1166406.9573601428\n",
            "Sharpe:  0.8536267094792879\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/td3:\n",
            " - Daily Return: 2007-2025_no_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/td3/\n",
            "\n",
            "========== Running A2C on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 30000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1383      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 7.57e+07  |\n",
            "|    reward             | 2177939.5 |\n",
            "|    std                | 0.885     |\n",
            "|    value_loss         | 4.95e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1416      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 1.02e+08  |\n",
            "|    reward             | 2396557.2 |\n",
            "|    std                | 0.868     |\n",
            "|    value_loss         | 6.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1410      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 1.04e+08  |\n",
            "|    reward             | 3824087.5 |\n",
            "|    std                | 0.838     |\n",
            "|    value_loss         | 1.58e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3281529.0205442463\n",
            "Sharpe:  1.0298098230832047\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1166      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 3.66e+07  |\n",
            "|    reward             | 1215531.8 |\n",
            "|    std                | 0.834     |\n",
            "|    value_loss         | 1.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1207      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 5.48e+07  |\n",
            "|    reward             | 1736481.1 |\n",
            "|    std                | 0.862     |\n",
            "|    value_loss         | 3.13e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1244      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 8.25e+07  |\n",
            "|    reward             | 2190022.0 |\n",
            "|    std                | 0.835     |\n",
            "|    value_loss         | 5.22e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1276      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 7.72e+07  |\n",
            "|    reward             | 2595737.5 |\n",
            "|    std                | 0.85      |\n",
            "|    value_loss         | 6.8e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2850720.7109373366\n",
            "Sharpe:  0.972461591721061\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1259      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 5.49e+07  |\n",
            "|    reward             | 1730422.8 |\n",
            "|    std                | 0.825     |\n",
            "|    value_loss         | 3.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1281      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 7.52e+07  |\n",
            "|    reward             | 2167888.5 |\n",
            "|    std                | 0.817     |\n",
            "|    value_loss         | 5.14e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1295      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 1.04e+08  |\n",
            "|    reward             | 3143307.2 |\n",
            "|    std                | 0.809     |\n",
            "|    value_loss         | 1.04e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2824036.6865427615\n",
            "Sharpe:  1.0210125663083895\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1284      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 3.44e+07  |\n",
            "|    reward             | 1130790.4 |\n",
            "|    std                | 0.807     |\n",
            "|    value_loss         | 1.28e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1296      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.82     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 3.67e+07  |\n",
            "|    reward             | 1595088.1 |\n",
            "|    std                | 0.788     |\n",
            "|    value_loss         | 2.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1307      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.28     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 4.41e+07  |\n",
            "|    reward             | 1700912.6 |\n",
            "|    std                | 0.749     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1316      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9        |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 5.58e+07  |\n",
            "|    reward             | 2249529.8 |\n",
            "|    std                | 0.727     |\n",
            "|    value_loss         | 5.53e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2259269.9534048634\n",
            "Sharpe:  0.8245499872829591\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1304      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.66     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 2.85e+07  |\n",
            "|    reward             | 1225983.9 |\n",
            "|    std                | 0.705     |\n",
            "|    value_loss         | 1.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1312      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.28     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 3.19e+07  |\n",
            "|    reward             | 1458570.1 |\n",
            "|    std                | 0.682     |\n",
            "|    value_loss         | 2.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1320      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.12     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 4.5e+07   |\n",
            "|    reward             | 2064581.0 |\n",
            "|    std                | 0.675     |\n",
            "|    value_loss         | 4.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1327      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.21     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 5.09e+07  |\n",
            "|    reward             | 1951080.6 |\n",
            "|    std                | 0.692     |\n",
            "|    value_loss         | 4.23e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1970480.979837391\n",
            "Sharpe:  0.8021179070603612\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1317      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.83     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 3.23e+07  |\n",
            "|    reward             | 1535267.9 |\n",
            "|    std                | 0.665     |\n",
            "|    value_loss         | 2.48e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1324      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.7      |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 3.64e+07  |\n",
            "|    reward             | 1628696.8 |\n",
            "|    std                | 0.659     |\n",
            "|    value_loss         | 2.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1330      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.34     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 4.15e+07  |\n",
            "|    reward             | 2281982.0 |\n",
            "|    std                | 0.639     |\n",
            "|    value_loss         | 5.54e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2017182.2816612339\n",
            "Sharpe:  0.8120504540294015\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1321      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.06     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 2.02e+07  |\n",
            "|    reward             | 1142723.0 |\n",
            "|    std                | 0.628     |\n",
            "|    value_loss         | 1.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1325      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.95     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 3.72e+07  |\n",
            "|    reward             | 1444992.4 |\n",
            "|    std                | 0.626     |\n",
            "|    value_loss         | 2.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1331      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.62     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 2.96e+07  |\n",
            "|    reward             | 1681120.6 |\n",
            "|    std                | 0.61      |\n",
            "|    value_loss         | 3.22e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.2      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 2.99e+07  |\n",
            "|    reward             | 1845661.5 |\n",
            "|    std                | 0.582     |\n",
            "|    value_loss         | 3.93e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1961264.7695858565\n",
            "Sharpe:  0.8486913332135673\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1329      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.96     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 3.01e+07  |\n",
            "|    reward             | 1278503.0 |\n",
            "|    std                | 0.575     |\n",
            "|    value_loss         | 1.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1333      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.85     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 2.59e+07  |\n",
            "|    reward             | 1585511.5 |\n",
            "|    std                | 0.572     |\n",
            "|    value_loss         | 2.67e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.64     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 3.62e+07  |\n",
            "|    reward             | 2141302.0 |\n",
            "|    std                | 0.57      |\n",
            "|    value_loss         | 4.86e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1962526.967078345\n",
            "Sharpe:  0.8399632635254476\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1331      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.58     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 1.28e+07  |\n",
            "|    reward             | 1092001.5 |\n",
            "|    std                | 0.569     |\n",
            "|    value_loss         | 1.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1332      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.33     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 2.14e+07  |\n",
            "|    reward             | 1444151.8 |\n",
            "|    std                | 0.564     |\n",
            "|    value_loss         | 2.21e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1333      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.03     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 3.09e+07  |\n",
            "|    reward             | 1498930.2 |\n",
            "|    std                | 0.544     |\n",
            "|    value_loss         | 2.36e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.02     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 3.54e+07  |\n",
            "|    reward             | 2185961.2 |\n",
            "|    std                | 0.549     |\n",
            "|    value_loss         | 5.33e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1987264.8493218925\n",
            "Sharpe:  0.8085422193751629\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1328      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.74     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 1.09e+07  |\n",
            "|    reward             | 1200445.6 |\n",
            "|    std                | 0.54      |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1332      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.45     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 1.28e+07  |\n",
            "|    reward             | 1425092.2 |\n",
            "|    std                | 0.524     |\n",
            "|    value_loss         | 2.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.11     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 1.81e+07  |\n",
            "|    reward             | 2078205.2 |\n",
            "|    std                | 0.513     |\n",
            "|    value_loss         | 4.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1339      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.13     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 2.35e+07  |\n",
            "|    reward             | 2092607.8 |\n",
            "|    std                | 0.516     |\n",
            "|    value_loss         | 4.44e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2092076.234211612\n",
            "Sharpe:  0.8557247795240062\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1334      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.23     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 2.03e+07  |\n",
            "|    reward             | 1431577.5 |\n",
            "|    std                | 0.531     |\n",
            "|    value_loss         | 2.2e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.96     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 1.81e+07  |\n",
            "|    reward             | 1617584.2 |\n",
            "|    std                | 0.513     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1339      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4        |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 2.07e+07  |\n",
            "|    reward             | 2329198.8 |\n",
            "|    std                | 0.52      |\n",
            "|    value_loss         | 5.69e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2061666.1915783666\n",
            "Sharpe:  0.9423289037000528\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1335      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.96     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 1.7e+07   |\n",
            "|    reward             | 1159296.9 |\n",
            "|    std                | 0.517     |\n",
            "|    value_loss         | 1.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1335      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.69     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 1.8e+07   |\n",
            "|    reward             | 1526626.2 |\n",
            "|    std                | 0.52      |\n",
            "|    value_loss         | 2.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.52     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 1.11e+07  |\n",
            "|    reward             | 1891006.2 |\n",
            "|    std                | 0.509     |\n",
            "|    value_loss         | 3.85e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1339      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.43     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 1.47e+07  |\n",
            "|    reward             | 2169928.0 |\n",
            "|    std                | 0.51      |\n",
            "|    value_loss         | 4.82e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2156146.2148364563\n",
            "Sharpe:  0.9878378404074762\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1334      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.34     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 1.13e+07  |\n",
            "|    reward             | 1302012.2 |\n",
            "|    std                | 0.518     |\n",
            "|    value_loss         | 1.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.08     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 2.09e+07  |\n",
            "|    reward             | 1474947.5 |\n",
            "|    std                | 0.498     |\n",
            "|    value_loss         | 2.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1340      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.89     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 1.71e+07  |\n",
            "|    reward             | 2107029.2 |\n",
            "|    std                | 0.494     |\n",
            "|    value_loss         | 4.63e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1908665.4408555937\n",
            "Sharpe:  0.7817561685697481\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.78     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 1.05e+07  |\n",
            "|    reward             | 1067681.9 |\n",
            "|    std                | 0.49      |\n",
            "|    value_loss         | 1.21e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.6      |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 1.34e+07  |\n",
            "|    reward             | 1478954.5 |\n",
            "|    std                | 0.484     |\n",
            "|    value_loss         | 2.38e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.17     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 4.94e+06  |\n",
            "|    reward             | 1446071.4 |\n",
            "|    std                | 0.464     |\n",
            "|    value_loss         | 2.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.82     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 2.52e+07  |\n",
            "|    reward             | 2110182.2 |\n",
            "|    std                | 0.456     |\n",
            "|    value_loss         | 4.82e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1968191.3894904314\n",
            "Sharpe:  0.8477930643370103\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1334      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.66     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 9.09e+06  |\n",
            "|    reward             | 1161224.6 |\n",
            "|    std                | 0.453     |\n",
            "|    value_loss         | 1.41e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1335      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.5      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 4.95e+05  |\n",
            "|    reward             | 1311884.2 |\n",
            "|    std                | 0.452     |\n",
            "|    value_loss         | 1.89e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.26     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -2.68e+06 |\n",
            "|    reward             | 1895515.1 |\n",
            "|    std                | 0.456     |\n",
            "|    value_loss         | 3.66e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1339      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.12     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 1.44e+07  |\n",
            "|    reward             | 1839508.9 |\n",
            "|    std                | 0.459     |\n",
            "|    value_loss         | 3.55e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1906920.0218193373\n",
            "Sharpe:  0.8033253702246983\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1335      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.03     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 1.53e+06  |\n",
            "|    reward             | 1397235.9 |\n",
            "|    std                | 0.458     |\n",
            "|    value_loss         | 2.03e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.508    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 3.28e+06  |\n",
            "|    reward             | 1618528.2 |\n",
            "|    std                | 0.439     |\n",
            "|    value_loss         | 2.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1339      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.364    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 1.68e+07  |\n",
            "|    reward             | 2121533.5 |\n",
            "|    std                | 0.443     |\n",
            "|    value_loss         | 4.94e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1937597.9839611605\n",
            "Sharpe:  0.793690399247535\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.255    |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -1.38e+06 |\n",
            "|    reward             | 1171002.8 |\n",
            "|    std                | 0.449     |\n",
            "|    value_loss         | 1.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1337      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0451   |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -1.25e+05 |\n",
            "|    reward             | 1468319.6 |\n",
            "|    std                | 0.451     |\n",
            "|    value_loss         | 2.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.1       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 6.01e+05  |\n",
            "|    reward             | 1701066.8 |\n",
            "|    std                | 0.45      |\n",
            "|    value_loss         | 3.03e+13  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2015-2025_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1248214.7800565446\n",
            "Sharpe:  1.0430958877693173\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/a2c:\n",
            " - Daily Return: 2015-2025_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 30000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2709488.4686300326\n",
            "Sharpe:  0.9650166075579093\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2078      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 0         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1202070.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2380969.8076629546\n",
            "Sharpe:  0.864948117762954\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1726      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 2         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 6.26e+14  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | 5.27e-09  |\n",
            "|    reward               | 2063334.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.25e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3327471.011384966\n",
            "Sharpe:  1.168197912091584\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1699      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 5.11e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -1.67e-08 |\n",
            "|    reward               | 1558843.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.1e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3013547.481279762\n",
            "Sharpe:  1.068538291870547\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1690      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.76e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 1.32e-08  |\n",
            "|    reward               | 2228195.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.61e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2940446.561932729\n",
            "Sharpe:  1.0426261557721381\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1685      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.64e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | 1.8e-09   |\n",
            "|    reward               | 2606398.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.45e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2831656.8801394654\n",
            "Sharpe:  1.0358369031484946\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1680      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.64e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -2.92e-09 |\n",
            "|    reward               | 3554247.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.49e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2936400.2688714126\n",
            "Sharpe:  1.0647765049047024\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1678      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.36e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -1.2e-08  |\n",
            "|    reward               | 2554664.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.52e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2807453.8503991514\n",
            "Sharpe:  0.9754895522343706\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2651912.5297575006\n",
            "Sharpe:  0.962256568422845\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1661      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.62e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -4.43e-09 |\n",
            "|    reward               | 1148153.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.57e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2758414.198155933\n",
            "Sharpe:  0.9902419196047172\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1653      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 6.23e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -3.5e-08  |\n",
            "|    reward               | 1463264.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.22e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2898880.303182406\n",
            "Sharpe:  1.0380991339325543\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1648      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 5.38e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | 2.46e-09  |\n",
            "|    reward               | 1783816.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.14e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3394876.574852845\n",
            "Sharpe:  1.181193366012442\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1640      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 6.3e+14   |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -1.91e-08 |\n",
            "|    reward               | 2071213.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.23e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3296856.242195236\n",
            "Sharpe:  1.1506737092432713\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1636      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 8.24e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -2.63e-08 |\n",
            "|    reward               | 2260163.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.53e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3287564.847867672\n",
            "Sharpe:  1.1487988712760488\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1631      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 16        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 7.52e+14  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -8.87e-09 |\n",
            "|    reward               | 2787603.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.53e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2370690.903295026\n",
            "Sharpe:  0.8646932756322023\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1627      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 8.34e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | 2.24e-09  |\n",
            "|    reward               | 2570025.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.54e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2453418.162300485\n",
            "Sharpe:  0.8904368257547104\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3246436.200271624\n",
            "Sharpe:  1.1382495649878277\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1617      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 6.28e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | 2.37e-10  |\n",
            "|    reward               | 1066818.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.29e+15  |\n",
            "---------------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2015-2025_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1314195.9586701724\n",
            "Sharpe:  1.1895227275524627\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/ppo:\n",
            " - Daily Return: 2015-2025_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3430773.242318663\n",
            "Sharpe:  1.0483550774824737\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 190       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.74e+07 |\n",
            "|    critic_loss     | 1.36e+12  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 3490991.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 206       |\n",
            "|    time_elapsed    | 70        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.25e+08 |\n",
            "|    critic_loss     | 3.2e+12   |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 3490991.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 208       |\n",
            "|    time_elapsed    | 103       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.59e+08 |\n",
            "|    critic_loss     | 7.19e+12  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 3490991.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3490991.7806555005\n",
            "Sharpe:  1.0590616632132415\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 203       |\n",
            "|    time_elapsed    | 142       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.78e+08 |\n",
            "|    critic_loss     | 1.01e+13  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 3490991.8 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2015-2025_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1350617.7584682156\n",
            "Sharpe:  1.1474644338619848\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/ddpg:\n",
            " - Daily Return: 2015-2025_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3782395.9624976707\n",
            "Sharpe:  1.0982870497851327\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 150       |\n",
            "|    time_elapsed    | 48        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.33e+07 |\n",
            "|    critic_loss     | 1.56e+12  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 3927702.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 143       |\n",
            "|    time_elapsed    | 100       |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.41e+08 |\n",
            "|    critic_loss     | 5.27e+12  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 3927702.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 141       |\n",
            "|    time_elapsed    | 152       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e+08 |\n",
            "|    critic_loss     | 1.08e+13  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 3927702.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3927702.142020528\n",
            "Sharpe:  1.1227333543877795\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 139       |\n",
            "|    time_elapsed    | 206       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.94e+08 |\n",
            "|    critic_loss     | 1.55e+13  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 3927702.2 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2015-2025_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1437117.2164104164\n",
            "Sharpe:  1.3418221719746661\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/sac:\n",
            " - Daily Return: 2015-2025_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3188096.7258810503\n",
            "Sharpe:  1.1277836934008627\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 204       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.68e+07 |\n",
            "|    critic_loss     | 5.92e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 3302887.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 199       |\n",
            "|    time_elapsed    | 72        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.18e+07 |\n",
            "|    critic_loss     | 1.65e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 3302887.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 196       |\n",
            "|    time_elapsed    | 110       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.09e+07 |\n",
            "|    critic_loss     | 3.17e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 3302887.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3302887.287234922\n",
            "Sharpe:  1.1566935309165638\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 193       |\n",
            "|    time_elapsed    | 149       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.16e+08 |\n",
            "|    critic_loss     | 5.81e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 3302887.2 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2015-2025_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1365373.8601921208\n",
            "Sharpe:  1.3821582707679305\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/td3:\n",
            " - Daily Return: 2015-2025_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/td3/\n",
            "\n",
            "========== Running A2C on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 30000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1336      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 4.27e+07  |\n",
            "|    reward             | 1300839.8 |\n",
            "|    std                | 0.924     |\n",
            "|    value_loss         | 1.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1114      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 3.98e+07  |\n",
            "|    reward             | 1404574.2 |\n",
            "|    std                | 0.905     |\n",
            "|    value_loss         | 2.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1145      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 5.19e+07  |\n",
            "|    reward             | 1689899.8 |\n",
            "|    std                | 0.884     |\n",
            "|    value_loss         | 3.06e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1504402.9013087163\n",
            "Sharpe:  0.5044409800192307\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1136      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.9      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 3.18e+07  |\n",
            "|    reward             | 1151527.2 |\n",
            "|    std                | 0.845     |\n",
            "|    value_loss         | 1.4e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1177      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.82     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 3.01e+07  |\n",
            "|    reward             | 1209694.0 |\n",
            "|    std                | 0.834     |\n",
            "|    value_loss         | 1.51e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1206      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.81     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 4.77e+07  |\n",
            "|    reward             | 1442828.6 |\n",
            "|    std                | 0.838     |\n",
            "|    value_loss         | 2.33e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1226      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.66     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 4.21e+07  |\n",
            "|    reward             | 1440892.9 |\n",
            "|    std                | 0.821     |\n",
            "|    value_loss         | 2.13e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1564454.933716709\n",
            "Sharpe:  0.526778637486187\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.47     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 3.29e+07  |\n",
            "|    reward             | 1213373.0 |\n",
            "|    std                | 0.803     |\n",
            "|    value_loss         | 1.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1222      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.58     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 4.69e+07  |\n",
            "|    reward             | 1302285.5 |\n",
            "|    std                | 0.82      |\n",
            "|    value_loss         | 1.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1243      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.34     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 4.95e+07  |\n",
            "|    reward             | 1714511.8 |\n",
            "|    std                | 0.797     |\n",
            "|    value_loss         | 3.15e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1492767.2660916701\n",
            "Sharpe:  0.4556298937146883\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1235      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -9.05     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 2.92e+07  |\n",
            "|    reward             | 1117430.0 |\n",
            "|    std                | 0.771     |\n",
            "|    value_loss         | 1.28e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1249      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.97     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 3.55e+07  |\n",
            "|    reward             | 1282114.1 |\n",
            "|    std                | 0.762     |\n",
            "|    value_loss         | 1.73e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1258      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.86     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 2.86e+07  |\n",
            "|    reward             | 1282593.2 |\n",
            "|    std                | 0.752     |\n",
            "|    value_loss         | 1.61e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1266      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.68     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 3.98e+07  |\n",
            "|    reward             | 1522303.1 |\n",
            "|    std                | 0.734     |\n",
            "|    value_loss         | 2.5e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1503512.7705305535\n",
            "Sharpe:  0.45816895589074363\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1252      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.39     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 2.74e+07  |\n",
            "|    reward             | 1178178.2 |\n",
            "|    std                | 0.713     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1259      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.33     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 3.34e+07  |\n",
            "|    reward             | 1293035.0 |\n",
            "|    std                | 0.711     |\n",
            "|    value_loss         | 1.78e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1263      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.32     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 3.71e+07  |\n",
            "|    reward             | 1578261.1 |\n",
            "|    std                | 0.711     |\n",
            "|    value_loss         | 2.86e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1269      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -8.09     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 3.71e+07  |\n",
            "|    reward             | 1501937.4 |\n",
            "|    std                | 0.694     |\n",
            "|    value_loss         | 2.53e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1506570.1823117186\n",
            "Sharpe:  0.46010199249969436\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1261      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.9      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 2.44e+07  |\n",
            "|    reward             | 1291978.5 |\n",
            "|    std                | 0.681     |\n",
            "|    value_loss         | 1.76e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1268      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.3      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 2.92e+07  |\n",
            "|    reward             | 1402512.5 |\n",
            "|    std                | 0.641     |\n",
            "|    value_loss         | 2.05e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1273      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -7.18     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 4.34e+07  |\n",
            "|    reward             | 1758693.8 |\n",
            "|    std                | 0.64      |\n",
            "|    value_loss         | 3.22e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1588842.554177514\n",
            "Sharpe:  0.5293781204189836\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1265      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.7      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 1.57e+07  |\n",
            "|    reward             | 1152979.2 |\n",
            "|    std                | 0.607     |\n",
            "|    value_loss         | 1.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1271      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.46     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 2.04e+07  |\n",
            "|    reward             | 1297796.9 |\n",
            "|    std                | 0.595     |\n",
            "|    value_loss         | 1.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1276      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -6.12     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 2.44e+07  |\n",
            "|    reward             | 1448792.9 |\n",
            "|    std                | 0.572     |\n",
            "|    value_loss         | 2.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1281      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.8      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 2.54e+07  |\n",
            "|    reward             | 1556371.0 |\n",
            "|    std                | 0.545     |\n",
            "|    value_loss         | 2.81e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1633185.0895399526\n",
            "Sharpe:  0.5625999140812062\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1272      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.53     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 1.53e+07  |\n",
            "|    reward             | 1209901.1 |\n",
            "|    std                | 0.528     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1277      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.29     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 3.17e+07  |\n",
            "|    reward             | 1340465.8 |\n",
            "|    std                | 0.521     |\n",
            "|    value_loss         | 1.91e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1282      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.28     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 4.05e+07  |\n",
            "|    reward             | 1720381.8 |\n",
            "|    std                | 0.524     |\n",
            "|    value_loss         | 3.15e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1586880.0296663174\n",
            "Sharpe:  0.5740534687911217\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1277      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5.18     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 2.21e+07  |\n",
            "|    reward             | 1110897.6 |\n",
            "|    std                | 0.517     |\n",
            "|    value_loss         | 1.31e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1279      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -5        |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 1.91e+07  |\n",
            "|    reward             | 1289358.0 |\n",
            "|    std                | 0.504     |\n",
            "|    value_loss         | 1.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1284      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.96     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 1.52e+07  |\n",
            "|    reward             | 1323582.5 |\n",
            "|    std                | 0.505     |\n",
            "|    value_loss         | 1.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1289      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.82     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 2.62e+07  |\n",
            "|    reward             | 1765481.5 |\n",
            "|    std                | 0.507     |\n",
            "|    value_loss         | 3.45e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1656269.9317861688\n",
            "Sharpe:  0.6154470659972537\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1281      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.98     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 1.51e+07  |\n",
            "|    reward             | 1172017.5 |\n",
            "|    std                | 0.518     |\n",
            "|    value_loss         | 1.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1285      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.48     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 2.19e+07  |\n",
            "|    reward             | 1288304.5 |\n",
            "|    std                | 0.489     |\n",
            "|    value_loss         | 1.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1288      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.49     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 2.59e+07  |\n",
            "|    reward             | 1675146.0 |\n",
            "|    std                | 0.496     |\n",
            "|    value_loss         | 3.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1292      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -4.15     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 2.11e+07  |\n",
            "|    reward             | 1677660.5 |\n",
            "|    std                | 0.476     |\n",
            "|    value_loss         | 2.91e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1659767.145696272\n",
            "Sharpe:  0.6056627132357842\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1286      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.88     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 1.4e+07   |\n",
            "|    reward             | 1265108.0 |\n",
            "|    std                | 0.461     |\n",
            "|    value_loss         | 1.7e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1287      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.63     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 1.81e+07  |\n",
            "|    reward             | 1417912.4 |\n",
            "|    std                | 0.452     |\n",
            "|    value_loss         | 2.15e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1291      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.11     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 7.9e+06   |\n",
            "|    reward             | 1892644.2 |\n",
            "|    std                | 0.422     |\n",
            "|    value_loss         | 3.77e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1703990.5273558323\n",
            "Sharpe:  0.6588706332703527\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1286      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.02     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 9.33e+06  |\n",
            "|    reward             | 1156017.5 |\n",
            "|    std                | 0.427     |\n",
            "|    value_loss         | 1.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1290      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.68     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 6.49e+06  |\n",
            "|    reward             | 1277423.5 |\n",
            "|    std                | 0.415     |\n",
            "|    value_loss         | 1.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1293      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.45     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 1.85e+07  |\n",
            "|    reward             | 1510755.9 |\n",
            "|    std                | 0.41      |\n",
            "|    value_loss         | 2.45e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1296      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.47     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 1.47e+07  |\n",
            "|    reward             | 1669263.6 |\n",
            "|    std                | 0.415     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1625853.9457606683\n",
            "Sharpe:  0.5889584508998261\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1292      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.25     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 4.64e+06  |\n",
            "|    reward             | 1218235.9 |\n",
            "|    std                | 0.4       |\n",
            "|    value_loss         | 1.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1295      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.01     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 5.8e+06   |\n",
            "|    reward             | 1301707.0 |\n",
            "|    std                | 0.391     |\n",
            "|    value_loss         | 1.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1297      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.89     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 1.31e+07  |\n",
            "|    reward             | 1733493.1 |\n",
            "|    std                | 0.395     |\n",
            "|    value_loss         | 3.1e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1625102.5327098132\n",
            "Sharpe:  0.611222088662873\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1294      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.81     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 1.3e+07   |\n",
            "|    reward             | 1070384.6 |\n",
            "|    std                | 0.394     |\n",
            "|    value_loss         | 1.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1295      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.7      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 9.71e+06  |\n",
            "|    reward             | 1252400.4 |\n",
            "|    std                | 0.397     |\n",
            "|    value_loss         | 1.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1298      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.43     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 9.15e+06  |\n",
            "|    reward             | 1259380.0 |\n",
            "|    std                | 0.384     |\n",
            "|    value_loss         | 2.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1300      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.35     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 6.29e+05  |\n",
            "|    reward             | 1685022.4 |\n",
            "|    std                | 0.381     |\n",
            "|    value_loss         | 3.03e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1600948.111808329\n",
            "Sharpe:  0.6474754334217399\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1297      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.24     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 1.06e+07  |\n",
            "|    reward             | 1119399.2 |\n",
            "|    std                | 0.383     |\n",
            "|    value_loss         | 1.33e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1299      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.08     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 5.59e+06  |\n",
            "|    reward             | 1160704.8 |\n",
            "|    std                | 0.379     |\n",
            "|    value_loss         | 1.51e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1301      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.96     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -4.48e+06 |\n",
            "|    reward             | 1588961.9 |\n",
            "|    std                | 0.378     |\n",
            "|    value_loss         | 2.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1303      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.615    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 7.34e+06  |\n",
            "|    reward             | 1573433.4 |\n",
            "|    std                | 0.372     |\n",
            "|    value_loss         | 2.6e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1588273.9668681335\n",
            "Sharpe:  0.5755545012543444\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1298      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.374    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -1.09e+06 |\n",
            "|    reward             | 1215616.0 |\n",
            "|    std                | 0.37      |\n",
            "|    value_loss         | 1.55e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1299      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.358    |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -9.68e+05 |\n",
            "|    reward             | 1347503.6 |\n",
            "|    std                | 0.372     |\n",
            "|    value_loss         | 1.95e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1299      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.255     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 4.46e+06  |\n",
            "|    reward             | 1683058.9 |\n",
            "|    std                | 0.351     |\n",
            "|    value_loss         | 3.11e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1540526.4953308718\n",
            "Sharpe:  0.5374327506223375\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1295      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.52      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -5.09e+04 |\n",
            "|    reward             | 1171477.9 |\n",
            "|    std                | 0.34      |\n",
            "|    value_loss         | 1.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1295      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.507     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -2.25e+06 |\n",
            "|    reward             | 1251669.6 |\n",
            "|    std                | 0.343     |\n",
            "|    value_loss         | 1.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1296      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | 0.716     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 2.39e+05  |\n",
            "|    reward             | 1396318.1 |\n",
            "|    std                | 0.348     |\n",
            "|    value_loss         | 2.03e+13  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2015-2025_no_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1176544.920195825\n",
            "Sharpe:  0.772289455713193\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/a2c:\n",
            " - Daily Return: 2015-2025_no_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 30000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1451557.1555071657\n",
            "Sharpe:  0.47831847082482826\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 1804      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 1         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1116176.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1386302.7555797421\n",
            "Sharpe:  0.427067380590769\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1750      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 2         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.58e+14  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -2.96e-08 |\n",
            "|    reward               | 1236026.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.83e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1489649.377171937\n",
            "Sharpe:  0.5119694911503114\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1738      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.28e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -1.99e-08 |\n",
            "|    reward               | 1115174.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.49e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1563829.6389856266\n",
            "Sharpe:  0.5609469610184531\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1720      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.33e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | 6.83e-09  |\n",
            "|    reward               | 1420521.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.9e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1659196.235178173\n",
            "Sharpe:  0.626966883739874\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1712      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.68e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -1.57e-09 |\n",
            "|    reward               | 1463642.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.26e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1441722.9069654115\n",
            "Sharpe:  0.46947640531011076\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1710      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.65e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -8.98e-09 |\n",
            "|    reward               | 1786114.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.51e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1464735.725228009\n",
            "Sharpe:  0.49062909279757316\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1713      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.63e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -3.87e-08 |\n",
            "|    reward               | 1419471.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.35e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1526993.7492775344\n",
            "Sharpe:  0.5341448930657258\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1546562.1123810506\n",
            "Sharpe:  0.5566964130141944\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1694      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.57e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -5.51e-08 |\n",
            "|    reward               | 1147384.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.38e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1466841.193346688\n",
            "Sharpe:  0.4797640725321192\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1686      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 10        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.6e+14   |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -3.17e-08 |\n",
            "|    reward               | 1197188.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5e+14     |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1467296.8045054416\n",
            "Sharpe:  0.48322597000285794\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1685      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.4e+14   |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | 1.55e-08  |\n",
            "|    reward               | 1229828.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.82e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1372501.9263348957\n",
            "Sharpe:  0.4170373338804186\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1684      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.54e+14  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -4.94e-08 |\n",
            "|    reward               | 1326623.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.96e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1539170.8820834274\n",
            "Sharpe:  0.5425052823012867\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1684      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 14        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.44e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -2.08e-08 |\n",
            "|    reward               | 1308071.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.78e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1441121.8941350577\n",
            "Sharpe:  0.46661515349961513\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1684      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.58e+14  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -5.11e-08 |\n",
            "|    reward               | 1572775.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.2e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1400991.1682488231\n",
            "Sharpe:  0.43935776524072423\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1683      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.52e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -5.31e-08 |\n",
            "|    reward               | 1738850.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.02e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1746516.3761597867\n",
            "Sharpe:  0.681525812428598\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1407757.1821093066\n",
            "Sharpe:  0.4471937573161593\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1676      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 1e-05     |\n",
            "|    loss                 | 2.76e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -2.5e-08  |\n",
            "|    reward               | 1062468.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.67e+14  |\n",
            "---------------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2015-2025_no_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1158496.6917807348\n",
            "Sharpe:  0.7580667782237382\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/ppo:\n",
            " - Daily Return: 2015-2025_no_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1439884.906246185\n",
            "Sharpe:  0.547948166236414\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 189       |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.81e+07 |\n",
            "|    critic_loss     | 9.59e+10  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1421348.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 201       |\n",
            "|    time_elapsed    | 71        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.53e+07 |\n",
            "|    critic_loss     | 6.72e+11  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1421348.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 201       |\n",
            "|    time_elapsed    | 107       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.37e+07 |\n",
            "|    critic_loss     | 9.27e+11  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1421348.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1421348.1224448953\n",
            "Sharpe:  0.5328378715105542\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 146       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.59e+07 |\n",
            "|    critic_loss     | 1.24e+12  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1421348.1 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2015-2025_no_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1130131.8805522767\n",
            "Sharpe:  0.7552313799949666\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/ddpg:\n",
            " - Daily Return: 2015-2025_no_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 30000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1521498.0016618813\n",
            "Sharpe:  0.5462882746393589\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 143       |\n",
            "|    time_elapsed    | 50        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.92e+07 |\n",
            "|    critic_loss     | 1.69e+11  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1522735.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 147       |\n",
            "|    time_elapsed    | 97        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.45e+07 |\n",
            "|    critic_loss     | 5.59e+11  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1522735.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 151       |\n",
            "|    time_elapsed    | 142       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.28e+07 |\n",
            "|    critic_loss     | 3.68e+13  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1522735.1 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1522735.1206271024\n",
            "Sharpe:  0.5488040510658132\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 156       |\n",
            "|    time_elapsed    | 184       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.2e+07  |\n",
            "|    critic_loss     | 1.9e+12   |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1522735.1 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2015-2025_no_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1166074.6723946591\n",
            "Sharpe:  0.799746967218923\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/sac:\n",
            " - Daily Return: 2015-2025_no_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1496782.1766321885\n",
            "Sharpe:  0.5997136180665643\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 192       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.01e+07 |\n",
            "|    critic_loss     | 6.22e+10  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1471957.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 182       |\n",
            "|    time_elapsed    | 79        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.84e+07 |\n",
            "|    critic_loss     | 2.34e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1471957.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 116       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.34e+07 |\n",
            "|    critic_loss     | 8.34e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1471957.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1471957.0493316897\n",
            "Sharpe:  0.5802945685050404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 185       |\n",
            "|    time_elapsed    | 155       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.51e+07 |\n",
            "|    critic_loss     | 1.08e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1471957.0 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2015-2025_no_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1165175.865887595\n",
            "Sharpe:  0.9064384028194181\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/td3:\n",
            " - Daily Return: 2015-2025_no_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/td3/\n"
          ]
        }
      ],
      "source": [
        "datasets = {\n",
        "    \"2007-2025_no_crypto.csv\": processed_0,\n",
        "    \"2015-2025_crypto.csv\": processed_1,\n",
        "    \"2015-2025_no_crypto.csv\": processed_2\n",
        "}\n",
        "\n",
        "\n",
        "models = ['a2c', 'ppo', 'ddpg', 'sac', 'td3']\n",
        "\n",
        "date_configs = {\n",
        "    \"2007-2025_no_crypto.csv\": {\n",
        "        \"train_start\": '2007-06-01',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    },\n",
        "    \"2015-2025_crypto.csv\": {\n",
        "        \"train_start\": '2015-02-02',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    },\n",
        "    \"2015-2025_no_crypto.csv\": {\n",
        "        \"train_start\": '2015-02-02',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    }\n",
        "}\n",
        "\n",
        "for csv_name, processed_df in datasets.items():\n",
        "    dates = date_configs[csv_name]\n",
        "    \n",
        "    for model_name in models:\n",
        "        print(f\"\\n========== Running {model_name.upper()} on {csv_name} ==========\")\n",
        "        \n",
        "        return_csv = f\"{model_name}_daily_return.csv\"\n",
        "        action_csv = f\"{model_name}_actions.csv\"\n",
        "        \n",
        "        df_daily_return, df_actions = train_and_predict_drl(\n",
        "            df = processed_df,\n",
        "            train_start_date = dates[\"train_start\"],\n",
        "            train_end_date = dates[\"train_end\"],\n",
        "            trade_start_date = dates[\"trade_start\"],\n",
        "            trade_end_date = dates[\"trade_end\"],\n",
        "            model_name = model_name,\n",
        "            output_return_csv = return_csv,\n",
        "            output_action_csv = action_csv,\n",
        "            original_csv_path = csv_name\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
