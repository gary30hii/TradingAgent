{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_PortfolioAllocation_NeurIPS_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv3IDvrobU37"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Portfolio Allocation\n",
        "\n",
        "Tutorials to use OpenAI DRL to perform portfolio allocation in one Jupyter Notebook | Presented at NeurIPS 2020: Deep RL Workshop\n",
        "\n",
        "* This blog is based on our paper: FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance, presented at NeurIPS 2020: Deep RL Workshop.\n",
        "* Check out medium blog for detailed explanations: https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-Foundation/FinRL/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHCfEiTA80V"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUmLTmoQA7_w"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12v1i0jVkg48"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L63HKnWvkirx"
      },
      "source": [
        "This problem is to design an automated trading solution for portfolio alloacation. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A represents the weight of a stock in the porfolio: a ∈ (-1,1). Assume our stock pool includes N stocks, we can use a list [a<sub>1</sub>, a<sub>2</sub>, ... , a<sub>N</sub>] to determine the weight for each stock in the porfotlio, where a<sub>i</sub> ∈ (-1,1), a<sub>1</sub>+ a<sub>2</sub>+...+a<sub>N</sub>=1. For example, \"The weight of AAPL in the portfolio is 10%.\" is [0.1 , ...].\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_emqQCCklVt"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVCcCalAknGn"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pT8a0fvhA_TW",
        "outputId": "c7abc977-bf49-4a96-e636-cbf8cbfecd87",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "Requirement already satisfied: shimmy>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-ne5102h8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-ne5102h8\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-gkxnx92b/elegantrl_a6be8556b7574c698dbf00d0a22d619d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-gkxnx92b/elegantrl_a6be8556b7574c698dbf00d0a22d619d\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pandas_market_calendars\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2568cp5bU38"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNmvYN9YbU4B"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "ntfTb0e2bU4C",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings & Backend Setup\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for environments without GUI\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Jupyter Notebooks (optional, safe to keep)\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl import config, config_tickers\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.plot import (\n",
        "    backtest_stats, \n",
        "    backtest_plot, \n",
        "    get_daily_return, \n",
        "    get_baseline,\n",
        "    convert_daily_return_to_pyfolio_ts\n",
        ")\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "# ===========================\n",
        "# Custom Library Path\n",
        "# ===========================\n",
        "sys.path.append(\"../FinRL-Library\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIS2abxkwan"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBria_QbU4F"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "Dx1LMUjNqqPX"
      },
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = '2015-02-02'\n",
        "TRAIN_END_DATE = '2023-04-04'\n",
        "TRADE_START_DATE = '2023-04-05'\n",
        "TRADE_END_DATE = '2025-04-10'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UwKwzRbU4l"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "\n",
        "def process_csv_to_features(csv_path, lookback=252):\n",
        "    # Step 1: Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    # Step 3: Filter tickers\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 4: Apply technical indicators\n",
        "    fe = FeatureEngineer(use_technical_indicator=True, use_turbulence=False, user_defined_feature=False)\n",
        "    df_5day_full = fe.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 5: Combine and clean\n",
        "    df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    df.index = range(len(df))\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df[df.groupby('date')['date'].transform('count') > 1]\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Prepare for covariance matrix computation\n",
        "    df = df.sort_values(['date', 'tic'], ignore_index=True)\n",
        "    df.index = df.date.factorize()[0]  # Re-index based on unique date\n",
        "\n",
        "    cov_list = []\n",
        "    return_list = []\n",
        "    unique_indices = df.index.unique()\n",
        "\n",
        "    for i in range(lookback, len(unique_indices)):\n",
        "        data_lookback = df.loc[i - lookback:i, :]\n",
        "        price_lookback = data_lookback.pivot_table(index='date', columns='tic', values='close')\n",
        "        return_lookback = price_lookback.pct_change().dropna()\n",
        "        return_list.append(return_lookback)\n",
        "        cov_list.append(return_lookback.cov().values)\n",
        "\n",
        "    # Step 7: Merge covariance matrix and return series back\n",
        "    df_cov = pd.DataFrame({\n",
        "        'date': df.date.unique()[lookback:], \n",
        "        'cov_list': cov_list, \n",
        "        'return_list': return_list\n",
        "    })\n",
        "    df = df.merge(df_cov, on='date')\n",
        "    df = df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UooHj1OgbU4v"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQnmN1qdk88I"
      },
      "source": [
        "## Training data split: 2009-01-01 to 2020-07-01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxQTNjpblAMN"
      },
      "source": [
        "## Environment for Portfolio Allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "xlfE-VERbU40",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "\n",
        "class StockPortfolioEnv(gym.Env):\n",
        "    \"\"\"A single stock trading environment for OpenAI gym\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        df: DataFrame\n",
        "            input data\n",
        "        stock_dim : int\n",
        "            number of unique stocks\n",
        "        hmax : int\n",
        "            maximum number of shares to trade\n",
        "        initial_amount : int\n",
        "            start money\n",
        "        transaction_cost_pct: float\n",
        "            transaction cost percentage per trade\n",
        "        reward_scaling: float\n",
        "            scaling factor for reward, good for training\n",
        "        state_space: int\n",
        "            the dimension of input features\n",
        "        action_space: int\n",
        "            equals stock dimension\n",
        "        tech_indicator_list: list\n",
        "            a list of technical indicator names\n",
        "        turbulence_threshold: int\n",
        "            a threshold to control risk aversion\n",
        "        day: int\n",
        "            an increment number to control date\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    _sell_stock()\n",
        "        perform sell action based on the sign of the action\n",
        "    _buy_stock()\n",
        "        perform buy action based on the sign of the action\n",
        "    step()\n",
        "        at each step the agent will return actions, then\n",
        "        we will calculate the reward, and return the next observation.\n",
        "    reset()\n",
        "        reset the environment\n",
        "    render()\n",
        "        use render to return other functions\n",
        "    save_asset_memory()\n",
        "        return account value at each time step\n",
        "    save_action_memory()\n",
        "        return actions/positions at each time step\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self,\n",
        "                df,\n",
        "                stock_dim,\n",
        "                hmax,\n",
        "                initial_amount,\n",
        "                transaction_cost_pct,\n",
        "                reward_scaling,\n",
        "                state_space,\n",
        "                action_space,\n",
        "                tech_indicator_list,\n",
        "                turbulence_threshold=None,\n",
        "                lookback=252,\n",
        "                day = 0):\n",
        "        #super(StockEnv, self).__init__()\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.lookback=lookback\n",
        "        self.df = df\n",
        "        self.stock_dim = stock_dim\n",
        "        self.hmax = hmax\n",
        "        self.initial_amount = initial_amount\n",
        "        self.transaction_cost_pct =transaction_cost_pct\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "\n",
        "        # action_space normalization and shape is self.stock_dim\n",
        "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,))\n",
        "        # Shape = (34, 30)\n",
        "        # covariance matrix + technical indicators\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
        "\n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.terminal = False\n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
        "        self.portfolio_value = self.initial_amount\n",
        "\n",
        "        # memorize portfolio value each step\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        # memorize portfolio return each step\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "\n",
        "\n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            df = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df.columns = ['daily_return']\n",
        "            plt.plot(df.daily_return.cumsum(),'r')\n",
        "            plt.savefig('results/cumulative_reward.png')\n",
        "            plt.close()\n",
        "\n",
        "            plt.plot(self.portfolio_return_memory,'r')\n",
        "            plt.savefig('results/rewards.png')\n",
        "            plt.close()\n",
        "\n",
        "            print(\"=================================\")\n",
        "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
        "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
        "\n",
        "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df_daily_return.columns = ['daily_return']\n",
        "            if df_daily_return['daily_return'].std() !=0:\n",
        "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
        "                       df_daily_return['daily_return'].std()\n",
        "              print(\"Sharpe: \",sharpe)\n",
        "            print(\"=================================\")\n",
        "\n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            #print(\"Model actions: \",actions)\n",
        "            # actions are the portfolio weight\n",
        "            # normalize to sum of 1\n",
        "            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n",
        "            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n",
        "            #else:\n",
        "            #  norm_actions = actions\n",
        "            weights = self.softmax_normalization(actions)\n",
        "            #print(\"Normalized actions: \", weights)\n",
        "            self.actions_memory.append(weights)\n",
        "            last_day_memory = self.data\n",
        "\n",
        "            #load next state\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.covs = self.data['cov_list'].values[0]\n",
        "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "            #print(self.state)\n",
        "            # calcualte portfolio return\n",
        "            # individual stocks' return * weight\n",
        "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
        "            # update portfolio value\n",
        "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
        "            self.portfolio_value = new_portfolio_value\n",
        "\n",
        "            # save into memory\n",
        "            self.portfolio_return_memory.append(portfolio_return)\n",
        "            self.date_memory.append(self.data.date.unique()[0])\n",
        "            self.asset_memory.append(new_portfolio_value)\n",
        "\n",
        "            # the reward is the new portfolio value or end portfolo value\n",
        "            self.reward = new_portfolio_value\n",
        "            #print(\"Step reward: \", self.reward)\n",
        "            #self.reward = self.reward*self.reward_scaling\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        # load states\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.portfolio_value = self.initial_amount\n",
        "        #self.cost = 0\n",
        "        #self.trades = 0\n",
        "        self.terminal = False\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def softmax_normalization(self, actions):\n",
        "        numerator = np.exp(actions)\n",
        "        denominator = np.sum(np.exp(actions))\n",
        "        softmax_output = numerator/denominator\n",
        "        return softmax_output\n",
        "\n",
        "\n",
        "    def save_asset_memory(self):\n",
        "        date_list = self.date_memory\n",
        "        portfolio_return = self.portfolio_return_memory\n",
        "        #print(len(date_list))\n",
        "        #print(len(asset_list))\n",
        "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
        "        return df_account_value\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        # date and close price length must match actions length\n",
        "        date_list = self.date_memory\n",
        "        df_date = pd.DataFrame(date_list)\n",
        "        df_date.columns = ['date']\n",
        "\n",
        "        action_list = self.actions_memory\n",
        "        df_actions = pd.DataFrame(action_list)\n",
        "        df_actions.columns = self.data.tic.values\n",
        "        df_actions.index = df_date.date\n",
        "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
        "        return df_actions\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_sb_env(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdPe8uzflbXe"
      },
      "source": [
        "### Model 1: **A2C**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvrqTro3lhAh"
      },
      "source": [
        "### Model 2: **PPO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Iuv554xYFH"
      },
      "source": [
        "### Model 3: **DDPG**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPEXBcm-uBJo"
      },
      "source": [
        "### Model 4: **SAC**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iidB5E27dfzh"
      },
      "source": [
        "### Model 5: **TD3**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ma6YpTlnuZ"
      },
      "source": [
        "## Trading\n",
        "Assume that we have $1,000,000 initial capital at 2019-01-01. We use the A2C model to trade Dow jones 30 stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_drl(df, \n",
        "                        train_start_date, train_end_date, \n",
        "                        trade_start_date, trade_end_date, \n",
        "                        model_name,\n",
        "                        output_return_csv,\n",
        "                        output_action_csv,\n",
        "                        total_timesteps=None,\n",
        "                        original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Train, save, reload, and predict using DRL models (A2C, PPO, DDPG, SAC, TD3).\n",
        "    Outputs are organized into subfolder: <base_csv_folder>/<model_name>\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "    from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "    from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "    import shutil\n",
        "    from finrl.main import check_and_make_directories\n",
        "    from finrl.config import (\n",
        "        RESULTS_DIR,\n",
        "        INDICATORS,\n",
        "    )\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "    # === Prepare folders ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]  # e.g., data.csv -> data\n",
        "    target_folder = os.path.join(base_name, model_name)  # e.g., data/a2c\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    # === Fixed Model Hyperparameters ===\n",
        "    model_configs = {\n",
        "        \"a2c\": {\n",
        "            \"params\": {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"ppo\": {\n",
        "            \"params\": {\"n_steps\": 2048, \"ent_coef\": 0.005, \"learning_rate\": 0.0001, \"batch_size\": 128},\n",
        "            \"timesteps\": 80_000\n",
        "        },\n",
        "        \"ddpg\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.001},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"sac\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 100000, \"learning_rate\": 0.0003, \"learning_starts\": 100, \"ent_coef\": \"auto_0.1\"},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"td3\": {\n",
        "            \"params\": {\"batch_size\": 100, \"buffer_size\": 1_000_000, \"learning_rate\": 0.001},\n",
        "            \"timesteps\": 30_000\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # === Model Lookup ===\n",
        "    model_class_map = {\n",
        "        \"a2c\": A2C,\n",
        "        \"ppo\": PPO,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"sac\": SAC,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported. Choose from {list(model_configs.keys())}\")\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "    model_params = config[\"params\"]\n",
        "    timesteps = total_timesteps if total_timesteps else config[\"timesteps\"]\n",
        "\n",
        "    # === Data Split ===\n",
        "    train_data = data_split(df, train_start_date, train_end_date)\n",
        "    trade_data = data_split(df, trade_start_date, trade_end_date)\n",
        "\n",
        "    stock_dimension = len(train_data.tic.unique())\n",
        "    state_space = stock_dimension\n",
        "    print(f\"[INFO] Training {model_name.upper()} | Timesteps: {timesteps}\")\n",
        "\n",
        "    # === Environment Setup ===\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": 1_000_000,\n",
        "        \"transaction_cost_pct\": 0.001,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4\n",
        "    }\n",
        "\n",
        "    # === Train Model ===\n",
        "    e_train_gym = StockPortfolioEnv(df=train_data, **env_kwargs)\n",
        "    env_train, _ = e_train_gym.get_sb_env()\n",
        "\n",
        "    agent = DRLAgent(env=env_train)\n",
        "    model = agent.get_model(model_name=model_name, model_kwargs=model_params)\n",
        "    trained_model = agent.train_model(model=model, tb_log_name=model_name, total_timesteps=timesteps)\n",
        "\n",
        "    # === Save Model ===\n",
        "    model_save_path = os.path.join(target_folder, f\"{model_name}.zip\")\n",
        "    trained_model.save(model_save_path)\n",
        "    print(f\"[INFO] Model saved at {model_save_path}\")\n",
        "\n",
        "    # === Reload Model Automatically ===\n",
        "    ReloadedModelClass = model_class_map[model_name]\n",
        "    reloaded_model = ReloadedModelClass.load(model_save_path)\n",
        "    print(f\"[INFO] Model '{model_name.upper()}' reloaded from {model_save_path}\")\n",
        "\n",
        "    # === Predict ===\n",
        "    e_trade_gym = StockPortfolioEnv(df=trade_data, **env_kwargs)\n",
        "    df_daily_return, df_actions = DRLAgent.DRL_prediction(model=reloaded_model, environment=e_trade_gym)\n",
        "\n",
        "        # === Save Outputs into model folder ===\n",
        "    output_return_path = os.path.join(target_folder, output_return_csv)\n",
        "    output_action_path = os.path.join(target_folder, output_action_csv)\n",
        "\n",
        "    df_daily_return.to_csv(output_return_path, index=False)\n",
        "    df_actions.to_csv(output_action_path, index=False)\n",
        "\n",
        "    print(f\"[INFO] Outputs saved to {target_folder}:\")\n",
        "    print(f\" - Daily Return: {output_return_path}\")\n",
        "    print(f\" - Actions     : {output_action_path}\")\n",
        "\n",
        "    # === Move FinRL output directories if not empty ===\n",
        "    finrl_dirs = [RESULTS_DIR]\n",
        "    \n",
        "    for dir_path in finrl_dirs:\n",
        "        if os.path.exists(dir_path) and len(os.listdir(dir_path)) > 0:\n",
        "            new_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(new_path):\n",
        "                import shutil\n",
        "                shutil.rmtree(new_path)\n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[INFO] Skipped moving {dir_path} (empty or does not exist).\")\n",
        "\n",
        "    return df_daily_return, df_actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Running A2C on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 50000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1438      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 3.34e+07  |\n",
            "|    reward             | 970129.75 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 4.02e+07  |\n",
            "|    reward             | 1136122.8 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1493      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 4.78e+07  |\n",
            "|    reward             | 1363229.6 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1501      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 4.46e+07  |\n",
            "|    reward             | 1411070.9 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 2.12e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1498      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 5.2e+07   |\n",
            "|    reward             | 1576244.5 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 2.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1501      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 6.08e+07  |\n",
            "|    reward             | 1620617.1 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 2.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1502      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 6.67e+07  |\n",
            "|    reward             | 1993602.9 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 4.52e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1884774.6581382533\n",
            "Sharpe:  0.3597785189979017\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1374      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 2.47e+07  |\n",
            "|    reward             | 834412.75 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 7.82e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1389      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 3.88e+07  |\n",
            "|    reward             | 1183778.6 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 1.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1401      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 4.06e+07  |\n",
            "|    reward             | 1332106.0 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 1.91e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1412      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 4.96e+07  |\n",
            "|    reward             | 1479970.8 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 2.4e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1421      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 5.19e+07  |\n",
            "|    reward             | 1546772.9 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 2.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1426      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 5.62e+07  |\n",
            "|    reward             | 1637175.0 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 2.9e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1430      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 7.35e+07  |\n",
            "|    reward             | 2101647.8 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 4.69e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1979950.8089726793\n",
            "Sharpe:  0.37795787238230627\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1415      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 3.71e+07  |\n",
            "|    reward             | 928452.94 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 9.45e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1269      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 4.12e+07  |\n",
            "|    reward             | 1059880.0 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 1.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1257      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 3.42e+07  |\n",
            "|    reward             | 1241698.6 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 1.67e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1270      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 4.63e+07  |\n",
            "|    reward             | 1469997.0 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 2.29e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1281      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 5.43e+07  |\n",
            "|    reward             | 1452147.0 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 2.25e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1292      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 5.58e+07  |\n",
            "|    reward             | 1656100.2 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 2.93e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1302      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 6.09e+07  |\n",
            "|    reward             | 1701895.4 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 3.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1311      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 6.3e+07   |\n",
            "|    reward             | 1982373.0 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 4.12e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1895928.0801020926\n",
            "Sharpe:  0.3578459357045821\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1303      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 2.24e+07  |\n",
            "|    reward             | 776275.75 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 6.17e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1311      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 3.39e+07  |\n",
            "|    reward             | 1154586.9 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 1.38e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1316      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 3.79e+07  |\n",
            "|    reward             | 1165808.6 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.4e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1322      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 4.44e+07  |\n",
            "|    reward             | 1297956.4 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.77e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1327      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 5.28e+07  |\n",
            "|    reward             | 1436696.4 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 2.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1332      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 6.13e+07  |\n",
            "|    reward             | 1601836.8 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 2.67e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 6.25e+07  |\n",
            "|    reward             | 1906541.2 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 3.83e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1876169.9993029165\n",
            "Sharpe:  0.34874083854622884\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1334      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 2.95e+07  |\n",
            "|    reward             | 936122.94 |\n",
            "|    std                | 0.97      |\n",
            "|    value_loss         | 9.58e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1338      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 3.45e+07  |\n",
            "|    reward             | 893456.56 |\n",
            "|    std                | 0.97      |\n",
            "|    value_loss         | 8.94e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1342      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 3.07e+07  |\n",
            "|    reward             | 1089202.4 |\n",
            "|    std                | 0.97      |\n",
            "|    value_loss         | 1.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1346      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 3.94e+07  |\n",
            "|    reward             | 1275874.4 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 1.79e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1351      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 4.38e+07  |\n",
            "|    reward             | 1319036.2 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 1.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1356      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 5.02e+07  |\n",
            "|    reward             | 1430376.4 |\n",
            "|    std                | 0.968     |\n",
            "|    value_loss         | 2.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1361      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 5.75e+07  |\n",
            "|    reward             | 1512775.6 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 2.33e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1364      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 5.14e+07  |\n",
            "|    reward             | 1552212.0 |\n",
            "|    std                | 0.968     |\n",
            "|    value_loss         | 2.64e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1632984.5192878551\n",
            "Sharpe:  0.29140870807364233\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1359     |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 2.75e+07 |\n",
            "|    reward             | 881554.1 |\n",
            "|    std                | 0.966    |\n",
            "|    value_loss         | 8.25e+12 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1363      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1120522.4 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 1.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1367      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 4.06e+07  |\n",
            "|    reward             | 1294977.6 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 1.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1364      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 4.42e+07  |\n",
            "|    reward             | 1358139.8 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 2.2e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1367      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 5.22e+07  |\n",
            "|    reward             | 1573915.6 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 2.68e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1369      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 5.12e+07  |\n",
            "|    reward             | 1717103.1 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 3.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1373      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 7.21e+07  |\n",
            "|    reward             | 2220116.2 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 5.27e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1955718.1034932472\n",
            "Sharpe:  0.3745489875062869\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1367      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 2.92e+07  |\n",
            "|    reward             | 889306.94 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 8.85e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1369      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 3.01e+07  |\n",
            "|    reward             | 1075199.5 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 1.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1373      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 4.15e+07  |\n",
            "|    reward             | 1274992.0 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1376      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 4.87e+07  |\n",
            "|    reward             | 1415621.8 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 2.2e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1379      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 4.89e+07  |\n",
            "|    reward             | 1482304.8 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 2.4e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1382      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 4.84e+07  |\n",
            "|    reward             | 1663859.0 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 2.99e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1385      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 6.49e+07  |\n",
            "|    reward             | 1893504.6 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 3.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1388      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 5.66e+07  |\n",
            "|    reward             | 2038851.5 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 4.55e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2006509.5430317065\n",
            "Sharpe:  0.38872139119858945\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1383     |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 3.06e+07 |\n",
            "|    reward             | 874773.2 |\n",
            "|    std                | 0.953    |\n",
            "|    value_loss         | 7.99e+12 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1386      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 3.18e+07  |\n",
            "|    reward             | 1060264.5 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 1.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1388      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 4.13e+07  |\n",
            "|    reward             | 1273949.9 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 1.78e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1391      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 3.95e+07  |\n",
            "|    reward             | 1306011.8 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1393      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 5.25e+07  |\n",
            "|    reward             | 1523821.0 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 2.51e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1395      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 5.86e+07  |\n",
            "|    reward             | 1633200.8 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 2.88e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1398      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 6.17e+07  |\n",
            "|    reward             | 2011205.5 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 4.44e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1866658.4411286125\n",
            "Sharpe:  0.3568912523562243\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1394      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 2.35e+07  |\n",
            "|    reward             | 759508.06 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 6.12e+12  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1395      |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 3.38e+07  |\n",
            "|    reward             | 1173250.9 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 1.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1397      |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 4.51e+07  |\n",
            "|    reward             | 1296795.0 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 1.85e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1400      |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 4.81e+07  |\n",
            "|    reward             | 1516118.0 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 2.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1402      |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 5.36e+07  |\n",
            "|    reward             | 1567241.0 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 2.61e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1404      |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 5.88e+07  |\n",
            "|    reward             | 1699815.6 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 3.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1406      |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 6.85e+07  |\n",
            "|    reward             | 2074456.4 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 4.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1408      |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 6.92e+07  |\n",
            "|    reward             | 1970339.8 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 3.97e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2132950.4437085846\n",
            "Sharpe:  0.41697183600820215\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1387     |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 34000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | 3.04e+07 |\n",
            "|    reward             | 975686.6 |\n",
            "|    std                | 0.945    |\n",
            "|    value_loss         | 1.01e+13 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1376      |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 3.49e+07  |\n",
            "|    reward             | 1111965.2 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 1.38e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1378      |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 3.82e+07  |\n",
            "|    reward             | 1287730.8 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 1.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1380      |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 4.54e+07  |\n",
            "|    reward             | 1296233.0 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 1.85e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1382      |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 4.56e+07  |\n",
            "|    reward             | 1514843.4 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 2.42e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1384      |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 5.78e+07  |\n",
            "|    reward             | 1686587.6 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 2.99e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1385      |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 7.15e+07  |\n",
            "|    reward             | 2107833.8 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 4.73e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1962272.565203441\n",
            "Sharpe:  0.3836491088886986\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1382     |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 2.33e+07 |\n",
            "|    reward             | 758974.3 |\n",
            "|    std                | 0.94     |\n",
            "|    value_loss         | 5.75e+12 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1382      |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 3.56e+07  |\n",
            "|    reward             | 1090767.5 |\n",
            "|    std                | 0.938     |\n",
            "|    value_loss         | 1.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1384      |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1175241.8 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 1.45e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1386      |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 4.24e+07  |\n",
            "|    reward             | 1373879.0 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 2e+13     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1387      |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 4.07e+07  |\n",
            "|    reward             | 1403158.2 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 2.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1389      |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 5.13e+07  |\n",
            "|    reward             | 1505810.5 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 2.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1391      |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 5.84e+07  |\n",
            "|    reward             | 1909681.9 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 3.78e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1392      |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 5.45e+07  |\n",
            "|    reward             | 1838747.9 |\n",
            "|    std                | 0.934     |\n",
            "|    value_loss         | 3.32e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1867776.0629731053\n",
            "Sharpe:  0.3654433960757199\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1380      |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 2.85e+07  |\n",
            "|    reward             | 1029339.0 |\n",
            "|    std                | 0.933     |\n",
            "|    value_loss         | 1.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1382      |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 3.56e+07  |\n",
            "|    reward             | 1198689.4 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1383      |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 4.08e+07  |\n",
            "|    reward             | 1364491.4 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1385      |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 4.97e+07  |\n",
            "|    reward             | 1364521.6 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1387      |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 4.65e+07  |\n",
            "|    reward             | 1601345.9 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 2.7e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1388      |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 5.88e+07  |\n",
            "|    reward             | 1787165.4 |\n",
            "|    std                | 0.933     |\n",
            "|    value_loss         | 3.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1390      |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 7.36e+07  |\n",
            "|    reward             | 2272849.2 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 5.43e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2055264.16241224\n",
            "Sharpe:  0.40423078624775677\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1387      |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 2.26e+07  |\n",
            "|    reward             | 645543.9  |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 4.38e+12  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1379       |\n",
            "|    iterations         | 9100       |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 45500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -10.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0002     |\n",
            "|    n_updates          | 9099       |\n",
            "|    policy_loss        | 3.13e+07   |\n",
            "|    reward             | 1024303.75 |\n",
            "|    std                | 0.93       |\n",
            "|    value_loss         | 1.15e+13   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1381      |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1123214.2 |\n",
            "|    std                | 0.93      |\n",
            "|    value_loss         | 1.31e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1382      |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 3.75e+07  |\n",
            "|    reward             | 1238505.4 |\n",
            "|    std                | 0.928     |\n",
            "|    value_loss         | 1.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1384      |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 3.09e+07  |\n",
            "|    reward             | 1243333.1 |\n",
            "|    std                | 0.928     |\n",
            "|    value_loss         | 1.6e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1385      |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | 3.95e+07  |\n",
            "|    reward             | 1270697.0 |\n",
            "|    std                | 0.927     |\n",
            "|    value_loss         | 1.64e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1386      |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 5.5e+07   |\n",
            "|    reward             | 1630445.9 |\n",
            "|    std                | 0.926     |\n",
            "|    value_loss         | 2.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1388      |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 5.29e+07  |\n",
            "|    reward             | 1634215.2 |\n",
            "|    std                | 0.924     |\n",
            "|    value_loss         | 2.99e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1688522.0098181476\n",
            "Sharpe:  0.3091010841048793\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1385     |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0002   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 2.74e+07 |\n",
            "|    reward             | 846108.4 |\n",
            "|    std                | 0.923    |\n",
            "|    value_loss         | 7.03e+12 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1386      |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 3.49e+07  |\n",
            "|    reward             | 1060098.0 |\n",
            "|    std                | 0.922     |\n",
            "|    value_loss         | 1.2e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1387      |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 3.03e+07  |\n",
            "|    reward             | 1155063.2 |\n",
            "|    std                | 0.921     |\n",
            "|    value_loss         | 1.41e+13  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2007-2025_no_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1167336.9848921061\n",
            "Sharpe:  0.7919223618074622\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/a2c:\n",
            " - Daily Return: 2007-2025_no_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 80000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Using cpu device\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2225      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 0         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1257413.4 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1574914.858450272\n",
            "Sharpe:  0.27744290547304395\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2001      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 2         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.6e+14   |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -3.51e-07 |\n",
            "|    reward               | 966090.75 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 3.03e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1953      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.87e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -4.14e-07 |\n",
            "|    reward               | 1701152.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.42e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2076183.391371473\n",
            "Sharpe:  0.39786796028642685\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1904      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.66e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -1.34e-07 |\n",
            "|    reward               | 1129031.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.42e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1907      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.99e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -1.26e-07 |\n",
            "|    reward               | 1545950.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.77e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2012882.3100942548\n",
            "Sharpe:  0.38530527393008923\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1877      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.5e+14   |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -3.45e-07 |\n",
            "|    reward               | 1311888.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.86e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1883      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.3e+14   |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -1.7e-07  |\n",
            "|    reward               | 1863706.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.34e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2004419.5495765316\n",
            "Sharpe:  0.38334587785528423\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1863      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.41e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -1.6e-07  |\n",
            "|    reward               | 1362952.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.87e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1865      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.78e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -1.65e-07 |\n",
            "|    reward               | 2092065.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.89e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2006891.498854972\n",
            "Sharpe:  0.38693554483883774\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1853      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 4.05e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | -1.57e-07 |\n",
            "|    reward               | 1351751.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 8.21e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1857856.8565532637\n",
            "Sharpe:  0.34888904786358005\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1843      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.37e+14  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -2.84e-07 |\n",
            "|    reward               | 711282.4  |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.52e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1840      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.63e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -1.2e-07  |\n",
            "|    reward               | 1486398.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.6e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2220077.9620831655\n",
            "Sharpe:  0.42903869692786883\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 1726       |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 15         |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0        |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 2.3e+14    |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -3.13e-07  |\n",
            "|    reward               | 1035394.56 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 4.53e+14   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1735      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 16        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 4.28e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -1.58e-07 |\n",
            "|    reward               | 1640902.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 8.97e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1955338.8479286854\n",
            "Sharpe:  0.3756567283727538\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1736      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.55e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -1.21e-07 |\n",
            "|    reward               | 1198811.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.03e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1728      |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.37e+14  |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -2.54e-07 |\n",
            "|    reward               | 1962567.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.99e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2209004.0566699267\n",
            "Sharpe:  0.42831963329837885\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1730      |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 20        |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.69e+14  |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | -1.71e-07 |\n",
            "|    reward               | 1220721.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.17e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1737      |\n",
            "|    iterations           | 18        |\n",
            "|    time_elapsed         | 21        |\n",
            "|    total_timesteps      | 36864     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.71e+14  |\n",
            "|    n_updates            | 170       |\n",
            "|    policy_gradient_loss | -1.95e-07 |\n",
            "|    reward               | 2172860.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.29e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2033829.2763655712\n",
            "Sharpe:  0.39318249237140734\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1739      |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 22        |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.35e+14  |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | -1.56e-07 |\n",
            "|    reward               | 1252799.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.71e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1748      |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 23        |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.21e+14  |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -1.75e-07 |\n",
            "|    reward               | 1623163.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.15e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1742527.9843430428\n",
            "Sharpe:  0.3216169074823645\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1750      |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 24        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.94e+14  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -3.44e-07 |\n",
            "|    reward               | 1194060.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.02e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1631555.6285412917\n",
            "Sharpe:  0.29062665785614356\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1752      |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.79e+14  |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | -3.3e-07  |\n",
            "|    reward               | 722066.75 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 3.54e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1758      |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.08e+14  |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -2.96e-07 |\n",
            "|    reward               | 1500490.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.97e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2038878.9321248392\n",
            "Sharpe:  0.3897582015436498\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1759      |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 27        |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.04e+14  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -3.38e-07 |\n",
            "|    reward               | 1011737.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.05e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1766      |\n",
            "|    iterations           | 25        |\n",
            "|    time_elapsed         | 28        |\n",
            "|    total_timesteps      | 51200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.68e+14  |\n",
            "|    n_updates            | 240       |\n",
            "|    policy_gradient_loss | -2.19e-07 |\n",
            "|    reward               | 1509169.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.64e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1995507.5287100405\n",
            "Sharpe:  0.3804146732982287\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1765      |\n",
            "|    iterations           | 26        |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 53248     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.51e+14  |\n",
            "|    n_updates            | 250       |\n",
            "|    policy_gradient_loss | -2.92e-07 |\n",
            "|    reward               | 1298118.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.96e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1767      |\n",
            "|    iterations           | 27        |\n",
            "|    time_elapsed         | 31        |\n",
            "|    total_timesteps      | 55296     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.36e+14  |\n",
            "|    n_updates            | 260       |\n",
            "|    policy_gradient_loss | -2.9e-07  |\n",
            "|    reward               | 1731894.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.97e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2165394.4288794943\n",
            "Sharpe:  0.4180617267514968\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1765      |\n",
            "|    iterations           | 28        |\n",
            "|    time_elapsed         | 32        |\n",
            "|    total_timesteps      | 57344     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.56e+14  |\n",
            "|    n_updates            | 270       |\n",
            "|    policy_gradient_loss | -1.69e-07 |\n",
            "|    reward               | 1206335.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.77e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1767      |\n",
            "|    iterations           | 29        |\n",
            "|    time_elapsed         | 33        |\n",
            "|    total_timesteps      | 59392     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.3e+14   |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -5.12e-07 |\n",
            "|    reward               | 2055267.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.61e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1790944.0988899407\n",
            "Sharpe:  0.3319796168351592\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1765      |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 34        |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.22e+14  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | -2.88e-07 |\n",
            "|    reward               | 1334634.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.46e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1768      |\n",
            "|    iterations           | 31        |\n",
            "|    time_elapsed         | 35        |\n",
            "|    total_timesteps      | 63488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.13e+14  |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | -2.32e-07 |\n",
            "|    reward               | 1783406.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.4e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1818387.887798019\n",
            "Sharpe:  0.3399645400112289\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1767      |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 37        |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.69e+14  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -1.68e-07 |\n",
            "|    reward               | 1403259.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.3e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1831078.201377353\n",
            "Sharpe:  0.34460190744495556\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1767      |\n",
            "|    iterations           | 33        |\n",
            "|    time_elapsed         | 38        |\n",
            "|    total_timesteps      | 67584     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 1.9e+14   |\n",
            "|    n_updates            | 320       |\n",
            "|    policy_gradient_loss | -2.69e-07 |\n",
            "|    reward               | 860430.25 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 3.91e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1770      |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 39        |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.32e+14  |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | -2.68e-07 |\n",
            "|    reward               | 1631833.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.19e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2116828.3170627644\n",
            "Sharpe:  0.405944794293235\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1769      |\n",
            "|    iterations           | 35        |\n",
            "|    time_elapsed         | 40        |\n",
            "|    total_timesteps      | 71680     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.34e+14  |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | -3.01e-07 |\n",
            "|    reward               | 1234753.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.71e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1772      |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 41        |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.69e+14  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | -3.29e-07 |\n",
            "|    reward               | 1671150.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 7.86e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1990716.9385067944\n",
            "Sharpe:  0.38078590473727914\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1772      |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 42        |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.82e+14  |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -2.18e-07 |\n",
            "|    reward               | 1125836.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.56e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1774      |\n",
            "|    iterations           | 38        |\n",
            "|    time_elapsed         | 43        |\n",
            "|    total_timesteps      | 77824     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 3.62e+14  |\n",
            "|    n_updates            | 370       |\n",
            "|    policy_gradient_loss | -1.92e-07 |\n",
            "|    reward               | 1630203.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.55e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1722186.2535711324\n",
            "Sharpe:  0.3143301523316945\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1774      |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 45        |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.57e+14  |\n",
            "|    n_updates            | 380       |\n",
            "|    policy_gradient_loss | -1.86e-07 |\n",
            "|    reward               | 1171665.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.27e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1776      |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 46        |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.18e+14  |\n",
            "|    n_updates            | 390       |\n",
            "|    policy_gradient_loss | -3.18e-07 |\n",
            "|    reward               | 1846627.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.62e+14  |\n",
            "---------------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2007-2025_no_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1157755.4279192332\n",
            "Sharpe:  0.7552788194477299\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/ppo:\n",
            " - Daily Return: 2007-2025_no_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1837831.9224103088\n",
            "Sharpe:  0.36476009139253773\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 227       |\n",
            "|    time_elapsed    | 65        |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.01e+07 |\n",
            "|    critic_loss     | 2.11e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1845306.9 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 227       |\n",
            "|    time_elapsed    | 131       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.05e+08 |\n",
            "|    critic_loss     | 7.18e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1845306.9 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 232       |\n",
            "|    time_elapsed    | 193       |\n",
            "|    total_timesteps | 44832     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.18e+08 |\n",
            "|    critic_loss     | 1.36e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 44731     |\n",
            "|    reward          | 1845306.9 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1845306.8402594842\n",
            "Sharpe:  0.367205672993106\n",
            "=================================\n",
            "[INFO] Model saved at 2007-2025_no_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2007-2025_no_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1184758.5575932516\n",
            "Sharpe:  0.9068936889856025\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/ddpg:\n",
            " - Daily Return: 2007-2025_no_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2024281.2179426285\n",
            "Sharpe:  0.36804517388395147\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 79        |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.01e+07 |\n",
            "|    critic_loss     | 3.04e+11  |\n",
            "|    ent_coef        | 10.8      |\n",
            "|    ent_coef_loss   | -339      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1921148.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 189       |\n",
            "|    time_elapsed    | 157       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.06e+08 |\n",
            "|    critic_loss     | 5.92e+11  |\n",
            "|    ent_coef        | 951       |\n",
            "|    ent_coef_loss   | -977      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1921148.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 191       |\n",
            "|    time_elapsed    | 234       |\n",
            "|    total_timesteps | 44832     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 5.52e+07  |\n",
            "|    critic_loss     | 1.29e+14  |\n",
            "|    ent_coef        | 8.44e+04  |\n",
            "|    ent_coef_loss   | -1.62e+03 |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 44731     |\n",
            "|    reward          | 1921148.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1921148.780897209\n",
            "Sharpe:  0.3411509972548666\n",
            "=================================\n",
            "[INFO] Model saved at 2007-2025_no_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2007-2025_no_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1172318.9118009557\n",
            "Sharpe:  0.7353922915651105\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/sac:\n",
            " - Daily Return: 2007-2025_no_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2007-2025_no_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1955535.167677673\n",
            "Sharpe:  0.36250025567885835\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 253       |\n",
            "|    time_elapsed    | 59        |\n",
            "|    total_timesteps | 14944     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.28e+07 |\n",
            "|    critic_loss     | 5.77e+13  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14843     |\n",
            "|    reward          | 1856136.4 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1856136.332816384\n",
            "Sharpe:  0.3378340660315904\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 259       |\n",
            "|    time_elapsed    | 115       |\n",
            "|    total_timesteps | 29888     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.09e+07 |\n",
            "|    critic_loss     | 1.01e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 29787     |\n",
            "|    reward          | 1856136.4 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2007-2025_no_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2007-2025_no_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1130271.3631859994\n",
            "Sharpe:  0.604337199015955\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2007-2025_no_crypto/td3:\n",
            " - Daily Return: 2007-2025_no_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2007-2025_no_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2007-2025_no_crypto/td3/\n",
            "\n",
            "========== Running A2C on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 50000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1577      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 7.53e+07  |\n",
            "|    reward             | 2012220.2 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 4.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1593      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 8.75e+07  |\n",
            "|    reward             | 2268900.2 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 5.45e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1609      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 1.26e+08  |\n",
            "|    reward             | 3515170.8 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.34e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3267503.8873045384\n",
            "Sharpe:  1.0862885143010235\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1520      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 4.84e+07  |\n",
            "|    reward             | 1206054.6 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1540      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 7.94e+07  |\n",
            "|    reward             | 1900855.4 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 3.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1556      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 9.31e+07  |\n",
            "|    reward             | 2528635.8 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 6.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1556      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 1.06e+08  |\n",
            "|    reward             | 2770410.0 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 7.86e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3126544.761209145\n",
            "Sharpe:  1.0676512086570284\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1513      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 5.52e+07  |\n",
            "|    reward             | 1544789.8 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 2.39e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1525      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 8.37e+07  |\n",
            "|    reward             | 2137333.0 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 5.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1534      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 9.83e+07  |\n",
            "|    reward             | 3043736.5 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 9.85e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2776779.200094236\n",
            "Sharpe:  1.0076129490955337\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1512      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 4.03e+07  |\n",
            "|    reward             | 1136357.4 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 1.28e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1454      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 6.34e+07  |\n",
            "|    reward             | 1687479.8 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 3.11e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1424      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 7.69e+07  |\n",
            "|    reward             | 1999145.5 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 3.99e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1424      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 1.06e+08  |\n",
            "|    reward             | 2643606.2 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 7.65e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2531171.067320284\n",
            "Sharpe:  0.9060420844099256\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1413      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 4.36e+07  |\n",
            "|    reward             | 1329694.9 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 1.88e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1419      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 7.02e+07  |\n",
            "|    reward             | 1769048.5 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 3.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1429      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 1.19e+08  |\n",
            "|    reward             | 3162950.0 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 1.13e+14  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1433      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.17e+08  |\n",
            "|    reward             | 2955682.2 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 9.79e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3032033.3850970212\n",
            "Sharpe:  1.035306053701724\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1417      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 7.84e+07  |\n",
            "|    reward             | 2216275.0 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 4.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1428      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 7.54e+07  |\n",
            "|    reward             | 2084658.0 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 4.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1438      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 1.19e+08  |\n",
            "|    reward             | 3646497.2 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 1.45e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3096986.0357830552\n",
            "Sharpe:  1.051118979733623\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1431      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 4.3e+07   |\n",
            "|    reward             | 1173277.9 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1432      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 8.08e+07  |\n",
            "|    reward             | 1827269.0 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 3.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1438      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 8.56e+07  |\n",
            "|    reward             | 2316329.0 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 6.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1428      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 1.08e+08  |\n",
            "|    reward             | 2859093.5 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 9.42e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3100613.517900151\n",
            "Sharpe:  1.04888842050782\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1408      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 5.85e+07  |\n",
            "|    reward             | 1526136.2 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 2.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1409      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 8.81e+07  |\n",
            "|    reward             | 2351176.0 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 5.9e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1415      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 1.32e+08  |\n",
            "|    reward             | 3396080.0 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.24e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2871105.9345259\n",
            "Sharpe:  0.9655161953987017\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1411      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 4.27e+07  |\n",
            "|    reward             | 1106926.1 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 1.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1416      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 6.89e+07  |\n",
            "|    reward             | 1802207.6 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 3.36e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1423      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 7.6e+07   |\n",
            "|    reward             | 1827481.0 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 3.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1429      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 1.16e+08  |\n",
            "|    reward             | 2974227.2 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 9.97e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2821122.1029711207\n",
            "Sharpe:  0.9942151819921611\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1424      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 4.15e+07  |\n",
            "|    reward             | 1237294.0 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 1.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1428      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 6.03e+07  |\n",
            "|    reward             | 1704687.0 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 3e+13     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1434      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 9.5e+07   |\n",
            "|    reward             | 2649948.2 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 7.92e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1440      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 1.01e+08  |\n",
            "|    reward             | 2826325.5 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 7.87e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2913785.553032189\n",
            "Sharpe:  1.0011738994037882\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1435      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 6.87e+07  |\n",
            "|    reward             | 1890219.4 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 3.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1440      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 8.93e+07  |\n",
            "|    reward             | 2334844.0 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 5.9e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 1.28e+08  |\n",
            "|    reward             | 3894214.2 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 1.56e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3233194.5053934418\n",
            "Sharpe:  1.0609391266748285\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1441      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 5.01e+07  |\n",
            "|    reward             | 1196326.1 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1443      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 7.15e+07  |\n",
            "|    reward             | 1973408.6 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 4.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1443      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 1.01e+08  |\n",
            "|    reward             | 2700283.5 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 7.87e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 1.27e+08  |\n",
            "|    reward             | 3573565.5 |\n",
            "|    std                | 0.97      |\n",
            "|    value_loss         | 1.28e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3356649.014974209\n",
            "Sharpe:  1.125666434168777\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1441      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 4.58e+07  |\n",
            "|    reward             | 1465806.1 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 2.17e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 6.92e+07  |\n",
            "|    reward             | 1919219.4 |\n",
            "|    std                | 0.968     |\n",
            "|    value_loss         | 3.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 1.11e+08  |\n",
            "|    reward             | 3002857.2 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 9.46e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2614503.9919619258\n",
            "Sharpe:  0.9109764800221595\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1443      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 4.56e+07  |\n",
            "|    reward             | 1087281.0 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 1.25e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1445      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 6.73e+07  |\n",
            "|    reward             | 2069943.1 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 4.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1449      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 9.72e+07  |\n",
            "|    reward             | 1968592.0 |\n",
            "|    std                | 0.965     |\n",
            "|    value_loss         | 5.76e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1451      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 1.1e+08   |\n",
            "|    reward             | 3486247.2 |\n",
            "|    std                | 0.964     |\n",
            "|    value_loss         | 1.35e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3244127.194812682\n",
            "Sharpe:  1.1020668259916964\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 4.55e+07  |\n",
            "|    reward             | 1287421.1 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 1.68e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1449      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 7.32e+07  |\n",
            "|    reward             | 1748418.0 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 3.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1453      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 9.6e+07   |\n",
            "|    reward             | 2654996.0 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 7.01e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1455      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 9.11e+07  |\n",
            "|    reward             | 2789089.2 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 8.12e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3074225.3343837955\n",
            "Sharpe:  1.0430807662817054\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1451      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 5.96e+07  |\n",
            "|    reward             | 1731860.9 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 3.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1454      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 7.5e+07   |\n",
            "|    reward             | 2274851.8 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 5.59e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1456      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 1.37e+08  |\n",
            "|    reward             | 3809516.2 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 1.61e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3488483.090301143\n",
            "Sharpe:  1.1707913025642747\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1454      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 4.36e+07  |\n",
            "|    reward             | 1175292.9 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 1.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1456      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 7.02e+07  |\n",
            "|    reward             | 1855158.2 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 3.59e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1460      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 7.28e+07  |\n",
            "|    reward             | 1979439.2 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 4.11e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1462      |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 6.75e+07  |\n",
            "|    reward             | 2109738.5 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 4.86e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2240300.6878833077\n",
            "Sharpe:  0.7864070452840832\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1459      |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 5.44e+07  |\n",
            "|    reward             | 1356985.2 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 1.9e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1461      |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 6.78e+07  |\n",
            "|    reward             | 1838913.4 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 3.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1464      |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 1e+08     |\n",
            "|    reward             | 2977821.0 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 9.78e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2585704.8189583425\n",
            "Sharpe:  0.8978992671223153\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1460       |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0002     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | 3.75e+07   |\n",
            "|    reward             | 1044061.94 |\n",
            "|    std                | 0.958      |\n",
            "|    value_loss         | 1.08e+13   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1461      |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 6.71e+07  |\n",
            "|    reward             | 1798901.9 |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 3.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1464      |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 8.07e+07  |\n",
            "|    reward             | 2079459.5 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 4.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1466      |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 9.84e+07  |\n",
            "|    reward             | 3033156.5 |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 9.76e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2640214.9958602036\n",
            "Sharpe:  0.9305907001693337\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1463      |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 3.95e+07  |\n",
            "|    reward             | 1196655.8 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 1.49e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1465      |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 6.34e+07  |\n",
            "|    reward             | 1713461.4 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 3.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1467      |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 1.04e+08  |\n",
            "|    reward             | 2744527.5 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 7.39e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1469      |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 1.01e+08  |\n",
            "|    reward             | 2765671.2 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 8.14e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3221355.0567034907\n",
            "Sharpe:  1.110403179917337\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1466      |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 6.25e+07  |\n",
            "|    reward             | 1748348.5 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 3.04e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1468      |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 8.6e+07   |\n",
            "|    reward             | 2426845.8 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 6.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1469      |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 1.24e+08  |\n",
            "|    reward             | 3916401.2 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 1.61e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3276341.299761192\n",
            "Sharpe:  1.0956394059558028\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1467      |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 4.29e+07  |\n",
            "|    reward             | 1216760.6 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 1.48e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1468      |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 8.09e+07  |\n",
            "|    reward             | 2127460.8 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 5.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1470      |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 8.08e+07  |\n",
            "|    reward             | 2480691.0 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 6.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1471      |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 1.27e+08  |\n",
            "|    reward             | 3502025.5 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 1.24e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3520004.7144270497\n",
            "Sharpe:  1.11638996405091\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1468      |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 5.56e+07  |\n",
            "|    reward             | 1360821.2 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 2.02e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1470      |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 6.74e+07  |\n",
            "|    reward             | 1922140.6 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 3.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1471      |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 1.27e+08  |\n",
            "|    reward             | 3617122.0 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 1.36e+14  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 1.1e+08   |\n",
            "|    reward             | 3193970.2 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 1.08e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3310458.379120348\n",
            "Sharpe:  1.0798126323814734\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1470      |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 7.31e+07  |\n",
            "|    reward             | 2353836.8 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 5.76e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 7.74e+07  |\n",
            "|    reward             | 2504226.8 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 6.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 1.43e+08  |\n",
            "|    reward             | 3757967.8 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 1.47e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3044209.9917908697\n",
            "Sharpe:  1.0141046830679321\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1470      |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 4.52e+07  |\n",
            "|    reward             | 1265563.8 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 1.68e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1471      |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 6.96e+07  |\n",
            "|    reward             | 2094801.6 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 5.01e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 1.09e+08  |\n",
            "|    reward             | 2955124.2 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 8.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1475      |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 1.14e+08  |\n",
            "|    reward             | 3249053.0 |\n",
            "|    std                | 0.948     |\n",
            "|    value_loss         | 1.07e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3668515.776710035\n",
            "Sharpe:  1.1431805140449787\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 6.44e+07  |\n",
            "|    reward             | 1738934.0 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 3.04e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 9.06e+07  |\n",
            "|    reward             | 2508045.8 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 6.84e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1474      |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 1.23e+08  |\n",
            "|    reward             | 3692967.2 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 1.51e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3584069.801666855\n",
            "Sharpe:  1.1369720927509384\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1124650.0 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 1.35e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | 6.34e+07  |\n",
            "|    reward             | 1965225.5 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 4.04e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1475      |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 8.33e+07  |\n",
            "|    reward             | 2096149.5 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 4.41e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1476      |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 1.05e+08  |\n",
            "|    reward             | 3493374.8 |\n",
            "|    std                | 0.943     |\n",
            "|    value_loss         | 1.38e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3356166.121064393\n",
            "Sharpe:  1.0745253706038227\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 4.84e+07  |\n",
            "|    reward             | 1378317.4 |\n",
            "|    std                | 0.943     |\n",
            "|    value_loss         | 1.95e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1475      |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 6.42e+07  |\n",
            "|    reward             | 1898665.2 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 3.73e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1476      |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 1.2e+08   |\n",
            "|    reward             | 3781614.2 |\n",
            "|    std                | 0.94      |\n",
            "|    value_loss         | 1.4e+14   |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2015-2025_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1370468.9494411012\n",
            "Sharpe:  1.272868674130309\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/a2c:\n",
            " - Daily Return: 2015-2025_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 80000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2875072.553966235\n",
            "Sharpe:  1.0166521496440548\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2355      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 0         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1214547.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2795768.2394662667\n",
            "Sharpe:  0.9886618349968181\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2111      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 1         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.52e+14  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -5.3e-08  |\n",
            "|    reward               | 2131961.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.33e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2969292.9856416495\n",
            "Sharpe:  1.0515309632827377\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2030      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.8e+14   |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -2.8e-07  |\n",
            "|    reward               | 1652284.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.29e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3040424.6152697806\n",
            "Sharpe:  1.0738061665959986\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1997      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.29e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -1.82e-07 |\n",
            "|    reward               | 1826075.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.45e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2305235.051611421\n",
            "Sharpe:  0.8240995690855167\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1991      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.14e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -2.97e-07 |\n",
            "|    reward               | 2550091.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.4e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2767251.5322714453\n",
            "Sharpe:  0.9750486973055829\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1995      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.37e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -1.6e-07  |\n",
            "|    reward               | 2680006.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.1e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2228754.13893641\n",
            "Sharpe:  0.7683627975470201\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1992      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.61e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -3.5e-07  |\n",
            "|    reward               | 2943517.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.24e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3309734.0621700087\n",
            "Sharpe:  1.1323531114732865\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2557936.693766574\n",
            "Sharpe:  0.9019749844373975\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1970      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.43e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -3.82e-07 |\n",
            "|    reward               | 1199240.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.57e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2973368.5724314526\n",
            "Sharpe:  1.0587692648304756\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1961      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.23e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -2.17e-07 |\n",
            "|    reward               | 1614741.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.22e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2801088.17628731\n",
            "Sharpe:  0.9859937329300269\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1958      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 10        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.96e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 1872545.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.27e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2936968.9328508237\n",
            "Sharpe:  1.046016233067968\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1960      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.92e+14  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -2.38e-07 |\n",
            "|    reward               | 1934406.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.34e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2798960.2195443003\n",
            "Sharpe:  0.999829987562814\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1961      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.78e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -2.53e-07 |\n",
            "|    reward               | 2177858.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.4e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2946970.3746092236\n",
            "Sharpe:  1.0475504497039765\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1960      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.22e+14  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -2.05e-07 |\n",
            "|    reward               | 2762540.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.29e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2718771.2280256725\n",
            "Sharpe:  0.9473104855957915\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1963      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 14        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.81e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -1.57e-07 |\n",
            "|    reward               | 2486306.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.4e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2506926.731231406\n",
            "Sharpe:  0.897687884930417\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2418433.879447117\n",
            "Sharpe:  0.8527776015856733\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1954      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.78e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -9.17e-08 |\n",
            "|    reward               | 1063541.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.2e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2450097.6961286445\n",
            "Sharpe:  0.8853109806799434\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1954      |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 16        |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.52e+14  |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -2.45e-07 |\n",
            "|    reward               | 1324349.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.09e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3263350.526816046\n",
            "Sharpe:  1.1512495745890754\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1952      |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5e+14     |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | -1.67e-07 |\n",
            "|    reward               | 1852389.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.03e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2963248.6818973655\n",
            "Sharpe:  1.0567809330388904\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1950      |\n",
            "|    iterations           | 18        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 36864     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.25e+14  |\n",
            "|    n_updates            | 170       |\n",
            "|    policy_gradient_loss | -1.76e-07 |\n",
            "|    reward               | 1624196.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.57e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2906437.424978247\n",
            "Sharpe:  1.038281349685288\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1949      |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.48e+14  |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | -3.31e-07 |\n",
            "|    reward               | 2201013.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.23e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2916420.3312614765\n",
            "Sharpe:  1.0356246385568129\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1951      |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 20        |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.4e+14   |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -3.54e-07 |\n",
            "|    reward               | 2648391.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.29e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2611053.386036287\n",
            "Sharpe:  0.9190693872936482\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1954      |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 22        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.82e+14  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -2.67e-07 |\n",
            "|    reward               | 3228724.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.44e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2932567.972667319\n",
            "Sharpe:  1.0596568383366962\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1953      |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 23        |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.55e+14  |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | -1.83e-07 |\n",
            "|    reward               | 2523715.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.38e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2762552.6297909296\n",
            "Sharpe:  0.9811507563947687\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367785.336179423\n",
            "Sharpe:  1.160540103201743\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1948      |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 24        |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.73e+14  |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -2.35e-07 |\n",
            "|    reward               | 1139793.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.49e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2305822.806141743\n",
            "Sharpe:  0.8310645321036713\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1889      |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.41e+14  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -2.13e-07 |\n",
            "|    reward               | 1675418.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.6e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2833641.3572067977\n",
            "Sharpe:  1.011070965226833\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1894      |\n",
            "|    iterations           | 25        |\n",
            "|    time_elapsed         | 27        |\n",
            "|    total_timesteps      | 51200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 4.83e+14  |\n",
            "|    n_updates            | 240       |\n",
            "|    policy_gradient_loss | -3.11e-07 |\n",
            "|    reward               | 1752864.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 9.38e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2775999.3935200735\n",
            "Sharpe:  1.0122701156554246\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1899      |\n",
            "|    iterations           | 26        |\n",
            "|    time_elapsed         | 28        |\n",
            "|    total_timesteps      | 53248     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.29e+14  |\n",
            "|    n_updates            | 250       |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 2231420.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.28e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2884870.505159859\n",
            "Sharpe:  1.023561545843296\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1904      |\n",
            "|    iterations           | 27        |\n",
            "|    time_elapsed         | 29        |\n",
            "|    total_timesteps      | 55296     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.75e+14  |\n",
            "|    n_updates            | 260       |\n",
            "|    policy_gradient_loss | -1.62e-07 |\n",
            "|    reward               | 2356463.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.34e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2539614.9077387047\n",
            "Sharpe:  0.9151096917815259\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1905      |\n",
            "|    iterations           | 28        |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 57344     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.65e+14  |\n",
            "|    n_updates            | 270       |\n",
            "|    policy_gradient_loss | -1.57e-07 |\n",
            "|    reward               | 3044851.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.36e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2519059.8529920336\n",
            "Sharpe:  0.8957874518335993\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1909      |\n",
            "|    iterations           | 29        |\n",
            "|    time_elapsed         | 31        |\n",
            "|    total_timesteps      | 59392     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.54e+14  |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -2.18e-07 |\n",
            "|    reward               | 2922448.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.32e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2946364.833442803\n",
            "Sharpe:  1.0451052407688892\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2745670.038575084\n",
            "Sharpe:  0.9942932936237788\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1909      |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 32        |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.96e+14  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | -3.45e-07 |\n",
            "|    reward               | 1093305.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.56e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2565091.7025951794\n",
            "Sharpe:  0.9186653636921166\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1911      |\n",
            "|    iterations           | 31        |\n",
            "|    time_elapsed         | 33        |\n",
            "|    total_timesteps      | 63488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.01e+14  |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 1425932.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.27e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2384044.604765881\n",
            "Sharpe:  0.8453427671080846\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1914      |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 34        |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.59e+14  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -1.61e-07 |\n",
            "|    reward               | 1969578.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.11e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2889374.4949695785\n",
            "Sharpe:  1.0048999975291097\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1916      |\n",
            "|    iterations           | 33        |\n",
            "|    time_elapsed         | 35        |\n",
            "|    total_timesteps      | 67584     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.57e+14  |\n",
            "|    n_updates            | 320       |\n",
            "|    policy_gradient_loss | -1.83e-07 |\n",
            "|    reward               | 1869677.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.12e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2690219.3538743802\n",
            "Sharpe:  0.9499212864509438\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1918      |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 36        |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.77e+14  |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | -1.63e-07 |\n",
            "|    reward               | 1963846.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.29e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3268939.7668868853\n",
            "Sharpe:  1.1383360434215983\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1921      |\n",
            "|    iterations           | 35        |\n",
            "|    time_elapsed         | 37        |\n",
            "|    total_timesteps      | 71680     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6e+14     |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | -3.06e-07 |\n",
            "|    reward               | 3661477.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.27e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3174827.2139613396\n",
            "Sharpe:  1.109819800968518\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1923      |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 38        |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.74e+14  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | -1.42e-07 |\n",
            "|    reward               | 3381902.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.69e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3071215.178303153\n",
            "Sharpe:  1.0805765939192524\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1925      |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 39        |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.33e+14  |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -1.02e-07 |\n",
            "|    reward               | 2628771.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.85e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2676797.057365816\n",
            "Sharpe:  0.9651820491892844\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3010597.579450159\n",
            "Sharpe:  1.0601606121447156\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1924      |\n",
            "|    iterations           | 38        |\n",
            "|    time_elapsed         | 40        |\n",
            "|    total_timesteps      | 77824     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.35e+14  |\n",
            "|    n_updates            | 370       |\n",
            "|    policy_gradient_loss | -1.47e-07 |\n",
            "|    reward               | 1198838.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.31e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2928398.0936455363\n",
            "Sharpe:  1.02829815174504\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1926      |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 41        |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.12e+14  |\n",
            "|    n_updates            | 380       |\n",
            "|    policy_gradient_loss | -2.42e-07 |\n",
            "|    reward               | 1682059.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.53e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2751531.14515766\n",
            "Sharpe:  0.9861448429578029\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1927      |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 42        |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.09e+14  |\n",
            "|    n_updates            | 390       |\n",
            "|    policy_gradient_loss | -2.54e-07 |\n",
            "|    reward               | 1647687.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.39e+15  |\n",
            "---------------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2015-2025_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1312290.9044146438\n",
            "Sharpe:  1.1878718357405682\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/ppo:\n",
            " - Daily Return: 2015-2025_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3265061.4625894385\n",
            "Sharpe:  1.0825350876265198\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 253       |\n",
            "|    time_elapsed    | 28        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.42e+07 |\n",
            "|    critic_loss     | 1.37e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 251       |\n",
            "|    time_elapsed    | 57        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.21e+08 |\n",
            "|    critic_loss     | 3.08e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 252       |\n",
            "|    time_elapsed    | 85        |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+08 |\n",
            "|    critic_loss     | 6.86e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 250       |\n",
            "|    time_elapsed    | 115       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.79e+08 |\n",
            "|    critic_loss     | 5.7e+12   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 246       |\n",
            "|    time_elapsed    | 146       |\n",
            "|    total_timesteps | 36100     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.98e+08 |\n",
            "|    critic_loss     | 8.02e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 35999     |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 179       |\n",
            "|    total_timesteps | 43320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.1e+08  |\n",
            "|    critic_loss     | 8.36e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 43219     |\n",
            "|    reward          | 3367412.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3367412.2545056967\n",
            "Sharpe:  1.1054376783640176\n",
            "=================================\n",
            "[INFO] Model saved at 2015-2025_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2015-2025_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1374122.1138656172\n",
            "Sharpe:  1.3136710019616056\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/ddpg:\n",
            " - Daily Return: 2015-2025_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2435150.9037873643\n",
            "Sharpe:  0.874154894916313\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 189       |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.61e+07 |\n",
            "|    critic_loss     | 9.43e+11  |\n",
            "|    ent_coef        | 0.892     |\n",
            "|    ent_coef_loss   | 12.2      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 77        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.49e+07 |\n",
            "|    critic_loss     | 1.01e+12  |\n",
            "|    ent_coef        | 7.78      |\n",
            "|    ent_coef_loss   | -220      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 187       |\n",
            "|    time_elapsed    | 115       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.22e+08 |\n",
            "|    critic_loss     | 1.74e+12  |\n",
            "|    ent_coef        | 67.9      |\n",
            "|    ent_coef_loss   | -452      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 155       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.36e+08 |\n",
            "|    critic_loss     | 1.3e+12   |\n",
            "|    ent_coef        | 592       |\n",
            "|    ent_coef_loss   | -683      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 185       |\n",
            "|    time_elapsed    | 194       |\n",
            "|    total_timesteps | 36100     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.41e+08 |\n",
            "|    critic_loss     | 2.08e+12  |\n",
            "|    ent_coef        | 5.16e+03  |\n",
            "|    ent_coef_loss   | -918      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 35999     |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 187       |\n",
            "|    time_elapsed    | 231       |\n",
            "|    total_timesteps | 43320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.15e+07 |\n",
            "|    critic_loss     | 1.3e+12   |\n",
            "|    ent_coef        | 4.51e+04  |\n",
            "|    ent_coef_loss   | -1.15e+03 |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 43219     |\n",
            "|    reward          | 2408025.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2408025.6997794867\n",
            "Sharpe:  0.8634895155067689\n",
            "=================================\n",
            "[INFO] Model saved at 2015-2025_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2015-2025_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1279008.5935868348\n",
            "Sharpe:  1.0450581529993896\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/sac:\n",
            " - Daily Return: 2015-2025_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2015-2025_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2158675.1187709766\n",
            "Sharpe:  0.8543328670972176\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 254       |\n",
            "|    time_elapsed    | 28        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.76e+07 |\n",
            "|    critic_loss     | 2.49e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 2168569.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 252       |\n",
            "|    time_elapsed    | 57        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.23e+07 |\n",
            "|    critic_loss     | 7.06e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 2168569.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 249       |\n",
            "|    time_elapsed    | 86        |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.08e+07 |\n",
            "|    critic_loss     | 9.74e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 2168569.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2168569.037159014\n",
            "Sharpe:  0.8602464112198726\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 246       |\n",
            "|    time_elapsed    | 117       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.4e+07  |\n",
            "|    critic_loss     | 3.93e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 2168569.0 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2015-2025_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1258098.33104312\n",
            "Sharpe:  1.0748767947071112\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_crypto/td3:\n",
            " - Daily Return: 2015-2025_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2015-2025_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2015-2025_crypto/td3/\n",
            "\n",
            "========== Running A2C on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training A2C | Timesteps: 50000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1536      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 4.05e+07  |\n",
            "|    reward             | 1303021.2 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1581      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 4.53e+07  |\n",
            "|    reward             | 1419536.8 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 2.12e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1567      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 5.41e+07  |\n",
            "|    reward             | 1658450.4 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 2.95e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1469628.3259780058\n",
            "Sharpe:  0.5014469586561573\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1450      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 3.33e+07  |\n",
            "|    reward             | 1114169.1 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 4.01e+07  |\n",
            "|    reward             | 1211864.9 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1489      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 4.15e+07  |\n",
            "|    reward             | 1368069.8 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 2.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1504      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 4.71e+07  |\n",
            "|    reward             | 1341750.9 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 1.86e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1452759.0895557525\n",
            "Sharpe:  0.4794614567342636\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 3.62e+07  |\n",
            "|    reward             | 1181723.1 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1486      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 3.83e+07  |\n",
            "|    reward             | 1262881.1 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 1.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1496      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 5.01e+07  |\n",
            "|    reward             | 1558376.9 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 2.61e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1458565.7035027854\n",
            "Sharpe:  0.47698784004161987\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1477      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1089858.4 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 1.22e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1485      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 4.01e+07  |\n",
            "|    reward             | 1207194.1 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 1.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1494      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 3.8e+07   |\n",
            "|    reward             | 1232435.8 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1503      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 4.36e+07  |\n",
            "|    reward             | 1465848.6 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 2.32e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1451967.011981343\n",
            "Sharpe:  0.47216558586360924\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1485      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 3.74e+07  |\n",
            "|    reward             | 1156107.5 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1492      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 4.2e+07   |\n",
            "|    reward             | 1264374.0 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1496      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 5.46e+07  |\n",
            "|    reward             | 1496050.4 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1504      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 4.3e+07   |\n",
            "|    reward             | 1473617.6 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.42e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1496217.2613240054\n",
            "Sharpe:  0.5032465188044195\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1491      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 4.45e+07  |\n",
            "|    reward             | 1300232.1 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 1.78e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1498      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 5.05e+07  |\n",
            "|    reward             | 1452834.0 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 2.21e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1503      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 5.8e+07   |\n",
            "|    reward             | 1841153.4 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 3.51e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1613962.9129692733\n",
            "Sharpe:  0.5940180420583437\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1486      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 3.76e+07  |\n",
            "|    reward             | 1148909.1 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 1.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1486      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 3.64e+07  |\n",
            "|    reward             | 1270250.6 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1490      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 5.22e+07  |\n",
            "|    reward             | 1434237.4 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1488      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 4.52e+07  |\n",
            "|    reward             | 1457651.6 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 2.43e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1533227.4607818162\n",
            "Sharpe:  0.5270120491341527\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1475      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 3.88e+07  |\n",
            "|    reward             | 1207387.0 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1478      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 3.86e+07  |\n",
            "|    reward             | 1292513.2 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 1.77e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1482      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 5.5e+07   |\n",
            "|    reward             | 1641485.9 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 2.87e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1444165.1946044578\n",
            "Sharpe:  0.45367609537533604\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 3.39e+07  |\n",
            "|    reward             | 1102429.1 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 1.3e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1474      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 4.22e+07  |\n",
            "|    reward             | 1236411.5 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.66e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1477      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 3.51e+07  |\n",
            "|    reward             | 1209305.0 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 1.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1480      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 5.08e+07  |\n",
            "|    reward             | 1654335.1 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 3.03e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1526209.880752881\n",
            "Sharpe:  0.523292164509605\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1470      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 3.99e+07  |\n",
            "|    reward             | 1172721.6 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 1.45e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1472      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 4.19e+07  |\n",
            "|    reward             | 1226882.1 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 1.55e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1474      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 5.13e+07  |\n",
            "|    reward             | 1531019.2 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 2.57e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1476      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 5.69e+07  |\n",
            "|    reward             | 1620150.9 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 2.71e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1588062.2167912887\n",
            "Sharpe:  0.5627778000034787\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1467      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 4.09e+07  |\n",
            "|    reward             | 1204374.5 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 1.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1471      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 4.31e+07  |\n",
            "|    reward             | 1300676.1 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1473      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 5.59e+07  |\n",
            "|    reward             | 1642372.9 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1446443.354754906\n",
            "Sharpe:  0.46695050172082436\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1465      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 3.14e+07  |\n",
            "|    reward             | 1152315.9 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 1.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1465      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 3.98e+07  |\n",
            "|    reward             | 1286228.2 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.73e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1466      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 4.92e+07  |\n",
            "|    reward             | 1447731.6 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 2.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1467      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 5.36e+07  |\n",
            "|    reward             | 1602604.0 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 2.61e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1496985.3833257281\n",
            "Sharpe:  0.5021265840862448\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1460      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 3.93e+07  |\n",
            "|    reward             | 1220376.0 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.57e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1462      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 4.13e+07  |\n",
            "|    reward             | 1296805.5 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.76e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1464      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 5.47e+07  |\n",
            "|    reward             | 1703533.4 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 2.99e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1493125.108804128\n",
            "Sharpe:  0.49209015885990887\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1458      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 3.44e+07  |\n",
            "|    reward             | 1074823.8 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.22e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1459      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 4.03e+07  |\n",
            "|    reward             | 1260051.0 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.72e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1461      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 4.45e+07  |\n",
            "|    reward             | 1183722.8 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.86e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1462      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 5.12e+07  |\n",
            "|    reward             | 1540226.1 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 2.63e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1443823.9852265061\n",
            "Sharpe:  0.4599209315514033\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1456      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 4.02e+07  |\n",
            "|    reward             | 1122224.5 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 1.35e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1457      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 3.81e+07  |\n",
            "|    reward             | 1193233.6 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 1.61e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1457      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 5.24e+07  |\n",
            "|    reward             | 1573217.1 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 2.6e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1456      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 5.59e+07  |\n",
            "|    reward             | 1538074.6 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 2.5e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1540666.2140947122\n",
            "Sharpe:  0.5284540813244689\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 3.73e+07  |\n",
            "|    reward             | 1240299.1 |\n",
            "|    std                | 0.965     |\n",
            "|    value_loss         | 1.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 5.6e+07   |\n",
            "|    reward             | 1350919.9 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 5.08e+07  |\n",
            "|    reward             | 1692372.6 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 3.1e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1525269.519431728\n",
            "Sharpe:  0.5191559916650783\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 3.48e+07  |\n",
            "|    reward             | 1164017.2 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 1.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1445      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 4.02e+07  |\n",
            "|    reward             | 1309008.0 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 1.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 4.19e+07  |\n",
            "|    reward             | 1377898.8 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 1.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 4.8e+07   |\n",
            "|    reward             | 1474310.2 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 2.35e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1509118.374496189\n",
            "Sharpe:  0.5086422951461113\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1442      |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 3.68e+07  |\n",
            "|    reward             | 1191240.9 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1443      |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 3.55e+07  |\n",
            "|    reward             | 1321283.0 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 1.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 5.51e+07  |\n",
            "|    reward             | 1565927.2 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 2.56e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1477674.5453000518\n",
            "Sharpe:  0.48870669835409314\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1443      |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 3.09e+07  |\n",
            "|    reward             | 1019821.1 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 4.58e+07  |\n",
            "|    reward             | 1247085.6 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 4.78e+07  |\n",
            "|    reward             | 1415996.5 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 2.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 5.54e+07  |\n",
            "|    reward             | 1651781.5 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 2.93e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1501029.0013046751\n",
            "Sharpe:  0.5118306736566929\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 4.06e+07  |\n",
            "|    reward             | 1122501.0 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.31e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1445      |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 4.28e+07  |\n",
            "|    reward             | 1227459.2 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 1.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 4.73e+07  |\n",
            "|    reward             | 1463004.5 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 2.17e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 4.81e+07  |\n",
            "|    reward             | 1441997.1 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 2.18e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1535711.8032562374\n",
            "Sharpe:  0.5488722204224554\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1445      |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 3.69e+07  |\n",
            "|    reward             | 1199789.5 |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 4.04e+07  |\n",
            "|    reward             | 1297944.8 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 1.78e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 4.96e+07  |\n",
            "|    reward             | 1720480.8 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 3.12e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1538424.6913241744\n",
            "Sharpe:  0.5411485079281942\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1444      |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 3.42e+07  |\n",
            "|    reward             | 1111634.5 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 1.35e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 4.23e+07  |\n",
            "|    reward             | 1301207.1 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 4.37e+07  |\n",
            "|    reward             | 1360848.6 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 1.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1449      |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 4.51e+07  |\n",
            "|    reward             | 1560163.2 |\n",
            "|    std                | 0.948     |\n",
            "|    value_loss         | 2.4e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1504264.7032742412\n",
            "Sharpe:  0.5126431005003806\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 3.64e+07  |\n",
            "|    reward             | 1183509.5 |\n",
            "|    std                | 0.948     |\n",
            "|    value_loss         | 1.48e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 4.79e+07  |\n",
            "|    reward             | 1303598.2 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 1.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1449      |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 5.63e+07  |\n",
            "|    reward             | 1623885.1 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 2.77e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1450      |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 4.82e+07  |\n",
            "|    reward             | 1483807.1 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 2.44e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1516166.7419321164\n",
            "Sharpe:  0.5219716540098833\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 3.79e+07  |\n",
            "|    reward             | 1274506.4 |\n",
            "|    std                | 0.943     |\n",
            "|    value_loss         | 1.68e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 4.13e+07  |\n",
            "|    reward             | 1368641.1 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 1.98e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1449      |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 6.5e+07   |\n",
            "|    reward             | 1734063.2 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 3.16e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1547200.7387881621\n",
            "Sharpe:  0.5453676703347081\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 3.42e+07  |\n",
            "|    reward             | 1127776.5 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 1.36e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 4.34e+07  |\n",
            "|    reward             | 1187192.6 |\n",
            "|    std                | 0.94      |\n",
            "|    value_loss         | 1.59e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1450      |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 4.74e+07  |\n",
            "|    reward             | 1398951.9 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1450      |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 3.54e+07  |\n",
            "|    reward             | 1314898.4 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 1.77e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1399517.1869269917\n",
            "Sharpe:  0.43886470808438177\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 3.42e+07  |\n",
            "|    reward             | 1218470.8 |\n",
            "|    std                | 0.938     |\n",
            "|    value_loss         | 1.57e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 5.12e+07  |\n",
            "|    reward             | 1386834.9 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 2.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 5.68e+07  |\n",
            "|    reward             | 1815530.2 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 3.55e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1598210.6175245007\n",
            "Sharpe:  0.582823565229125\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1445      |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 3.76e+07  |\n",
            "|    reward             | 1086858.4 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 1.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | 3.62e+07  |\n",
            "|    reward             | 1267386.9 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 1.7e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 4.62e+07  |\n",
            "|    reward             | 1275723.8 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 1.72e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 5.37e+07  |\n",
            "|    reward             | 1631691.4 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 3.04e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1568258.4097600025\n",
            "Sharpe:  0.562274114811794\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1446      |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 3.38e+07  |\n",
            "|    reward             | 1190663.2 |\n",
            "|    std                | 0.934     |\n",
            "|    value_loss         | 1.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1447      |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 3.62e+07  |\n",
            "|    reward             | 1259248.2 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 1.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1448      |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 5.67e+07  |\n",
            "|    reward             | 1647143.4 |\n",
            "|    std                | 0.934     |\n",
            "|    value_loss         | 2.73e+13  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/a2c/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from 2015-2025_no_crypto/a2c/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1138254.9074253433\n",
            "Sharpe:  0.6205371525898747\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/a2c:\n",
            " - Daily Return: 2015-2025_no_crypto/a2c/a2c_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/a2c/a2c_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/a2c/\n",
            "\n",
            "========== Running PPO on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training PPO | Timesteps: 80000\n",
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1567250.1709803683\n",
            "Sharpe:  0.5625775984202155\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2345      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 0         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1131543.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1437173.9091772656\n",
            "Sharpe:  0.47308717522487354\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2108      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 1         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.5e+14   |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -2.12e-07 |\n",
            "|    reward               | 1245612.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.02e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1578172.1659293424\n",
            "Sharpe:  0.5721563066214467\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2049      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 2         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.22e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -3.68e-07 |\n",
            "|    reward               | 1083948.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.78e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1366448.2837526721\n",
            "Sharpe:  0.41357785489866156\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2022      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.58e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -1.12e-07 |\n",
            "|    reward               | 1401209.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.01e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1525548.914282861\n",
            "Sharpe:  0.5264754342756837\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2005      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.29e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -1.88e-07 |\n",
            "|    reward               | 1479989.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.7e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1508935.2103312763\n",
            "Sharpe:  0.5232826768014175\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1991      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.65e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -1.3e-07  |\n",
            "|    reward               | 1595643.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.21e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1437426.9875432986\n",
            "Sharpe:  0.46227204874814154\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1969      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.48e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -1.21e-07 |\n",
            "|    reward               | 1367180.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.1e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1467450.7655095796\n",
            "Sharpe:  0.48951295795320315\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1485652.7350537647\n",
            "Sharpe:  0.5041271598748508\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1943      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.56e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -4.39e-07 |\n",
            "|    reward               | 1171394.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.04e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1612869.2157240203\n",
            "Sharpe:  0.6023358404490832\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1935      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.44e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -2.61e-07 |\n",
            "|    reward               | 1173782.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.81e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1462994.7613506452\n",
            "Sharpe:  0.48533420034819535\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1932      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 10        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.76e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | -3.05e-07 |\n",
            "|    reward               | 1233549.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.53e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1498357.486765727\n",
            "Sharpe:  0.5205450962398595\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1936      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.4e+14   |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -2.93e-07 |\n",
            "|    reward               | 1301958.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.69e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1575413.4544238434\n",
            "Sharpe:  0.5696515989356629\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1919      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.45e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -3.07e-07 |\n",
            "|    reward               | 1237118.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.88e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1412457.722068203\n",
            "Sharpe:  0.4442257342030456\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1924      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.5e+14   |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -1.56e-07 |\n",
            "|    reward               | 1584802.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.05e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1451481.2784570246\n",
            "Sharpe:  0.4811198486238322\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1925      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 14        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.4e+14   |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -5.05e-07 |\n",
            "|    reward               | 1585944.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.9e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1519022.4357061617\n",
            "Sharpe:  0.522916749373666\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1417123.272347711\n",
            "Sharpe:  0.4492090030505059\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1914      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 16        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.51e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -4.41e-07 |\n",
            "|    reward               | 1062855.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.16e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1603967.8688177657\n",
            "Sharpe:  0.5865064028395162\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1818      |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.46e+14  |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -1.65e-07 |\n",
            "|    reward               | 1163599.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.88e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1585512.3798984385\n",
            "Sharpe:  0.5741077579719103\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1821      |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.47e+14  |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | -1.3e-07  |\n",
            "|    reward               | 1297502.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.14e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1622831.2686386232\n",
            "Sharpe:  0.6023158282697999\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1828      |\n",
            "|    iterations           | 18        |\n",
            "|    time_elapsed         | 20        |\n",
            "|    total_timesteps      | 36864     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.6e+14   |\n",
            "|    n_updates            | 170       |\n",
            "|    policy_gradient_loss | -2.26e-07 |\n",
            "|    reward               | 1300823.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.29e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1588425.3252682954\n",
            "Sharpe:  0.5717409492375345\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1836      |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 21        |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.95e+14  |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | -2.6e-07  |\n",
            "|    reward               | 1463811.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.62e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1662035.4895858907\n",
            "Sharpe:  0.6283568924004761\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1842      |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 22        |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.8e+14   |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -2.33e-07 |\n",
            "|    reward               | 1624521.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.61e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1534219.140496859\n",
            "Sharpe:  0.5397522282849085\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1848      |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 23        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.89e+14  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -3.23e-07 |\n",
            "|    reward               | 1674493.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.71e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1488164.017969102\n",
            "Sharpe:  0.5082659558929653\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1853      |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 24        |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.76e+14  |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | -1.29e-07 |\n",
            "|    reward               | 1427190.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.54e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1486039.3443148911\n",
            "Sharpe:  0.4957090954191039\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1430614.1460180683\n",
            "Sharpe:  0.4636036370366182\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1848      |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.53e+14  |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -3.42e-07 |\n",
            "|    reward               | 1146479.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.19e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1407956.7246573383\n",
            "Sharpe:  0.44902942196535545\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1852      |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.31e+14  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -2.83e-07 |\n",
            "|    reward               | 1255820.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.72e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1558053.7728502494\n",
            "Sharpe:  0.5590988984113245\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1856      |\n",
            "|    iterations           | 25        |\n",
            "|    time_elapsed         | 27        |\n",
            "|    total_timesteps      | 51200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.31e+14  |\n",
            "|    n_updates            | 240       |\n",
            "|    policy_gradient_loss | -2.09e-07 |\n",
            "|    reward               | 1270791.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.83e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1551132.809086953\n",
            "Sharpe:  0.5486416191567399\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1860      |\n",
            "|    iterations           | 26        |\n",
            "|    time_elapsed         | 28        |\n",
            "|    total_timesteps      | 53248     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.71e+14  |\n",
            "|    n_updates            | 250       |\n",
            "|    policy_gradient_loss | -3.1e-07  |\n",
            "|    reward               | 1411026.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.2e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1583070.8466445128\n",
            "Sharpe:  0.5711438204705968\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1864      |\n",
            "|    iterations           | 27        |\n",
            "|    time_elapsed         | 29        |\n",
            "|    total_timesteps      | 55296     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.59e+14  |\n",
            "|    n_updates            | 260       |\n",
            "|    policy_gradient_loss | -1.39e-07 |\n",
            "|    reward               | 1414148.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.35e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1496752.82452433\n",
            "Sharpe:  0.508684584392171\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1869      |\n",
            "|    iterations           | 28        |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 57344     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.74e+14  |\n",
            "|    n_updates            | 270       |\n",
            "|    policy_gradient_loss | -3.46e-07 |\n",
            "|    reward               | 1645220.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.34e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1438694.95590323\n",
            "Sharpe:  0.47854590960515914\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1868      |\n",
            "|    iterations           | 29        |\n",
            "|    time_elapsed         | 31        |\n",
            "|    total_timesteps      | 59392     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.6e+14   |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 1452521.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.3e+14   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1435720.2284662796\n",
            "Sharpe:  0.47387252452034306\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1464745.2110934854\n",
            "Sharpe:  0.4813735943051759\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1862      |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 32        |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.63e+14  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | -2.48e-07 |\n",
            "|    reward               | 1089891.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.05e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1497286.7509745255\n",
            "Sharpe:  0.5102278433365961\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1861      |\n",
            "|    iterations           | 31        |\n",
            "|    time_elapsed         | 34        |\n",
            "|    total_timesteps      | 63488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.42e+14  |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | -1.79e-07 |\n",
            "|    reward               | 1183575.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.84e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1511025.3052143585\n",
            "Sharpe:  0.5236945868782921\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1861      |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 35        |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.45e+14  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -3.53e-07 |\n",
            "|    reward               | 1296766.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 4.83e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1572333.0521760238\n",
            "Sharpe:  0.5685692505136045\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1862      |\n",
            "|    iterations           | 33        |\n",
            "|    time_elapsed         | 36        |\n",
            "|    total_timesteps      | 67584     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.55e+14  |\n",
            "|    n_updates            | 320       |\n",
            "|    policy_gradient_loss | -2.14e-07 |\n",
            "|    reward               | 1332975.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.01e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1622225.4534883157\n",
            "Sharpe:  0.6019861474113939\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1865      |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 37        |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.64e+14  |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | -3.6e-07  |\n",
            "|    reward               | 1167493.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.31e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1621539.0094842126\n",
            "Sharpe:  0.604292651005402\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1867      |\n",
            "|    iterations           | 35        |\n",
            "|    time_elapsed         | 38        |\n",
            "|    total_timesteps      | 71680     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.77e+14  |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | -3.67e-07 |\n",
            "|    reward               | 1546243.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.53e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1465962.5740106339\n",
            "Sharpe:  0.48732031500707007\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1868      |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 39        |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.83e+14  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | -1.88e-07 |\n",
            "|    reward               | 1679021.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.41e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1590716.9714721146\n",
            "Sharpe:  0.5788790128677098\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1870      |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 40        |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.71e+14  |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -1.7e-07  |\n",
            "|    reward               | 1514545.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.23e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1477767.5522068716\n",
            "Sharpe:  0.49438308293803723\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1461868.2169825367\n",
            "Sharpe:  0.48352091386371426\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1867      |\n",
            "|    iterations           | 38        |\n",
            "|    time_elapsed         | 41        |\n",
            "|    total_timesteps      | 77824     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.56e+14  |\n",
            "|    n_updates            | 370       |\n",
            "|    policy_gradient_loss | -1.53e-07 |\n",
            "|    reward               | 1134496.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.23e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1668332.47694275\n",
            "Sharpe:  0.6296593831887766\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1868      |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 42        |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.38e+14  |\n",
            "|    n_updates            | 380       |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 1200434.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.03e+14  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1505404.0388732664\n",
            "Sharpe:  0.5172311746087593\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1871      |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 43        |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -11.4     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 2.77e+14  |\n",
            "|    n_updates            | 390       |\n",
            "|    policy_gradient_loss | -2.61e-07 |\n",
            "|    reward               | 1239432.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 5.5e+14   |\n",
            "---------------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/ppo/ppo.zip\n",
            "[INFO] Model 'PPO' reloaded from 2015-2025_no_crypto/ppo/ppo.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1157940.2848745715\n",
            "Sharpe:  0.7556773765591154\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/ppo:\n",
            " - Daily Return: 2015-2025_no_crypto/ppo/ppo_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/ppo/ppo_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/ppo/\n",
            "\n",
            "========== Running DDPG on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training DDPG | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1461343.065210168\n",
            "Sharpe:  0.46637396849791457\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 240       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.89e+07 |\n",
            "|    critic_loss     | 8.45e+10  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 59        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.7e+07  |\n",
            "|    critic_loss     | 5.48e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 89        |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.6e+07  |\n",
            "|    critic_loss     | 8.43e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 119       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.85e+07 |\n",
            "|    critic_loss     | 1.02e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 238       |\n",
            "|    time_elapsed    | 151       |\n",
            "|    total_timesteps | 36100     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.07e+08 |\n",
            "|    critic_loss     | 1.71e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 35999     |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 182       |\n",
            "|    total_timesteps | 43320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.14e+08 |\n",
            "|    critic_loss     | 1.79e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 43219     |\n",
            "|    reward          | 1445198.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1445198.7997477679\n",
            "Sharpe:  0.4536048726932688\n",
            "=================================\n",
            "[INFO] Model saved at 2015-2025_no_crypto/ddpg/ddpg.zip\n",
            "[INFO] Model 'DDPG' reloaded from 2015-2025_no_crypto/ddpg/ddpg.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1116379.688120613\n",
            "Sharpe:  0.5565006942416667\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/ddpg:\n",
            " - Daily Return: 2015-2025_no_crypto/ddpg/ddpg_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/ddpg/ddpg_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/ddpg/\n",
            "\n",
            "========== Running SAC on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training SAC | Timesteps: 50000\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1409740.686363166\n",
            "Sharpe:  0.47798559449051287\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378787.422876926\n",
            "Sharpe:  0.45212780635038025\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 189       |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.63e+07 |\n",
            "|    critic_loss     | 7.04e+10  |\n",
            "|    ent_coef        | 0.927     |\n",
            "|    ent_coef_loss   | 7.35      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 185       |\n",
            "|    time_elapsed    | 77        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.35e+07 |\n",
            "|    critic_loss     | 1.67e+11  |\n",
            "|    ent_coef        | 8.08      |\n",
            "|    ent_coef_loss   | -204      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 183       |\n",
            "|    time_elapsed    | 117       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.03e+07 |\n",
            "|    critic_loss     | 3.49e+11  |\n",
            "|    ent_coef        | 70.5      |\n",
            "|    ent_coef_loss   | -414      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 188       |\n",
            "|    time_elapsed    | 153       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.3e+07  |\n",
            "|    critic_loss     | 4.85e+11  |\n",
            "|    ent_coef        | 615       |\n",
            "|    ent_coef_loss   | -627      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 190       |\n",
            "|    time_elapsed    | 189       |\n",
            "|    total_timesteps | 36100     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.34e+07 |\n",
            "|    critic_loss     | 7.19e+11  |\n",
            "|    ent_coef        | 5.36e+03  |\n",
            "|    ent_coef_loss   | -838      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 35999     |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 191       |\n",
            "|    time_elapsed    | 225       |\n",
            "|    total_timesteps | 43320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.08e+07 |\n",
            "|    critic_loss     | 5.64e+11  |\n",
            "|    ent_coef        | 4.68e+04  |\n",
            "|    ent_coef_loss   | -1.05e+03 |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 43219     |\n",
            "|    reward          | 1378786.0 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1378785.9805170798\n",
            "Sharpe:  0.4521267389030994\n",
            "=================================\n",
            "[INFO] Model saved at 2015-2025_no_crypto/sac/sac.zip\n",
            "[INFO] Model 'SAC' reloaded from 2015-2025_no_crypto/sac/sac.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1109596.0428397686\n",
            "Sharpe:  0.5982570483628321\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/sac:\n",
            " - Daily Return: 2015-2025_no_crypto/sac/sac_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/sac/sac_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/sac/\n",
            "\n",
            "========== Running TD3 on 2015-2025_no_crypto.csv ==========\n",
            "[INFO] Training TD3 | Timesteps: 30000\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1518073.76868472\n",
            "Sharpe:  0.5810138493247435\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 261       |\n",
            "|    time_elapsed    | 27        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.05e+07 |\n",
            "|    critic_loss     | 1.12e+11  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 1520597.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 249       |\n",
            "|    time_elapsed    | 57        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.87e+07 |\n",
            "|    critic_loss     | 2.75e+13  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 1520597.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 251       |\n",
            "|    time_elapsed    | 86        |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.3e+07  |\n",
            "|    critic_loss     | 6.7e+11   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 1520597.8 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1520597.7078308356\n",
            "Sharpe:  0.5861279101249076\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 121       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.48e+07 |\n",
            "|    critic_loss     | 1.06e+12  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 1520597.8 |\n",
            "----------------------------------\n",
            "[INFO] Model saved at 2015-2025_no_crypto/td3/td3.zip\n",
            "[INFO] Model 'TD3' reloaded from 2015-2025_no_crypto/td3/td3.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1177212.7591999862\n",
            "Sharpe:  0.8653638664889568\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved to 2015-2025_no_crypto/td3:\n",
            " - Daily Return: 2015-2025_no_crypto/td3/td3_daily_return.csv\n",
            " - Actions     : 2015-2025_no_crypto/td3/td3_actions.csv\n",
            "[INFO] Moved results to 2015-2025_no_crypto/td3/\n"
          ]
        }
      ],
      "source": [
        "datasets = {\n",
        "    \"2007-2025_no_crypto.csv\": processed_0,\n",
        "    \"2015-2025_crypto.csv\": processed_1,\n",
        "    \"2015-2025_no_crypto.csv\": processed_2\n",
        "}\n",
        "\n",
        "\n",
        "models = ['a2c', 'ppo', 'ddpg', 'sac', 'td3']\n",
        "\n",
        "date_configs = {\n",
        "    \"2007-2025_no_crypto.csv\": {\n",
        "        \"train_start\": '2007-06-01',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    },\n",
        "    \"2015-2025_crypto.csv\": {\n",
        "        \"train_start\": '2015-02-02',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    },\n",
        "    \"2015-2025_no_crypto.csv\": {\n",
        "        \"train_start\": '2015-02-02',\n",
        "        \"train_end\": '2023-04-04',\n",
        "        \"trade_start\": '2023-04-05',\n",
        "        \"trade_end\": '2025-04-10'\n",
        "    }\n",
        "}\n",
        "\n",
        "for csv_name, processed_df in datasets.items():\n",
        "    dates = date_configs[csv_name]\n",
        "    \n",
        "    for model_name in models:\n",
        "        print(f\"\\n========== Running {model_name.upper()} on {csv_name} ==========\")\n",
        "        \n",
        "        return_csv = f\"{model_name}_daily_return.csv\"\n",
        "        action_csv = f\"{model_name}_actions.csv\"\n",
        "        \n",
        "        df_daily_return, df_actions = train_and_predict_drl(\n",
        "            df = processed_df,\n",
        "            train_start_date = dates[\"train_start\"],\n",
        "            train_end_date = dates[\"train_end\"],\n",
        "            trade_start_date = dates[\"trade_start\"],\n",
        "            trade_end_date = dates[\"trade_end\"],\n",
        "            model_name = model_name,\n",
        "            output_return_csv = return_csv,\n",
        "            output_action_csv = action_csv,\n",
        "            original_csv_path = csv_name\n",
        "        )\n",
        "\n",
        "\n",
        "# df_daily_return_1, df_actions_1 = train_and_predict_drl(\n",
        "#     df = processed_1,\n",
        "#     train_start_date = '2016-02-02',\n",
        "#     train_end_date = '2023-04-04',\n",
        "#     trade_start_date = '2023-04-05',\n",
        "#     trade_end_date = '2025-04-10',\n",
        "#     model_name = 'a2c',\n",
        "#     output_return_csv = 'df_daily_return_1.csv',\n",
        "#     output_action_csv = 'df_actions_1.csv',\n",
        "#     original_csv_path = '2007-2025_no_crypto.csv'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFO42LcomPUT"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAvxipWFmUe8"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
