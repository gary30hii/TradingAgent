{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_PortfolioAllocation_NeurIPS_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv3IDvrobU37"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Portfolio Allocation\n",
        "\n",
        "Tutorials to use OpenAI DRL to perform portfolio allocation in one Jupyter Notebook | Presented at NeurIPS 2020: Deep RL Workshop\n",
        "\n",
        "* This blog is based on our paper: FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance, presented at NeurIPS 2020: Deep RL Workshop.\n",
        "* Check out medium blog for detailed explanations: https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-Foundation/FinRL/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHCfEiTA80V"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUmLTmoQA7_w"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12v1i0jVkg48"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L63HKnWvkirx"
      },
      "source": [
        "This problem is to design an automated trading solution for portfolio alloacation. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A represents the weight of a stock in the porfolio: a ∈ (-1,1). Assume our stock pool includes N stocks, we can use a list [a<sub>1</sub>, a<sub>2</sub>, ... , a<sub>N</sub>] to determine the weight for each stock in the porfotlio, where a<sub>i</sub> ∈ (-1,1), a<sub>1</sub>+ a<sub>2</sub>+...+a<sub>N</sub>=1. For example, \"The weight of AAPL in the portfolio is 10%.\" is [0.1 , ...].\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_emqQCCklVt"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVCcCalAknGn"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pT8a0fvhA_TW",
        "outputId": "c7abc977-bf49-4a96-e636-cbf8cbfecd87",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "Requirement already satisfied: shimmy>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from shimmy>=2.0) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-01fjwjnm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-01fjwjnm\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit d25d902a6de54931a329adc38a2663e8f576adc4\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-j23zyn2u/elegantrl_6194f6e594394cb2a633ef2edb0e78f4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-j23zyn2u/elegantrl_6194f6e594394cb2a633ef2edb0e78f4\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install pandas_market_calendars\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2568cp5bU38"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNmvYN9YbU4B"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "ntfTb0e2bU4C",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings & Backend Setup\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for environments without GUI\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Jupyter Notebooks (optional, safe to keep)\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl import config, config_tickers\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.plot import (\n",
        "    backtest_stats, \n",
        "    backtest_plot, \n",
        "    get_daily_return, \n",
        "    get_baseline,\n",
        "    convert_daily_return_to_pyfolio_ts\n",
        ")\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.meta.data_processors.processor_yahoofinance import YahooFinanceProcessor\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "# ===========================\n",
        "# Custom Library Path\n",
        "# ===========================\n",
        "sys.path.append(\"../FinRL-Library\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIS2abxkwan"
      },
      "source": [
        "<a id='1.4'></a>\n",
        "## 2.4. Create Folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBria_QbU4F"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "Dx1LMUjNqqPX"
      },
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = '2016-02-02'\n",
        "TRAIN_END_DATE = '2023-04-04'\n",
        "TRADE_START_DATE = '2023-04-05'\n",
        "TRADE_END_DATE = '2025-04-10'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9UwKwzRbU4l"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "\n",
        "def process_csv_to_features(csv_path, lookback=252):\n",
        "    # Step 1: Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    # Step 3: Filter tickers\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 4: Apply technical indicators\n",
        "    fe = FeatureEngineer(use_technical_indicator=True, use_turbulence=False, user_defined_feature=False)\n",
        "    df_5day_full = fe.preprocess_data(df_5day_full)\n",
        "    df_7day_full = fe.preprocess_data(df_7day_full)\n",
        "\n",
        "    # Step 5: Combine and clean\n",
        "    df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    df.index = range(len(df))\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df[df.groupby('date')['date'].transform('count') > 1]\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Prepare for covariance matrix computation\n",
        "    df = df.sort_values(['date', 'tic'], ignore_index=True)\n",
        "    df.index = df.date.factorize()[0]  # Re-index based on unique date\n",
        "\n",
        "    cov_list = []\n",
        "    return_list = []\n",
        "    unique_indices = df.index.unique()\n",
        "\n",
        "    for i in range(lookback, len(unique_indices)):\n",
        "        data_lookback = df.loc[i - lookback:i, :]\n",
        "        price_lookback = data_lookback.pivot_table(index='date', columns='tic', values='close')\n",
        "        return_lookback = price_lookback.pct_change().dropna()\n",
        "        return_list.append(return_lookback)\n",
        "        cov_list.append(return_lookback.cov().values)\n",
        "\n",
        "    # Step 7: Merge covariance matrix and return series back\n",
        "    df_cov = pd.DataFrame({\n",
        "        'date': df.date.unique()[lookback:], \n",
        "        'cov_list': cov_list, \n",
        "        'return_list': return_list\n",
        "    })\n",
        "    df = df.merge(df_cov, on='date')\n",
        "    df = df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n"
          ]
        }
      ],
      "source": [
        "processed = process_csv_to_features('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>cov_list</th>\n",
              "      <th>return_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>109.32</td>\n",
              "      <td>109.38</td>\n",
              "      <td>109.1301</td>\n",
              "      <td>109.23</td>\n",
              "      <td>4835115.0</td>\n",
              "      <td>agg</td>\n",
              "      <td>1</td>\n",
              "      <td>0.207871</td>\n",
              "      <td>109.414436</td>\n",
              "      <td>108.155564</td>\n",
              "      <td>56.401620</td>\n",
              "      <td>128.048919</td>\n",
              "      <td>18.311124</td>\n",
              "      <td>108.540000</td>\n",
              "      <td>108.509083</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>91.34</td>\n",
              "      <td>91.36</td>\n",
              "      <td>91.3400</td>\n",
              "      <td>91.34</td>\n",
              "      <td>1627284.0</td>\n",
              "      <td>bil</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003626</td>\n",
              "      <td>91.386806</td>\n",
              "      <td>91.327194</td>\n",
              "      <td>46.986934</td>\n",
              "      <td>-124.094203</td>\n",
              "      <td>12.399460</td>\n",
              "      <td>91.360333</td>\n",
              "      <td>91.364333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>372.93</td>\n",
              "      <td>374.41</td>\n",
              "      <td>371.1700</td>\n",
              "      <td>371.33</td>\n",
              "      <td>6817.0</td>\n",
              "      <td>btcusd</td>\n",
              "      <td>1</td>\n",
              "      <td>-12.397434</td>\n",
              "      <td>422.125588</td>\n",
              "      <td>352.168412</td>\n",
              "      <td>45.804685</td>\n",
              "      <td>-84.272928</td>\n",
              "      <td>40.704445</td>\n",
              "      <td>405.695000</td>\n",
              "      <td>419.803333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>108.09</td>\n",
              "      <td>108.18</td>\n",
              "      <td>107.3500</td>\n",
              "      <td>107.92</td>\n",
              "      <td>6656018.0</td>\n",
              "      <td>gld</td>\n",
              "      <td>1</td>\n",
              "      <td>1.135824</td>\n",
              "      <td>108.620100</td>\n",
              "      <td>102.477820</td>\n",
              "      <td>56.790972</td>\n",
              "      <td>143.671556</td>\n",
              "      <td>35.269843</td>\n",
              "      <td>104.479307</td>\n",
              "      <td>103.650320</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>190.16</td>\n",
              "      <td>191.97</td>\n",
              "      <td>189.5400</td>\n",
              "      <td>191.96</td>\n",
              "      <td>182564890.0</td>\n",
              "      <td>spy</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.744645</td>\n",
              "      <td>199.106331</td>\n",
              "      <td>183.296129</td>\n",
              "      <td>43.397377</td>\n",
              "      <td>-51.889822</td>\n",
              "      <td>20.952982</td>\n",
              "      <td>195.478153</td>\n",
              "      <td>201.393583</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date   close    high       low    open       volume     tic  day  \\\n",
              "0 2016-02-02  109.32  109.38  109.1301  109.23    4835115.0     agg    1   \n",
              "1 2016-02-02   91.34   91.36   91.3400   91.34    1627284.0     bil    1   \n",
              "2 2016-02-02  372.93  374.41  371.1700  371.33       6817.0  btcusd    1   \n",
              "3 2016-02-02  108.09  108.18  107.3500  107.92    6656018.0     gld    1   \n",
              "4 2016-02-02  190.16  191.97  189.5400  191.96  182564890.0     spy    1   \n",
              "\n",
              "        macd     boll_ub     boll_lb     rsi_30      cci_30      dx_30  \\\n",
              "0   0.207871  109.414436  108.155564  56.401620  128.048919  18.311124   \n",
              "1  -0.003626   91.386806   91.327194  46.986934 -124.094203  12.399460   \n",
              "2 -12.397434  422.125588  352.168412  45.804685  -84.272928  40.704445   \n",
              "3   1.135824  108.620100  102.477820  56.790972  143.671556  35.269843   \n",
              "4  -2.744645  199.106331  183.296129  43.397377  -51.889822  20.952982   \n",
              "\n",
              "   close_30_sma  close_60_sma  \\\n",
              "0    108.540000    108.509083   \n",
              "1     91.360333     91.364333   \n",
              "2    405.695000    419.803333   \n",
              "3    104.479307    103.650320   \n",
              "4    195.478153    201.393583   \n",
              "\n",
              "                                            cov_list  \\\n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "1  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "2  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "3  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "4  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "\n",
              "                                         return_list  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "1  tic              agg       bil    btcusd      ...  \n",
              "2  tic              agg       bil    btcusd      ...  \n",
              "3  tic              agg       bil    btcusd      ...  \n",
              "4  tic              agg       bil    btcusd      ...  "
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UooHj1OgbU4v"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQnmN1qdk88I"
      },
      "source": [
        "## Training data split: 2009-01-01 to 2020-07-01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "NrPxgv4eBQ_R",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "train = data_split(processed, TRAIN_START_DATE, TRAIN_END_DATE)\n",
        "trade = data_split(processed, TRADE_START_DATE, TRADE_END_DATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>cov_list</th>\n",
              "      <th>return_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>109.32</td>\n",
              "      <td>109.38</td>\n",
              "      <td>109.1301</td>\n",
              "      <td>109.23</td>\n",
              "      <td>4835115.0</td>\n",
              "      <td>agg</td>\n",
              "      <td>1</td>\n",
              "      <td>0.207871</td>\n",
              "      <td>109.414436</td>\n",
              "      <td>108.155564</td>\n",
              "      <td>56.401620</td>\n",
              "      <td>128.048919</td>\n",
              "      <td>18.311124</td>\n",
              "      <td>108.540000</td>\n",
              "      <td>108.509083</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>91.34</td>\n",
              "      <td>91.36</td>\n",
              "      <td>91.3400</td>\n",
              "      <td>91.34</td>\n",
              "      <td>1627284.0</td>\n",
              "      <td>bil</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003626</td>\n",
              "      <td>91.386806</td>\n",
              "      <td>91.327194</td>\n",
              "      <td>46.986934</td>\n",
              "      <td>-124.094203</td>\n",
              "      <td>12.399460</td>\n",
              "      <td>91.360333</td>\n",
              "      <td>91.364333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>372.93</td>\n",
              "      <td>374.41</td>\n",
              "      <td>371.1700</td>\n",
              "      <td>371.33</td>\n",
              "      <td>6817.0</td>\n",
              "      <td>btcusd</td>\n",
              "      <td>1</td>\n",
              "      <td>-12.397434</td>\n",
              "      <td>422.125588</td>\n",
              "      <td>352.168412</td>\n",
              "      <td>45.804685</td>\n",
              "      <td>-84.272928</td>\n",
              "      <td>40.704445</td>\n",
              "      <td>405.695000</td>\n",
              "      <td>419.803333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>108.09</td>\n",
              "      <td>108.18</td>\n",
              "      <td>107.3500</td>\n",
              "      <td>107.92</td>\n",
              "      <td>6656018.0</td>\n",
              "      <td>gld</td>\n",
              "      <td>1</td>\n",
              "      <td>1.135824</td>\n",
              "      <td>108.620100</td>\n",
              "      <td>102.477820</td>\n",
              "      <td>56.790972</td>\n",
              "      <td>143.671556</td>\n",
              "      <td>35.269843</td>\n",
              "      <td>104.479307</td>\n",
              "      <td>103.650320</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>190.16</td>\n",
              "      <td>191.97</td>\n",
              "      <td>189.5400</td>\n",
              "      <td>191.96</td>\n",
              "      <td>182564890.0</td>\n",
              "      <td>spy</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.744645</td>\n",
              "      <td>199.106331</td>\n",
              "      <td>183.296129</td>\n",
              "      <td>43.397377</td>\n",
              "      <td>-51.889822</td>\n",
              "      <td>20.952982</td>\n",
              "      <td>195.478153</td>\n",
              "      <td>201.393583</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date   close    high       low    open       volume     tic  day  \\\n",
              "0 2016-02-02  109.32  109.38  109.1301  109.23    4835115.0     agg    1   \n",
              "0 2016-02-02   91.34   91.36   91.3400   91.34    1627284.0     bil    1   \n",
              "0 2016-02-02  372.93  374.41  371.1700  371.33       6817.0  btcusd    1   \n",
              "0 2016-02-02  108.09  108.18  107.3500  107.92    6656018.0     gld    1   \n",
              "0 2016-02-02  190.16  191.97  189.5400  191.96  182564890.0     spy    1   \n",
              "\n",
              "        macd     boll_ub     boll_lb     rsi_30      cci_30      dx_30  \\\n",
              "0   0.207871  109.414436  108.155564  56.401620  128.048919  18.311124   \n",
              "0  -0.003626   91.386806   91.327194  46.986934 -124.094203  12.399460   \n",
              "0 -12.397434  422.125588  352.168412  45.804685  -84.272928  40.704445   \n",
              "0   1.135824  108.620100  102.477820  56.790972  143.671556  35.269843   \n",
              "0  -2.744645  199.106331  183.296129  43.397377  -51.889822  20.952982   \n",
              "\n",
              "   close_30_sma  close_60_sma  \\\n",
              "0    108.540000    108.509083   \n",
              "0     91.360333     91.364333   \n",
              "0    405.695000    419.803333   \n",
              "0    104.479307    103.650320   \n",
              "0    195.478153    201.393583   \n",
              "\n",
              "                                            cov_list  \\\n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "\n",
              "                                         return_list  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  "
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "vU2vXEll0hfk",
        "outputId": "a36eef56-27f9-49c3-cb75-52f5e4fb4a16",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>tic</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>cov_list</th>\n",
              "      <th>return_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>109.32</td>\n",
              "      <td>109.38</td>\n",
              "      <td>109.1301</td>\n",
              "      <td>109.23</td>\n",
              "      <td>4835115.0</td>\n",
              "      <td>agg</td>\n",
              "      <td>1</td>\n",
              "      <td>0.207871</td>\n",
              "      <td>109.414436</td>\n",
              "      <td>108.155564</td>\n",
              "      <td>56.401620</td>\n",
              "      <td>128.048919</td>\n",
              "      <td>18.311124</td>\n",
              "      <td>108.540000</td>\n",
              "      <td>108.509083</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>91.34</td>\n",
              "      <td>91.36</td>\n",
              "      <td>91.3400</td>\n",
              "      <td>91.34</td>\n",
              "      <td>1627284.0</td>\n",
              "      <td>bil</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003626</td>\n",
              "      <td>91.386806</td>\n",
              "      <td>91.327194</td>\n",
              "      <td>46.986934</td>\n",
              "      <td>-124.094203</td>\n",
              "      <td>12.399460</td>\n",
              "      <td>91.360333</td>\n",
              "      <td>91.364333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>372.93</td>\n",
              "      <td>374.41</td>\n",
              "      <td>371.1700</td>\n",
              "      <td>371.33</td>\n",
              "      <td>6817.0</td>\n",
              "      <td>btcusd</td>\n",
              "      <td>1</td>\n",
              "      <td>-12.397434</td>\n",
              "      <td>422.125588</td>\n",
              "      <td>352.168412</td>\n",
              "      <td>45.804685</td>\n",
              "      <td>-84.272928</td>\n",
              "      <td>40.704445</td>\n",
              "      <td>405.695000</td>\n",
              "      <td>419.803333</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>108.09</td>\n",
              "      <td>108.18</td>\n",
              "      <td>107.3500</td>\n",
              "      <td>107.92</td>\n",
              "      <td>6656018.0</td>\n",
              "      <td>gld</td>\n",
              "      <td>1</td>\n",
              "      <td>1.135824</td>\n",
              "      <td>108.620100</td>\n",
              "      <td>102.477820</td>\n",
              "      <td>56.790972</td>\n",
              "      <td>143.671556</td>\n",
              "      <td>35.269843</td>\n",
              "      <td>104.479307</td>\n",
              "      <td>103.650320</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-02</td>\n",
              "      <td>190.16</td>\n",
              "      <td>191.97</td>\n",
              "      <td>189.5400</td>\n",
              "      <td>191.96</td>\n",
              "      <td>182564890.0</td>\n",
              "      <td>spy</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.744645</td>\n",
              "      <td>199.106331</td>\n",
              "      <td>183.296129</td>\n",
              "      <td>43.397377</td>\n",
              "      <td>-51.889822</td>\n",
              "      <td>20.952982</td>\n",
              "      <td>195.478153</td>\n",
              "      <td>201.393583</td>\n",
              "      <td>[[5.736252007431309e-06, 6.710303377777988e-09...</td>\n",
              "      <td>tic              agg       bil    btcusd      ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date   close    high       low    open       volume     tic  day  \\\n",
              "0 2016-02-02  109.32  109.38  109.1301  109.23    4835115.0     agg    1   \n",
              "0 2016-02-02   91.34   91.36   91.3400   91.34    1627284.0     bil    1   \n",
              "0 2016-02-02  372.93  374.41  371.1700  371.33       6817.0  btcusd    1   \n",
              "0 2016-02-02  108.09  108.18  107.3500  107.92    6656018.0     gld    1   \n",
              "0 2016-02-02  190.16  191.97  189.5400  191.96  182564890.0     spy    1   \n",
              "\n",
              "        macd     boll_ub     boll_lb     rsi_30      cci_30      dx_30  \\\n",
              "0   0.207871  109.414436  108.155564  56.401620  128.048919  18.311124   \n",
              "0  -0.003626   91.386806   91.327194  46.986934 -124.094203  12.399460   \n",
              "0 -12.397434  422.125588  352.168412  45.804685  -84.272928  40.704445   \n",
              "0   1.135824  108.620100  102.477820  56.790972  143.671556  35.269843   \n",
              "0  -2.744645  199.106331  183.296129  43.397377  -51.889822  20.952982   \n",
              "\n",
              "   close_30_sma  close_60_sma  \\\n",
              "0    108.540000    108.509083   \n",
              "0     91.360333     91.364333   \n",
              "0    405.695000    419.803333   \n",
              "0    104.479307    103.650320   \n",
              "0    195.478153    201.393583   \n",
              "\n",
              "                                            cov_list  \\\n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "0  [[5.736252007431309e-06, 6.710303377777988e-09...   \n",
              "\n",
              "                                         return_list  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  \n",
              "0  tic              agg       bil    btcusd      ...  "
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxQTNjpblAMN"
      },
      "source": [
        "## Environment for Portfolio Allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "xlfE-VERbU40",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "\n",
        "class StockPortfolioEnv(gym.Env):\n",
        "    \"\"\"A single stock trading environment for OpenAI gym\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        df: DataFrame\n",
        "            input data\n",
        "        stock_dim : int\n",
        "            number of unique stocks\n",
        "        hmax : int\n",
        "            maximum number of shares to trade\n",
        "        initial_amount : int\n",
        "            start money\n",
        "        transaction_cost_pct: float\n",
        "            transaction cost percentage per trade\n",
        "        reward_scaling: float\n",
        "            scaling factor for reward, good for training\n",
        "        state_space: int\n",
        "            the dimension of input features\n",
        "        action_space: int\n",
        "            equals stock dimension\n",
        "        tech_indicator_list: list\n",
        "            a list of technical indicator names\n",
        "        turbulence_threshold: int\n",
        "            a threshold to control risk aversion\n",
        "        day: int\n",
        "            an increment number to control date\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    _sell_stock()\n",
        "        perform sell action based on the sign of the action\n",
        "    _buy_stock()\n",
        "        perform buy action based on the sign of the action\n",
        "    step()\n",
        "        at each step the agent will return actions, then\n",
        "        we will calculate the reward, and return the next observation.\n",
        "    reset()\n",
        "        reset the environment\n",
        "    render()\n",
        "        use render to return other functions\n",
        "    save_asset_memory()\n",
        "        return account value at each time step\n",
        "    save_action_memory()\n",
        "        return actions/positions at each time step\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self,\n",
        "                df,\n",
        "                stock_dim,\n",
        "                hmax,\n",
        "                initial_amount,\n",
        "                transaction_cost_pct,\n",
        "                reward_scaling,\n",
        "                state_space,\n",
        "                action_space,\n",
        "                tech_indicator_list,\n",
        "                turbulence_threshold=None,\n",
        "                lookback=252,\n",
        "                day = 0):\n",
        "        #super(StockEnv, self).__init__()\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.lookback=lookback\n",
        "        self.df = df\n",
        "        self.stock_dim = stock_dim\n",
        "        self.hmax = hmax\n",
        "        self.initial_amount = initial_amount\n",
        "        self.transaction_cost_pct =transaction_cost_pct\n",
        "        self.reward_scaling = reward_scaling\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "\n",
        "        # action_space normalization and shape is self.stock_dim\n",
        "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,))\n",
        "        # Shape = (34, 30)\n",
        "        # covariance matrix + technical indicators\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
        "\n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.terminal = False\n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
        "        self.portfolio_value = self.initial_amount\n",
        "\n",
        "        # memorize portfolio value each step\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        # memorize portfolio return each step\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "\n",
        "\n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            df = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df.columns = ['daily_return']\n",
        "            plt.plot(df.daily_return.cumsum(),'r')\n",
        "            plt.savefig('results/cumulative_reward.png')\n",
        "            plt.close()\n",
        "\n",
        "            plt.plot(self.portfolio_return_memory,'r')\n",
        "            plt.savefig('results/rewards.png')\n",
        "            plt.close()\n",
        "\n",
        "            print(\"=================================\")\n",
        "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
        "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
        "\n",
        "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
        "            df_daily_return.columns = ['daily_return']\n",
        "            if df_daily_return['daily_return'].std() !=0:\n",
        "              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
        "                       df_daily_return['daily_return'].std()\n",
        "              print(\"Sharpe: \",sharpe)\n",
        "            print(\"=================================\")\n",
        "\n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            #print(\"Model actions: \",actions)\n",
        "            # actions are the portfolio weight\n",
        "            # normalize to sum of 1\n",
        "            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n",
        "            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n",
        "            #else:\n",
        "            #  norm_actions = actions\n",
        "            weights = self.softmax_normalization(actions)\n",
        "            #print(\"Normalized actions: \", weights)\n",
        "            self.actions_memory.append(weights)\n",
        "            last_day_memory = self.data\n",
        "\n",
        "            #load next state\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.covs = self.data['cov_list'].values[0]\n",
        "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "            #print(self.state)\n",
        "            # calcualte portfolio return\n",
        "            # individual stocks' return * weight\n",
        "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
        "            # update portfolio value\n",
        "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
        "            self.portfolio_value = new_portfolio_value\n",
        "\n",
        "            # save into memory\n",
        "            self.portfolio_return_memory.append(portfolio_return)\n",
        "            self.date_memory.append(self.data.date.unique()[0])\n",
        "            self.asset_memory.append(new_portfolio_value)\n",
        "\n",
        "            # the reward is the new portfolio value or end portfolo value\n",
        "            self.reward = new_portfolio_value\n",
        "            #print(\"Step reward: \", self.reward)\n",
        "            #self.reward = self.reward*self.reward_scaling\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [self.initial_amount]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        # load states\n",
        "        self.covs = self.data['cov_list'].values[0]\n",
        "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        self.portfolio_value = self.initial_amount\n",
        "        #self.cost = 0\n",
        "        #self.trades = 0\n",
        "        self.terminal = False\n",
        "        self.portfolio_return_memory = [0]\n",
        "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
        "        self.date_memory=[self.data.date.unique()[0]]\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def softmax_normalization(self, actions):\n",
        "        numerator = np.exp(actions)\n",
        "        denominator = np.sum(np.exp(actions))\n",
        "        softmax_output = numerator/denominator\n",
        "        return softmax_output\n",
        "\n",
        "\n",
        "    def save_asset_memory(self):\n",
        "        date_list = self.date_memory\n",
        "        portfolio_return = self.portfolio_return_memory\n",
        "        #print(len(date_list))\n",
        "        #print(len(asset_list))\n",
        "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
        "        return df_account_value\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        # date and close price length must match actions length\n",
        "        date_list = self.date_memory\n",
        "        df_date = pd.DataFrame(date_list)\n",
        "        df_date.columns = ['date']\n",
        "\n",
        "        action_list = self.actions_memory\n",
        "        df_actions = pd.DataFrame(action_list)\n",
        "        df_actions.columns = self.data.tic.values\n",
        "        df_actions.index = df_date.date\n",
        "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
        "        return df_actions\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_sb_env(self):\n",
        "        e = DummyVecEnv([lambda: self])\n",
        "        obs = e.reset()\n",
        "        return e, obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzD06X0CbU43",
        "outputId": "b486b75a-a6f5-44d8-a75b-dcda1c64d95b",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 9, State Space: 9\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "jyg0_ZuVEVQ5",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n"
          ]
        }
      ],
      "source": [
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"transaction_cost_pct\": 0.001,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": config.INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "\n",
        "}\n",
        "\n",
        "e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTlOf8SJGdkl",
        "outputId": "2a2fdabd-9751-420a-dd13-dd05a1ca3fd6",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eKIu5UPlPnk"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "VDxU0iCEGdnb",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# initialize\n",
        "agent = DRLAgent(env = env_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdPe8uzflbXe"
      },
      "source": [
        "### Model 1: **A2C**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1tORf1fIcQ2",
        "outputId": "d7857242-9e6b-4afc-8ab3-0e55b094cc93",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
        "model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DazEdrMpIdyz",
        "outputId": "c3f99924-ae82-47a9-abcf-10c5bc0ca3c2",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1083      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 7.61e+07  |\n",
            "|    reward             | 2050692.1 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 4.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 969       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 1.01e+08  |\n",
            "|    reward             | 2345458.5 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 5.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1016      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 1.17e+08  |\n",
            "|    reward             | 3376825.0 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 1.23e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2972455.4764868687\n",
            "Sharpe:  1.0467021075488436\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 971       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 4.2e+07   |\n",
            "|    reward             | 1150399.6 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 1.43e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1002      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 6.01e+07  |\n",
            "|    reward             | 1595145.1 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 2.64e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1025      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 8.13e+07  |\n",
            "|    reward             | 2102664.8 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 4.77e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1040      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 8.39e+07  |\n",
            "|    reward             | 2215777.0 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 5.04e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2442066.48626749\n",
            "Sharpe:  0.8656152086207456\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1015      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 5.48e+07  |\n",
            "|    reward             | 1570942.2 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 2.49e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1015      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 6.35e+07  |\n",
            "|    reward             | 1896942.2 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 3.98e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1026      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 1.11e+08  |\n",
            "|    reward             | 3318394.5 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.13e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2958852.465517829\n",
            "Sharpe:  1.0326962317792276\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1015      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 3.73e+07  |\n",
            "|    reward             | 1122448.1 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 1.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1023      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 6.41e+07  |\n",
            "|    reward             | 1975687.0 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 4.22e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1031      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 8.02e+07  |\n",
            "|    reward             | 2237924.5 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 5.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1037      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 1.19e+08  |\n",
            "|    reward             | 3140258.8 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 1.07e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3232397.9095551916\n",
            "Sharpe:  1.1457435101791333\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 5.77e+07  |\n",
            "|    reward             | 1269561.5 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 1.79e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1005      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 5.41e+07  |\n",
            "|    reward             | 1558481.9 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 2.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1008      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 1.06e+08  |\n",
            "|    reward             | 2642831.2 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 7.86e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 9.8e+07   |\n",
            "|    reward             | 2370879.2 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 6.23e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2468660.1656646496\n",
            "Sharpe:  0.9031959940235194\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 999       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 6.84e+07  |\n",
            "|    reward             | 1886999.2 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 3.67e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1004      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 8.39e+07  |\n",
            "|    reward             | 1963821.9 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 4.1e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 1.04e+08  |\n",
            "|    reward             | 3063555.8 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.03e+14  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2754552.748714198\n",
            "Sharpe:  1.015008563547181\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1002      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 4.47e+07  |\n",
            "|    reward             | 1136719.1 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.42e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1003      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 6.62e+07  |\n",
            "|    reward             | 1634652.4 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 2.86e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1008      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 7.4e+07   |\n",
            "|    reward             | 1832597.9 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 3.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 8.7e+07   |\n",
            "|    reward             | 2159967.5 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 5.41e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2290145.024016685\n",
            "Sharpe:  0.8799736579266281\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1007      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 4.78e+07  |\n",
            "|    reward             | 1466269.9 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.29e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1010      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 6.93e+07  |\n",
            "|    reward             | 1931385.8 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 4.05e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1013      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 1.04e+08  |\n",
            "|    reward             | 2865076.0 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 8.69e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2678438.9192483798\n",
            "Sharpe:  1.025436228021175\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 3.76e+07  |\n",
            "|    reward             | 1104178.4 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 1.3e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1007      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 6.76e+07  |\n",
            "|    reward             | 1618822.2 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 2.72e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1011      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 4.92e+07  |\n",
            "|    reward             | 1462626.4 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 2.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1014      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 7.51e+07  |\n",
            "|    reward             | 2323293.2 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 6.06e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2077573.9618117157\n",
            "Sharpe:  0.7734491812323303\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1008      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 4.26e+07  |\n",
            "|    reward             | 1210710.6 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1010      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 5.57e+07  |\n",
            "|    reward             | 1590391.1 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 2.63e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 8.34e+07  |\n",
            "|    reward             | 2340965.8 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 6.01e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1015      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 7.67e+07  |\n",
            "|    reward             | 2218312.2 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 4.89e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2272609.44647221\n",
            "Sharpe:  0.8662852437219757\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 5.25e+07  |\n",
            "|    reward             | 1565785.4 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 2.66e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1010      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 7.18e+07  |\n",
            "|    reward             | 1784767.5 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 3.44e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1013      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 9.76e+07  |\n",
            "|    reward             | 2536317.2 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 6.75e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2099707.748442249\n",
            "Sharpe:  0.8065195989587737\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 4.78e+07  |\n",
            "|    reward             | 1205608.2 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 6.26e+07  |\n",
            "|    reward             | 1667775.8 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 2.89e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 7.13e+07  |\n",
            "|    reward             | 1974169.9 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 4.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1014      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 9.73e+07  |\n",
            "|    reward             | 2535105.2 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 6.53e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2435170.9873121125\n",
            "Sharpe:  0.9434039621327941\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1009      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 5.2e+07   |\n",
            "|    reward             | 1479527.0 |\n",
            "|    std                | 0.97      |\n",
            "|    value_loss         | 2.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1011      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 6.7e+07   |\n",
            "|    reward             | 1825203.0 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 3.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1013      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 8.58e+07  |\n",
            "|    reward             | 2599461.5 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 7.12e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2414710.4077917077\n",
            "Sharpe:  0.96363276384006\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1010      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 4e+07     |\n",
            "|    reward             | 1067118.6 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 1.21e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 6.32e+07  |\n",
            "|    reward             | 1710065.6 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 3.19e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1012      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 7.14e+07  |\n",
            "|    reward             | 1768030.8 |\n",
            "|    std                | 0.965     |\n",
            "|    value_loss         | 4.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1010      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 1.07e+08  |\n",
            "|    reward             | 2735641.5 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 8.27e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2647600.686877002\n",
            "Sharpe:  1.0428672007496609\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1002      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 5.48e+07  |\n",
            "|    reward             | 1234295.8 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 1.59e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1000      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 5.55e+07  |\n",
            "|    reward             | 1614413.0 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 2.87e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 998       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 8.84e+07  |\n",
            "|    reward             | 2599253.2 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 6.84e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 994       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 8.41e+07  |\n",
            "|    reward             | 2382148.8 |\n",
            "|    std                | 0.964     |\n",
            "|    value_loss         | 5.93e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2499093.4284024034\n",
            "Sharpe:  0.9916944529842658\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 990       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 5.21e+07  |\n",
            "|    reward             | 1581722.5 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 2.57e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 988       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 7.79e+07  |\n",
            "|    reward             | 1938802.0 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 4.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 989       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 1.03e+08  |\n",
            "|    reward             | 2773661.5 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 8.42e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2591176.620543975\n",
            "Sharpe:  1.0274753061599657\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 985       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 4.53e+07  |\n",
            "|    reward             | 1199292.2 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 5.61e+07  |\n",
            "|    reward             | 1572490.2 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 2.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 985       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 7.26e+07  |\n",
            "|    reward             | 1887091.6 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 3.73e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 7.24e+07  |\n",
            "|    reward             | 2173468.5 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 5.11e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2332813.9802212366\n",
            "Sharpe:  0.9507315482888173\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 4.34e+07  |\n",
            "|    reward             | 1304513.1 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 985       |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 6.92e+07  |\n",
            "|    reward             | 1670553.5 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 2.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 1.06e+08  |\n",
            "|    reward             | 2641980.2 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 7.5e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2390166.5942882034\n",
            "Sharpe:  0.9714752815703502\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 3.51e+07  |\n",
            "|    reward             | 1019034.2 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 5.77e+07  |\n",
            "|    reward             | 1626770.8 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 3.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 988       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 7.98e+07  |\n",
            "|    reward             | 1932842.0 |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 3.89e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 989       |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 8.11e+07  |\n",
            "|    reward             | 2548565.8 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 7.05e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2391367.085847914\n",
            "Sharpe:  0.9593559744905314\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 3.69e+07  |\n",
            "|    reward             | 1190541.2 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 1.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 989       |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 6.04e+07  |\n",
            "|    reward             | 1611306.1 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 2.84e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 990       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 9.28e+07  |\n",
            "|    reward             | 2342591.8 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 5.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 991       |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 7.88e+07  |\n",
            "|    reward             | 2500867.5 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 6.59e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2700715.4422012307\n",
            "Sharpe:  1.1009426667969475\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 978       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 4.95e+07  |\n",
            "|    reward             | 1388382.1 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 1.96e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 980       |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 6.18e+07  |\n",
            "|    reward             | 1615274.2 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 2.81e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 981       |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 7.65e+07  |\n",
            "|    reward             | 2230372.0 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 5.18e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2004504.2665706826\n",
            "Sharpe:  0.7996193610188354\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 980       |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 3.58e+07  |\n",
            "|    reward             | 1152074.6 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 1.38e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 981       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 6.75e+07  |\n",
            "|    reward             | 1677524.8 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 3.13e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 981       |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 6.2e+07   |\n",
            "|    reward             | 1960240.9 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 3.88e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 983       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 7.33e+07  |\n",
            "|    reward             | 2441583.0 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 5.94e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2421247.649422568\n",
            "Sharpe:  1.0013002439696854\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 982       |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 4.56e+07  |\n",
            "|    reward             | 1300400.1 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 1.8e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 983       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 6.6e+07   |\n",
            "|    reward             | 1582731.4 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 2.68e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 8.58e+07  |\n",
            "|    reward             | 2481389.8 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 6.47e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 985       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 8.32e+07  |\n",
            "|    reward             | 2294219.2 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 5.61e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2352369.161392976\n",
            "Sharpe:  0.9821106311091236\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 983       |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 6.49e+07  |\n",
            "|    reward             | 1619114.4 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 2.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 5.66e+07  |\n",
            "|    reward             | 1636238.6 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 985       |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 8.12e+07  |\n",
            "|    reward             | 2226663.0 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 5.16e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1975189.9107936393\n",
            "Sharpe:  0.7879688864431422\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 4.03e+07  |\n",
            "|    reward             | 1204224.6 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 1.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 984       |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 5.2e+07   |\n",
            "|    reward             | 1595556.0 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 2.86e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 6.84e+07  |\n",
            "|    reward             | 2045540.2 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 4.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 7.43e+07  |\n",
            "|    reward             | 2135551.0 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 4.64e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2272575.45963205\n",
            "Sharpe:  0.9138442562469479\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 5.25e+07  |\n",
            "|    reward             | 1378796.2 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 1.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 6.07e+07  |\n",
            "|    reward             | 1694188.4 |\n",
            "|    std                | 0.94      |\n",
            "|    value_loss         | 3.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 47        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 9.35e+07  |\n",
            "|    reward             | 2432578.8 |\n",
            "|    std                | 0.94      |\n",
            "|    value_loss         | 6.45e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2090295.2877517133\n",
            "Sharpe:  0.8479328704622618\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 47        |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 3.78e+07  |\n",
            "|    reward             | 1084011.1 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 1.26e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | 5.77e+07  |\n",
            "|    reward             | 1710934.5 |\n",
            "|    std                | 0.938     |\n",
            "|    value_loss         | 3.08e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 6.64e+07  |\n",
            "|    reward             | 1909918.4 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 3.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 988       |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 1.09e+08  |\n",
            "|    reward             | 2624763.5 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 7.82e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2492680.071019361\n",
            "Sharpe:  1.0542640702911683\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 987       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 3.83e+07  |\n",
            "|    reward             | 1222248.9 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 986       |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 7.03e+07  |\n",
            "|    reward             | 1508630.8 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 2.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 988       |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 6.77e+07  |\n",
            "|    reward             | 2238400.2 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 4.87e+13  |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                                tb_log_name='a2c',\n",
        "                                total_timesteps=50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "cfhURI26P_Nr",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "trained_a2c.save('trained_models/a2c.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvrqTro3lhAh"
      },
      "source": [
        "### Model 2: **PPO**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVXta7jVKMhV",
        "outputId": "eab95391-634c-4f4f-8136-f05ec33611db",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.005,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5XlUIszKUGx",
        "outputId": "750155ee-fd20-4be5-f810-5321d1254cbb",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3094027.6597703025\n",
            "Sharpe:  1.0800524793526054\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 2274      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 0         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 1272788.6 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2823765.8780237655\n",
            "Sharpe:  1.0190661908886416\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2066      |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 1         |\n",
            "|    total_timesteps      | 4096      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.74e+14  |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -9.07e-08 |\n",
            "|    reward               | 2015034.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.43e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2799126.5482112616\n",
            "Sharpe:  0.9980365972006827\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 2009      |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 3         |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.73e+14  |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -2.12e-07 |\n",
            "|    reward               | 1500731.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.28e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2599514.1486001527\n",
            "Sharpe:  0.9484935236121897\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1984      |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 4         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.91e+14  |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -1.36e-07 |\n",
            "|    reward               | 2088212.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.23e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3255554.3765407167\n",
            "Sharpe:  1.139422189643071\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1971      |\n",
            "|    iterations           | 5         |\n",
            "|    time_elapsed         | 5         |\n",
            "|    total_timesteps      | 10240     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.74e+14  |\n",
            "|    n_updates            | 40        |\n",
            "|    policy_gradient_loss | -1.93e-07 |\n",
            "|    reward               | 2534051.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.22e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2781962.170705028\n",
            "Sharpe:  0.9921075611418272\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1923      |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 6         |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.74e+14  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -1.4e-07  |\n",
            "|    reward               | 3291618.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.58e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2730182.6414978243\n",
            "Sharpe:  0.9723144463567418\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1932      |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 7         |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 8.04e+14  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -3.3e-07  |\n",
            "|    reward               | 2542093.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.53e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2824261.154294285\n",
            "Sharpe:  1.0020625593055896\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3126761.259354827\n",
            "Sharpe:  1.110169448472915\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1916      |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 8         |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.13e+14  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -1.43e-07 |\n",
            "|    reward               | 1182823.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.47e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2856319.3537778235\n",
            "Sharpe:  1.024227441736886\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1918      |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.29e+14  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -7.29e-08 |\n",
            "|    reward               | 1522018.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.39e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2997582.117610216\n",
            "Sharpe:  1.0775078961255213\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1923      |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 10        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.44e+14  |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | -1.79e-07 |\n",
            "|    reward               | 1870417.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.13e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2625299.3642276092\n",
            "Sharpe:  0.9313592127531422\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1928      |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 11        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.63e+14  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -1.32e-07 |\n",
            "|    reward               | 2065913.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.35e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2538399.454157414\n",
            "Sharpe:  0.8849002210698045\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1932      |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 12        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.08e+14  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -1.69e-07 |\n",
            "|    reward               | 2230589.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.16e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3165793.34279057\n",
            "Sharpe:  1.1175428850960678\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1932      |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 13        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.6e+14   |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -1.17e-07 |\n",
            "|    reward               | 2631952.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.27e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2463824.8532313327\n",
            "Sharpe:  0.875189885527691\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1934      |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 14        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.32e+14  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -2.29e-07 |\n",
            "|    reward               | 2551670.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.46e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2616747.4257687083\n",
            "Sharpe:  0.9593830626226811\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2594857.0618334524\n",
            "Sharpe:  0.9337970198332812\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1925      |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.49e+14  |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -2.48e-07 |\n",
            "|    reward               | 1063355.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.26e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2710494.946567128\n",
            "Sharpe:  0.9694156821095474\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1924      |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 17        |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.1e+14   |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -2.11e-07 |\n",
            "|    reward               | 1296314.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.2e+15   |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2514146.010258251\n",
            "Sharpe:  0.8874935039526278\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1922      |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 18        |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.79e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.67e+14  |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | -1.71e-07 |\n",
            "|    reward               | 1935106.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.16e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3120707.939651771\n",
            "Sharpe:  1.1154867515187799\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1916      |\n",
            "|    iterations           | 18        |\n",
            "|    time_elapsed         | 19        |\n",
            "|    total_timesteps      | 36864     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.69e+14  |\n",
            "|    n_updates            | 170       |\n",
            "|    policy_gradient_loss | -2.72e-07 |\n",
            "|    reward               | 1681779.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.15e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2575221.4730247315\n",
            "Sharpe:  0.9025832297771937\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1914      |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 20        |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.25e+14  |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | -2.13e-07 |\n",
            "|    reward               | 1942713.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.51e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2534137.730437304\n",
            "Sharpe:  0.9236286260410642\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1915      |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 21        |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.81e+14  |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -1.92e-07 |\n",
            "|    reward               | 2899531.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.16e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2849823.5840677326\n",
            "Sharpe:  1.032662255638605\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1905      |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 22        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.12e+14  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -1.42e-07 |\n",
            "|    reward               | 2921553.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.25e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2533935.1388776\n",
            "Sharpe:  0.9204879747959338\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1905      |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 23        |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.96e+14  |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | -2.12e-07 |\n",
            "|    reward               | 2482258.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.45e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2703548.8004901274\n",
            "Sharpe:  0.9700602763144085\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2972574.083037996\n",
            "Sharpe:  1.0511695156064325\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1901      |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 24        |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7e+14     |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -1.12e-07 |\n",
            "|    reward               | 1199805.6 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.35e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2982459.1327951397\n",
            "Sharpe:  1.0781818972603725\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1901      |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.45e+14  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -1.91e-07 |\n",
            "|    reward               | 1651034.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.34e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2834042.1911928337\n",
            "Sharpe:  0.9969163385430393\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1903      |\n",
            "|    iterations           | 25        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 51200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.84e+14  |\n",
            "|    n_updates            | 240       |\n",
            "|    policy_gradient_loss | -1.35e-07 |\n",
            "|    reward               | 1742900.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.39e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2870299.920932156\n",
            "Sharpe:  1.0094712541256932\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1906      |\n",
            "|    iterations           | 26        |\n",
            "|    time_elapsed         | 27        |\n",
            "|    total_timesteps      | 53248     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.27e+14  |\n",
            "|    n_updates            | 250       |\n",
            "|    policy_gradient_loss | -1.9e-07  |\n",
            "|    reward               | 1673457.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.34e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2274227.663939534\n",
            "Sharpe:  0.824365608116869\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1906      |\n",
            "|    iterations           | 27        |\n",
            "|    time_elapsed         | 29        |\n",
            "|    total_timesteps      | 55296     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.37e+14  |\n",
            "|    n_updates            | 260       |\n",
            "|    policy_gradient_loss | -1.58e-07 |\n",
            "|    reward               | 2185986.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.26e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2804908.100002025\n",
            "Sharpe:  0.9940693674040527\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1908      |\n",
            "|    iterations           | 28        |\n",
            "|    time_elapsed         | 30        |\n",
            "|    total_timesteps      | 57344     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.11e+14  |\n",
            "|    n_updates            | 270       |\n",
            "|    policy_gradient_loss | -1.12e-07 |\n",
            "|    reward               | 3220113.2 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.02e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2817915.893609797\n",
            "Sharpe:  1.011938499475525\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1909      |\n",
            "|    iterations           | 29        |\n",
            "|    time_elapsed         | 31        |\n",
            "|    total_timesteps      | 59392     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.7e+14   |\n",
            "|    n_updates            | 280       |\n",
            "|    policy_gradient_loss | -2.03e-07 |\n",
            "|    reward               | 2644617.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.44e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2637194.311747985\n",
            "Sharpe:  0.9384723916253612\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2935473.223260315\n",
            "Sharpe:  1.0562875048496463\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1907      |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 32        |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.88e+14  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | -3.24e-07 |\n",
            "|    reward               | 1112773.9 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.43e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3055171.8631587937\n",
            "Sharpe:  1.085169471399678\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1906      |\n",
            "|    iterations           | 31        |\n",
            "|    time_elapsed         | 33        |\n",
            "|    total_timesteps      | 63488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.4e+14   |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | -2.81e-07 |\n",
            "|    reward               | 1300586.0 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.38e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2415206.4510924746\n",
            "Sharpe:  0.8642994364060407\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1905      |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 34        |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.03e+14  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -1.66e-07 |\n",
            "|    reward               | 1875728.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.38e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2768415.5940126036\n",
            "Sharpe:  0.992089637911742\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1906      |\n",
            "|    iterations           | 33        |\n",
            "|    time_elapsed         | 35        |\n",
            "|    total_timesteps      | 67584     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.44e+14  |\n",
            "|    n_updates            | 320       |\n",
            "|    policy_gradient_loss | -2.17e-07 |\n",
            "|    reward               | 1865829.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.08e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2592570.6476160823\n",
            "Sharpe:  0.9301429977778598\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1907      |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 36        |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.5e+14   |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | -3.83e-07 |\n",
            "|    reward               | 1922666.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.37e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3055717.066118363\n",
            "Sharpe:  1.0652284016905464\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1905      |\n",
            "|    iterations           | 35        |\n",
            "|    time_elapsed         | 37        |\n",
            "|    total_timesteps      | 71680     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.85e+14  |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | -1.88e-07 |\n",
            "|    reward               | 3266753.8 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.23e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2722163.2705626907\n",
            "Sharpe:  0.9622668724707973\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1895      |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 38        |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.88e+14  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | -1.97e-07 |\n",
            "|    reward               | 2956266.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.54e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2777579.5946493805\n",
            "Sharpe:  0.9700677768803992\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1888      |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 40        |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.26e+14  |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -1.36e-07 |\n",
            "|    reward               | 2523119.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.51e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2518808.5313158766\n",
            "Sharpe:  0.9045379596928067\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2884996.098787516\n",
            "Sharpe:  1.0168485992632788\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1874      |\n",
            "|    iterations           | 38        |\n",
            "|    time_elapsed         | 41        |\n",
            "|    total_timesteps      | 77824     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 5.38e+14  |\n",
            "|    n_updates            | 370       |\n",
            "|    policy_gradient_loss | -3.44e-07 |\n",
            "|    reward               | 1219649.4 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.22e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3085641.5644450737\n",
            "Sharpe:  1.0754917602522758\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1864      |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 42        |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 6.06e+14  |\n",
            "|    n_updates            | 380       |\n",
            "|    policy_gradient_loss | -9.18e-08 |\n",
            "|    reward               | 1697367.5 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.27e+15  |\n",
            "---------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2793391.4681958845\n",
            "Sharpe:  1.0031754798349801\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1860      |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 44        |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 7.49e+14  |\n",
            "|    n_updates            | 390       |\n",
            "|    policy_gradient_loss | -2.31e-07 |\n",
            "|    reward               | 1766998.1 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 1.53e+15  |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "zmMmk1amUlEm",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "trained_ppo.save('trained_models/ppo.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Iuv554xYFH"
      },
      "source": [
        "### Model 3: **DDPG**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojmppgo4LPLz",
        "outputId": "2fb4d362-925b-48e5-caf1-bfdddb4d3458",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "DDPG_PARAMS = {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.001}\n",
        "\n",
        "\n",
        "model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWt6BIR0LT25",
        "outputId": "797bd696-c355-41ad-a89d-44295a6fb974",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_ddpg \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_model(model\u001b[38;5;241m=\u001b[39mmodel_ddpg,\n\u001b[1;32m      2\u001b[0m                              tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddpg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                              total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    118\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    119\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    120\u001b[0m         callback\u001b[38;5;241m=\u001b[39mTensorboardCallback(),\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    124\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    125\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    126\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    127\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    128\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    129\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    130\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    223\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    224\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    225\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    226\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    227\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    228\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    229\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, gradient_steps\u001b[38;5;241m=\u001b[39mgradient_steps)\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:202\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 202\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[1;32m    203\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/utils.py:473\u001b[0m, in \u001b[0;36mpolyak_update\u001b[0;34m(params, target_params, tau)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# zip does not raise an exception if length of parameters does not match.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m zip_strict(params, target_params):\n\u001b[0;32m--> 473\u001b[0m         target_param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m tau)\n\u001b[1;32m    474\u001b[0m         th\u001b[38;5;241m.\u001b[39madd(target_param\u001b[38;5;241m.\u001b[39mdata, param\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39mtau, out\u001b[38;5;241m=\u001b[39mtarget_param\u001b[38;5;241m.\u001b[39mdata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4C64xZ1UrQA",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save('trained_models/ddpg.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPEXBcm-uBJo"
      },
      "source": [
        "### Model 4: **SAC**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaWf2QeiLqyO",
        "outputId": "8570c7e7-fe22-423a-f5c8-149c355ab236",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgYVPqtKLvi3",
        "outputId": "83a2a60a-cf75-4eaa-8fea-b24c193f1633",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3654744.626536127\n",
            "Sharpe:  1.146103019070211\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 199       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 7220      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.42e+07 |\n",
            "|    critic_loss     | 2.09e+12  |\n",
            "|    ent_coef        | 0.921     |\n",
            "|    ent_coef_loss   | 10.6      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7119      |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 198       |\n",
            "|    time_elapsed    | 72        |\n",
            "|    total_timesteps | 14440     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.3e+08  |\n",
            "|    critic_loss     | 6.22e+12  |\n",
            "|    ent_coef        | 8.04      |\n",
            "|    ent_coef_loss   | -270      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 14339     |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 198       |\n",
            "|    time_elapsed    | 108       |\n",
            "|    total_timesteps | 21660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+08 |\n",
            "|    critic_loss     | 1e+13     |\n",
            "|    ent_coef        | 70.1      |\n",
            "|    ent_coef_loss   | -552      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 21559     |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 194       |\n",
            "|    time_elapsed    | 148       |\n",
            "|    total_timesteps | 28880     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.96e+08 |\n",
            "|    critic_loss     | 8.69e+12  |\n",
            "|    ent_coef        | 611       |\n",
            "|    ent_coef_loss   | -833      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 28779     |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 196       |\n",
            "|    time_elapsed    | 183       |\n",
            "|    total_timesteps | 36100     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.05e+08 |\n",
            "|    critic_loss     | 8.82e+12  |\n",
            "|    ent_coef        | 5.33e+03  |\n",
            "|    ent_coef_loss   | -1.11e+03 |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 35999     |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 24        |\n",
            "|    fps             | 198       |\n",
            "|    time_elapsed    | 217       |\n",
            "|    total_timesteps | 43320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.23e+08 |\n",
            "|    critic_loss     | 1.01e+13  |\n",
            "|    ent_coef        | 4.66e+04  |\n",
            "|    ent_coef_loss   | -1.39e+03 |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 43219     |\n",
            "|    reward          | 3719214.2 |\n",
            "----------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:3719214.3125598216\n",
            "Sharpe:  1.1565868781979913\n",
            "=================================\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "-nUIx62dUvfF",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "trained_sac.save('trained_models/sac.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iidB5E27dfzh"
      },
      "source": [
        "### Model 5: **TD3**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtRp1mWydkvs",
        "outputId": "7ce328d8-38b1-4552-982d-12472f790b39",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "argM0DtodmNL",
        "outputId": "c8f9b7b4-f84f-474e-8070-df3577bc8f50",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_td3 \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_model(model\u001b[38;5;241m=\u001b[39mmodel_td3,\n\u001b[1;32m      2\u001b[0m                              tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                              total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    118\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    119\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    120\u001b[0m         callback\u001b[38;5;241m=\u001b[39mTensorboardCallback(),\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    223\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    224\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    225\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    226\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    227\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    228\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    229\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, gradient_steps\u001b[38;5;241m=\u001b[39mgradient_steps)\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/td3/td3.py:174\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    171\u001b[0m next_actions \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target(replay_data\u001b[38;5;241m.\u001b[39mnext_observations) \u001b[38;5;241m+\u001b[39m noise)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Compute the next Q-values: min over all critics targets\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(replay_data\u001b[38;5;241m.\u001b[39mnext_observations, next_actions), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    175\u001b[0m next_q_values, _ \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mmin(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m replay_data\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m replay_data\u001b[38;5;241m.\u001b[39mdones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q_values\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py:977\u001b[0m, in \u001b[0;36mContinuousCritic.forward\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    975\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m    976\u001b[0m qvalue_input \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([features, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(q_net(qvalue_input) \u001b[38;5;28;01mfor\u001b[39;00m q_net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_networks)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/policies.py:977\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    975\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m    976\u001b[0m qvalue_input \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([features, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(q_net(qvalue_input) \u001b[38;5;28;01mfor\u001b[39;00m q_net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_networks)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=30000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "nGXdd4uEUzCk",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "trained_td3.save('trained_models/td3.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ma6YpTlnuZ"
      },
      "source": [
        "## Trading\n",
        "Assume that we have $1,000,000 initial capital at 2019-01-01. We use the A2C model to trade Dow jones 30 stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "J_SeNPDwPgHZ"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "\n",
        "trained_a2c = A2C.load(\"trained_models/a2c\")\n",
        "trained_ddpg = DDPG.load(\"trained_models/ddpg\")\n",
        "trained_ppo = PPO.load(\"trained_models/ppo\")\n",
        "trained_td3 = TD3.load(\"trained_models/td3\")\n",
        "trained_sac = SAC.load(\"trained_models/sac\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_drl(df, \n",
        "                        train_start_date, train_end_date, \n",
        "                        trade_start_date, trade_end_date, \n",
        "                        model_name,\n",
        "                        output_return_csv,\n",
        "                        output_action_csv,\n",
        "                        total_timesteps=None):\n",
        "    \"\"\"\n",
        "    Train, save, reload, and predict using DRL models (A2C, PPO, DDPG, SAC, TD3).\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
        "    from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "    from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "\n",
        "    # === Fixed Model Hyperparameters ===\n",
        "    model_configs = {\n",
        "        \"a2c\": {\n",
        "            \"params\": {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"ppo\": {\n",
        "            \"params\": {\"n_steps\": 2048, \"ent_coef\": 0.005, \"learning_rate\": 0.0001, \"batch_size\": 128},\n",
        "            \"timesteps\": 80_000\n",
        "        },\n",
        "        \"ddpg\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.001},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"sac\": {\n",
        "            \"params\": {\"batch_size\": 128, \"buffer_size\": 100000, \"learning_rate\": 0.0003, \"learning_starts\": 100, \"ent_coef\": \"auto_0.1\"},\n",
        "            \"timesteps\": 50_000\n",
        "        },\n",
        "        \"td3\": {\n",
        "            \"params\": {\"batch_size\": 100, \"buffer_size\": 1_000_000, \"learning_rate\": 0.001},\n",
        "            \"timesteps\": 30_000\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # === Mapping model_name to class ===\n",
        "    model_class_map = {\n",
        "        \"a2c\": A2C,\n",
        "        \"ppo\": PPO,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"sac\": SAC,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported. Choose from {list(model_configs.keys())}\")\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "    model_params = config[\"params\"]\n",
        "    timesteps = total_timesteps if total_timesteps else config[\"timesteps\"]\n",
        "\n",
        "    # === Data Split ===\n",
        "    train_data = data_split(df, train_start_date, train_end_date)\n",
        "    trade_data = data_split(df, trade_start_date, trade_end_date)\n",
        "\n",
        "    stock_dimension = len(train_data.tic.unique())\n",
        "    state_space = stock_dimension\n",
        "    print(f\"[INFO] Training {model_name.upper()} | Timesteps: {timesteps}\")\n",
        "\n",
        "    # === Environment Setup ===\n",
        "    env_kwargs = {\n",
        "        \"hmax\": 100,\n",
        "        \"initial_amount\": 1_000_000,\n",
        "        \"transaction_cost_pct\": 0.001,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": INDICATORS,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": 1e-4\n",
        "    }\n",
        "\n",
        "    # === Train Model ===\n",
        "    e_train_gym = StockPortfolioEnv(df=train_data, **env_kwargs)\n",
        "    env_train, _ = e_train_gym.get_sb_env()\n",
        "\n",
        "    agent = DRLAgent(env=env_train)\n",
        "    model = agent.get_model(model_name=model_name, model_kwargs=model_params)\n",
        "    trained_model = agent.train_model(model=model, tb_log_name=model_name, total_timesteps=timesteps)\n",
        "\n",
        "    # === Save Model ===\n",
        "    os.makedirs(\"trained_models\", exist_ok=True)\n",
        "    model_save_path = f\"trained_models/{model_name}.zip\"\n",
        "    trained_model.save(model_save_path)\n",
        "    print(f\"[INFO] Model saved at {model_save_path}\")\n",
        "\n",
        "    # === Reload Model Automatically ===\n",
        "    ReloadedModelClass = model_class_map[model_name]\n",
        "    reloaded_model = ReloadedModelClass.load(model_save_path)\n",
        "    print(f\"[INFO] Model '{model_name.upper()}' reloaded from {model_save_path}\")\n",
        "\n",
        "    # === Predict ===\n",
        "    e_trade_gym = StockPortfolioEnv(df=trade_data, **env_kwargs)\n",
        "    df_daily_return, df_actions = DRLAgent.DRL_prediction(model=reloaded_model, environment=e_trade_gym)\n",
        "\n",
        "    # === Save Outputs ===\n",
        "    df_daily_return.to_csv(output_return_csv, index=False)\n",
        "    df_actions.to_csv(output_action_csv, index=False)\n",
        "\n",
        "    print(f\"[INFO] Outputs saved:\")\n",
        "    print(f\" - Daily Return: {output_return_csv}\")\n",
        "    print(f\" - Actions     : {output_action_csv}\")\n",
        "\n",
        "    return df_daily_return, df_actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Training A2C | Timesteps: 50000\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
            "Using cpu device\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1320      |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 6.96e+07  |\n",
            "|    reward             | 1956946.4 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 4.03e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1235      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 9.01e+07  |\n",
            "|    reward             | 2198705.2 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 5.07e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1156      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 1.34e+08  |\n",
            "|    reward             | 2989904.0 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 9.62e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2695515.571152871\n",
            "Sharpe:  0.9559721696682782\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1095      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 4.31e+07  |\n",
            "|    reward             | 1225301.9 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.58e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1132      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 7.16e+07  |\n",
            "|    reward             | 1803879.8 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 3.38e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1164      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 7.79e+07  |\n",
            "|    reward             | 2433081.0 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 6.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1185      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 1.23e+08  |\n",
            "|    reward             | 2764972.2 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 7.84e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2991147.6887693633\n",
            "Sharpe:  1.098347512623536\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1153      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 6.31e+07  |\n",
            "|    reward             | 1538978.8 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 2.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1171      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 7.39e+07  |\n",
            "|    reward             | 1932248.8 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 4.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1182      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 1.13e+08  |\n",
            "|    reward             | 3016295.0 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 9.41e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2727550.9848130024\n",
            "Sharpe:  1.0168092397319595\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1163      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 4.41e+07  |\n",
            "|    reward             | 1145579.9 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 1.29e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1173      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 5.91e+07  |\n",
            "|    reward             | 1693876.4 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 3.12e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1188      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 6.93e+07  |\n",
            "|    reward             | 1912906.4 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 3.71e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1201      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 9.99e+07  |\n",
            "|    reward             | 2446357.5 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 6.64e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2550330.5095792226\n",
            "Sharpe:  0.9522962539724624\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1195      |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 4.6e+07   |\n",
            "|    reward             | 1260038.2 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 1.72e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1196      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 5.44e+07  |\n",
            "|    reward             | 1578131.8 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 2.64e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1203      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 1.05e+08  |\n",
            "|    reward             | 2581485.8 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 7.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1208      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.09e+08  |\n",
            "|    reward             | 2366887.0 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 6.21e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2448187.547430751\n",
            "Sharpe:  0.8939583889452216\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1199      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 7.24e+07  |\n",
            "|    reward             | 1973605.2 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 4.09e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1205      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 6.31e+07  |\n",
            "|    reward             | 2089562.8 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 4.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1207      |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 1.14e+08  |\n",
            "|    reward             | 3199929.0 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 1.1e+14   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2918823.368011607\n",
            "Sharpe:  1.0472305788284186\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1202      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 4.16e+07  |\n",
            "|    reward             | 1192981.9 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 1.57e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1201      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 5.81e+07  |\n",
            "|    reward             | 1733041.5 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 3.24e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1207      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 9.97e+07  |\n",
            "|    reward             | 2032958.0 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 4.72e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1211      |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 8.33e+07  |\n",
            "|    reward             | 2159291.8 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 5.41e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2355833.604247802\n",
            "Sharpe:  0.9042378825347313\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1203      |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 5.72e+07  |\n",
            "|    reward             | 1477081.1 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.31e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1209      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 7.52e+07  |\n",
            "|    reward             | 2012557.6 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 4.3e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 1.17e+08  |\n",
            "|    reward             | 2992289.2 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 9.34e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2802238.29110522\n",
            "Sharpe:  1.0962540792571296\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1212      |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 3.92e+07  |\n",
            "|    reward             | 1120664.5 |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 1.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1215      |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 6.34e+07  |\n",
            "|    reward             | 1773254.6 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 3.3e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1219      |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 6.28e+07  |\n",
            "|    reward             | 1642842.4 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 2.83e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1224      |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 1.17e+08  |\n",
            "|    reward             | 2648526.0 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 7.83e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2458907.7709156205\n",
            "Sharpe:  0.9435054048951164\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1220      |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 4.36e+07  |\n",
            "|    reward             | 1213067.0 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1223      |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 5.06e+07  |\n",
            "|    reward             | 1601868.0 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 2.65e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1225      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 9.77e+07  |\n",
            "|    reward             | 2429557.2 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 6.6e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1228      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 8.81e+07  |\n",
            "|    reward             | 2520378.0 |\n",
            "|    std                | 0.976     |\n",
            "|    value_loss         | 6.32e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2543197.3002636246\n",
            "Sharpe:  0.9861877553678637\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1225      |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 7.13e+07  |\n",
            "|    reward             | 1597872.2 |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 2.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1228      |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 2.98e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 8.02e+07  |\n",
            "|    reward             | 1969513.2 |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 4.25e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 1.05e+08  |\n",
            "|    reward             | 2910988.8 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 8.91e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2371852.6018364686\n",
            "Sharpe:  0.9564943063780046\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1230      |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 4.21e+07  |\n",
            "|    reward             | 1175952.1 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 1.48e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 5.49e+07  |\n",
            "|    reward             | 1532797.8 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 2.46e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1237      |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 8.17e+07  |\n",
            "|    reward             | 1974866.1 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 4.17e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1240      |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 9.56e+07  |\n",
            "|    reward             | 2355853.8 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 5.64e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2292750.943096275\n",
            "Sharpe:  0.9288757089012761\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1237      |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 4.8e+07   |\n",
            "|    reward             | 1384407.0 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 1.95e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1241      |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 5.89e+07  |\n",
            "|    reward             | 1620675.4 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 2.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1244      |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 8.23e+07  |\n",
            "|    reward             | 2387565.0 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 5.94e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2214565.976101682\n",
            "Sharpe:  0.8774940685649119\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 4.07e+07  |\n",
            "|    reward             | 1068295.8 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 1.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 6.43e+07  |\n",
            "|    reward             | 1740518.6 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 3.31e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1245      |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 7.37e+07  |\n",
            "|    reward             | 1728327.1 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 4.23e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1239      |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 9.95e+07  |\n",
            "|    reward             | 2795877.5 |\n",
            "|    std                | 0.965     |\n",
            "|    value_loss         | 8.58e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2746265.2093302133\n",
            "Sharpe:  1.1104696886586303\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1235      |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 5.39e+07  |\n",
            "|    reward             | 1209032.6 |\n",
            "|    std                | 0.964     |\n",
            "|    value_loss         | 1.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1238      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 5.13e+07  |\n",
            "|    reward             | 1457388.2 |\n",
            "|    std                | 0.964     |\n",
            "|    value_loss         | 2.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1240      |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 7.63e+07  |\n",
            "|    reward             | 2304934.8 |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 5.37e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 7.38e+07  |\n",
            "|    reward             | 2249004.0 |\n",
            "|    std                | 0.962     |\n",
            "|    value_loss         | 5.31e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2360169.740678299\n",
            "Sharpe:  0.9563919627029464\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1239      |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 5.46e+07  |\n",
            "|    reward             | 1490555.1 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 2.28e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1241      |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 5.71e+07  |\n",
            "|    reward             | 1700743.6 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 3.12e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1243      |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 8.92e+07  |\n",
            "|    reward             | 2523365.0 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 6.96e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2329958.0456860885\n",
            "Sharpe:  0.9522424355849163\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1240      |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 4.59e+07  |\n",
            "|    reward             | 1208505.6 |\n",
            "|    std                | 0.959     |\n",
            "|    value_loss         | 1.56e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1241      |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 5.98e+07  |\n",
            "|    reward             | 1642304.9 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 2.82e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 6.15e+07  |\n",
            "|    reward             | 1880365.4 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 3.69e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1243      |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 7.73e+07  |\n",
            "|    reward             | 2109237.8 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 4.81e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2263618.3845607904\n",
            "Sharpe:  0.9390615997602586\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1238      |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 5.63e+07  |\n",
            "|    reward             | 1333324.2 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 1.88e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1238      |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 7.39e+07  |\n",
            "|    reward             | 1639717.0 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 2.84e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1240      |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 8.71e+07  |\n",
            "|    reward             | 2464830.5 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 6.56e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2450815.7526948676\n",
            "Sharpe:  0.9901070721983442\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1238      |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 3.99e+07  |\n",
            "|    reward             | 1019131.3 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 1.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1239      |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 7.11e+07  |\n",
            "|    reward             | 1531020.4 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 2.75e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1241      |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 6.99e+07  |\n",
            "|    reward             | 1838130.4 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 3.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1243      |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 9.14e+07  |\n",
            "|    reward             | 2443958.0 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 6.39e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2299593.4194015604\n",
            "Sharpe:  0.9293944797776926\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1241      |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | 4.67e+07  |\n",
            "|    reward             | 1155365.0 |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 1.39e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 7000      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 5.21e+07  |\n",
            "|    reward             | 1462013.9 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 2.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1243      |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 6.62e+07  |\n",
            "|    reward             | 1858615.9 |\n",
            "|    std                | 0.95      |\n",
            "|    value_loss         | 3.48e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1245      |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 8.43e+07  |\n",
            "|    reward             | 1910729.5 |\n",
            "|    std                | 0.949     |\n",
            "|    value_loss         | 3.9e+13   |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2089646.2600445265\n",
            "Sharpe:  0.8405325355418221\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1242      |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 5.5e+07   |\n",
            "|    reward             | 1513005.1 |\n",
            "|    std                | 0.948     |\n",
            "|    value_loss         | 2.32e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1245      |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 7.52e+07  |\n",
            "|    reward             | 1810493.0 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 3.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1247      |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | 7.82e+07  |\n",
            "|    reward             | 2605916.8 |\n",
            "|    std                | 0.946     |\n",
            "|    value_loss         | 7.11e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2371165.6298812767\n",
            "Sharpe:  0.9710941443915303\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1245      |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 3.58e+07  |\n",
            "|    reward             | 1130690.9 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 1.34e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1225      |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 5.55e+07  |\n",
            "|    reward             | 1593803.6 |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 2.76e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1228      |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 6.56e+07  |\n",
            "|    reward             | 1909642.9 |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 3.6e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1229      |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 7.88e+07  |\n",
            "|    reward             | 2471963.5 |\n",
            "|    std                | 0.943     |\n",
            "|    value_loss         | 6.11e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2447387.2492114897\n",
            "Sharpe:  0.9936728691029186\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1227      |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 3.83e+07  |\n",
            "|    reward             | 1224305.1 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 1.61e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1229      |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 4.75e+07  |\n",
            "|    reward             | 1537050.8 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 2.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1230      |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 8.61e+07  |\n",
            "|    reward             | 2356515.8 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 5.74e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1232      |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 8.54e+07  |\n",
            "|    reward             | 2192535.5 |\n",
            "|    std                | 0.941     |\n",
            "|    value_loss         | 5.15e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2264109.460488781\n",
            "Sharpe:  0.9151383450122856\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1230      |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 6.46e+07  |\n",
            "|    reward             | 1708549.4 |\n",
            "|    std                | 0.939     |\n",
            "|    value_loss         | 3.06e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1231      |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 6.22e+07  |\n",
            "|    reward             | 1832652.8 |\n",
            "|    std                | 0.938     |\n",
            "|    value_loss         | 3.54e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 9.57e+07  |\n",
            "|    reward             | 2696898.0 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 7.54e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2274111.9250795594\n",
            "Sharpe:  0.9112385922899082\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1231      |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 4.47e+07  |\n",
            "|    reward             | 1192232.4 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 1.5e+13   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1231      |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 5.55e+07  |\n",
            "|    reward             | 1500679.1 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 2.53e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1232      |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 6.55e+07  |\n",
            "|    reward             | 1870273.1 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 3.52e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 7.74e+07  |\n",
            "|    reward             | 2027949.1 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 4.12e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2155594.0006703055\n",
            "Sharpe:  0.8493429805477158\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1228      |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 6.17e+07  |\n",
            "|    reward             | 1448646.6 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 2.17e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1229      |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 6.98e+07  |\n",
            "|    reward             | 1833613.4 |\n",
            "|    std                | 0.937     |\n",
            "|    value_loss         | 3.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1231      |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 8.41e+07  |\n",
            "|    reward             | 2372799.8 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 6.14e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2272834.079148228\n",
            "Sharpe:  0.8815823049701986\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1229      |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 3.08e+07  |\n",
            "|    reward             | 1088169.1 |\n",
            "|    std                | 0.936     |\n",
            "|    value_loss         | 1.27e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1230      |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | 5.51e+07  |\n",
            "|    reward             | 1567833.8 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 2.59e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1232      |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 6.67e+07  |\n",
            "|    reward             | 1695114.6 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 2.97e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 8.44e+07  |\n",
            "|    reward             | 2358634.8 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 6.38e+13  |\n",
            "-------------------------------------\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:2169578.850224479\n",
            "Sharpe:  0.8541845789155122\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1232      |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 4.5e+07   |\n",
            "|    reward             | 1254118.0 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 1.62e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1233      |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 4.48e+07  |\n",
            "|    reward             | 1443296.0 |\n",
            "|    std                | 0.934     |\n",
            "|    value_loss         | 2.16e+13  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1234      |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0002    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 8.12e+07  |\n",
            "|    reward             | 2228659.5 |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 4.88e+13  |\n",
            "-------------------------------------\n",
            "[INFO] Model saved at trained_models/a2c.zip\n",
            "[INFO] Model 'A2C' reloaded from trained_models/a2c.zip\n",
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1236320.178322115\n",
            "Sharpe:  0.9852691391184168\n",
            "=================================\n",
            "hit end!\n",
            "[INFO] Outputs saved:\n",
            " - Daily Return: df_daily_return_1.csv\n",
            " - Actions     : df_actions_1.csv\n"
          ]
        }
      ],
      "source": [
        "df_daily_return_1, df_actions_1 = train_and_predict_drl(\n",
        "    df = processed,\n",
        "    train_start_date = TRAIN_START_DATE,\n",
        "    train_end_date = TRAIN_END_DATE,\n",
        "    trade_start_date = TRADE_START_DATE,\n",
        "    trade_end_date = TRADE_END_DATE,\n",
        "    model_name = 'a2c',\n",
        "    output_return_csv = 'df_daily_return_1.csv',\n",
        "    output_action_csv = 'df_actions_1.csv'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agg</th>\n",
              "      <th>bil</th>\n",
              "      <th>btcusd</th>\n",
              "      <th>gld</th>\n",
              "      <th>spy</th>\n",
              "      <th>vb</th>\n",
              "      <th>vnq</th>\n",
              "      <th>vo</th>\n",
              "      <th>vwo</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-04-05</th>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-06</th>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.063995</td>\n",
              "      <td>0.130088</td>\n",
              "      <td>0.074334</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.076903</td>\n",
              "      <td>0.063995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-10</th>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.063995</td>\n",
              "      <td>0.130088</td>\n",
              "      <td>0.074334</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.076903</td>\n",
              "      <td>0.063995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-11</th>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.063995</td>\n",
              "      <td>0.130088</td>\n",
              "      <td>0.074334</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.173956</td>\n",
              "      <td>0.076903</td>\n",
              "      <td>0.063995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-12</th>\n",
              "      <td>0.173322</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>0.063761</td>\n",
              "      <td>0.132204</td>\n",
              "      <td>0.073280</td>\n",
              "      <td>0.173322</td>\n",
              "      <td>0.173322</td>\n",
              "      <td>0.077039</td>\n",
              "      <td>0.063761</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 agg       bil    btcusd       gld       spy        vb  \\\n",
              "date                                                                     \n",
              "2023-04-05  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
              "2023-04-06  0.173956  0.068817  0.063995  0.130088  0.074334  0.173956   \n",
              "2023-04-10  0.173956  0.068817  0.063995  0.130088  0.074334  0.173956   \n",
              "2023-04-11  0.173956  0.068817  0.063995  0.130088  0.074334  0.173956   \n",
              "2023-04-12  0.173322  0.069989  0.063761  0.132204  0.073280  0.173322   \n",
              "\n",
              "                 vnq        vo       vwo  \n",
              "date                                      \n",
              "2023-04-05  0.111111  0.111111  0.111111  \n",
              "2023-04-06  0.173956  0.076903  0.063995  \n",
              "2023-04-10  0.173956  0.076903  0.063995  \n",
              "2023-04-11  0.173956  0.076903  0.063995  \n",
              "2023-04-12  0.173322  0.077039  0.063761  "
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_daily_return_1.head()\n",
        "df_actions_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "CmEywyFYrlGA"
      },
      "outputs": [],
      "source": [
        "# trade = data_split(df, TRADE_START_DATE, TRADE_END_DATE)\n",
        "e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1194539.9683249586\n",
            "Sharpe:  0.9781944121670434\n",
            "=================================\n",
            "hit end!\n"
          ]
        }
      ],
      "source": [
        "df_daily_return_1, df_actions_1 = DRLAgent.DRL_prediction(model=trained_a2c,\n",
        "                        environment = e_trade_gym)\n",
        "\n",
        "df_daily_return_1.to_csv('df_daily_return_1_ori.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGYlhyal205",
        "outputId": "2c50560a-af3a-4d6c-c874-19fd1ee38520",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4545, 18)"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trade.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq4W9FbSstT7",
        "outputId": "9ccaad67-a171-45cb-fa86-8054f6015a42",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1371133.7426636913\n",
            "Sharpe:  1.211784917270324\n",
            "=================================\n",
            "hit end!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1389576.5969557953\n",
            "Sharpe:  1.3415812321615228\n",
            "=================================\n",
            "hit end!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1313117.5508216247\n",
            "Sharpe:  1.1890414136527794\n",
            "=================================\n",
            "hit end!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1409861.4051000425\n",
            "Sharpe:  1.38241719833914\n",
            "=================================\n",
            "hit end!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\n",
            "begin_total_asset:1000000\n",
            "end_total_asset:1401228.280985471\n",
            "Sharpe:  1.4658790003944726\n",
            "=================================\n",
            "hit end!\n"
          ]
        }
      ],
      "source": [
        "df_daily_return_1, df_actions_1 = DRLAgent.DRL_prediction(model=trained_a2c,\n",
        "                        environment = e_trade_gym)\n",
        "df_daily_return_2, df_actions_2 = DRLAgent.DRL_prediction(model=trained_ddpg,\n",
        "                        environment = e_trade_gym)\n",
        "df_daily_return_3, df_actions_3 = DRLAgent.DRL_prediction(model=trained_ppo,\n",
        "                        environment = e_trade_gym)\n",
        "df_daily_return_4, df_actions_4 = DRLAgent.DRL_prediction(model=trained_sac,\n",
        "                        environment = e_trade_gym)\n",
        "df_daily_return_5, df_actions_5 = DRLAgent.DRL_prediction(model=trained_td3,\n",
        "                        environment = e_trade_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uJvj3pXt_Ukg",
        "outputId": "a48972fd-f0dc-4df0-b3d7-54f86a56b4d6",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>daily_return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-04-05</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-04-06</td>\n",
              "      <td>0.000290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-04-10</td>\n",
              "      <td>0.010468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-04-11</td>\n",
              "      <td>0.006661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-04-12</td>\n",
              "      <td>-0.003919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  daily_return\n",
              "0 2023-04-05      0.000000\n",
              "1 2023-04-06      0.000290\n",
              "2 2023-04-10      0.010468\n",
              "3 2023-04-11      0.006661\n",
              "4 2023-04-12     -0.003919"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_daily_return_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "vXMMG_9SdKTu",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "df_daily_return_1.to_csv('df_daily_return_1.csv')\n",
        "df_daily_return_2.to_csv('df_daily_return_2.csv')\n",
        "df_daily_return_3.to_csv('df_daily_return_3.csv')\n",
        "df_daily_return_4.to_csv('df_daily_return_4.csv')\n",
        "df_daily_return_5.to_csv('df_daily_return_5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "tByVcZ2L9TAJ",
        "outputId": "38f20b3b-8c36-49ea-bec2-9e630b0a2245",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agg</th>\n",
              "      <th>bil</th>\n",
              "      <th>btcusd</th>\n",
              "      <th>gld</th>\n",
              "      <th>spy</th>\n",
              "      <th>vb</th>\n",
              "      <th>vnq</th>\n",
              "      <th>vo</th>\n",
              "      <th>vwo</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-04-05</th>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-06</th>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-10</th>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-11</th>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-04-12</th>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.171251</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.063000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 agg       bil    btcusd       gld       spy        vb  \\\n",
              "date                                                                     \n",
              "2023-04-05  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
              "2023-04-06  0.171251  0.171251  0.171251  0.171251  0.063000  0.063000   \n",
              "2023-04-10  0.171251  0.171251  0.171251  0.171251  0.063000  0.063000   \n",
              "2023-04-11  0.171251  0.171251  0.171251  0.171251  0.063000  0.063000   \n",
              "2023-04-12  0.171251  0.171251  0.171251  0.171251  0.063000  0.063000   \n",
              "\n",
              "                 vnq        vo       vwo  \n",
              "date                                      \n",
              "2023-04-05  0.111111  0.111111  0.111111  \n",
              "2023-04-06  0.063000  0.063000  0.063000  \n",
              "2023-04-10  0.063000  0.063000  0.063000  \n",
              "2023-04-11  0.063000  0.063000  0.063000  \n",
              "2023-04-12  0.063000  0.063000  0.063000  "
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_actions_1.head()\n",
        "df_actions_2.head()\n",
        "df_actions_3.head()\n",
        "df_actions_4.head()\n",
        "df_actions_5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "xBX3Y68o1vRG",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "df_actions_1.to_csv('df_actions_1.csv')\n",
        "df_actions_2.to_csv('df_actions_2.csv')\n",
        "df_actions_3.to_csv('df_actions_3.csv')\n",
        "df_actions_4.to_csv('df_actions_4.csv')\n",
        "df_actions_5.to_csv('df_actions_5.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFO42LcomPUT"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAvxipWFmUe8"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oGu3PCa8l6L",
        "outputId": "de715ecb-182c-4fb6-b382-b9b927582fd1",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performance Stats for A2C:\n",
            "\n",
            "Annual return          0.170588\n",
            "Cumulative returns     0.371134\n",
            "Annual volatility      0.137816\n",
            "Sharpe ratio           1.211785\n",
            "Calmar ratio           1.247506\n",
            "Stability              0.918375\n",
            "Max drawdown          -0.136743\n",
            "Omega ratio            1.229136\n",
            "Sortino ratio          1.872602\n",
            "Skew                   0.489047\n",
            "Kurtosis               5.805193\n",
            "Tail ratio             1.222568\n",
            "Daily value at risk   -0.016700\n",
            "dtype: float64\n",
            "\n",
            "Performance Stats for DDPG:\n",
            "\n",
            "Annual return          0.178419\n",
            "Cumulative returns     0.389577\n",
            "Annual volatility      0.128544\n",
            "Sharpe ratio           1.341581\n",
            "Calmar ratio           1.585477\n",
            "Stability              0.927624\n",
            "Max drawdown          -0.112533\n",
            "Omega ratio            1.254921\n",
            "Sortino ratio          2.076112\n",
            "Skew                   0.387732\n",
            "Kurtosis               4.785854\n",
            "Tail ratio             1.164639\n",
            "Daily value at risk   -0.015511\n",
            "dtype: float64\n",
            "\n",
            "Performance Stats for PPO:\n",
            "\n",
            "Annual return          0.145604\n",
            "Cumulative returns     0.313118\n",
            "Annual volatility      0.120420\n",
            "Sharpe ratio           1.189041\n",
            "Calmar ratio           1.206107\n",
            "Stability              0.920170\n",
            "Max drawdown          -0.120723\n",
            "Omega ratio            1.226571\n",
            "Sortino ratio          1.816508\n",
            "Skew                   0.509924\n",
            "Kurtosis               7.408333\n",
            "Tail ratio             1.164709\n",
            "Daily value at risk   -0.014603\n",
            "dtype: float64\n",
            "\n",
            "Performance Stats for SAC:\n",
            "\n",
            "Annual return          0.186972\n",
            "Cumulative returns     0.409861\n",
            "Annual volatility      0.130130\n",
            "Sharpe ratio           1.382417\n",
            "Calmar ratio           1.758296\n",
            "Stability              0.930533\n",
            "Max drawdown          -0.106337\n",
            "Omega ratio            1.262939\n",
            "Sortino ratio          2.145947\n",
            "Skew                   0.352162\n",
            "Kurtosis               4.136835\n",
            "Tail ratio             1.097981\n",
            "Daily value at risk   -0.015681\n",
            "dtype: float64\n",
            "\n",
            "Performance Stats for TD3:\n",
            "\n",
            "Annual return          0.183340\n",
            "Cumulative returns     0.401228\n",
            "Annual volatility      0.119747\n",
            "Sharpe ratio           1.465879\n",
            "Calmar ratio           2.001659\n",
            "Stability              0.931883\n",
            "Max drawdown          -0.091594\n",
            "Omega ratio            1.279267\n",
            "Sortino ratio          2.302347\n",
            "Skew                   0.370690\n",
            "Kurtosis               3.450950\n",
            "Tail ratio             1.097194\n",
            "Daily value at risk   -0.014390\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from pyfolio import timeseries\n",
        "\n",
        "return_dfs = [df_daily_return_1, df_daily_return_2, df_daily_return_3, df_daily_return_4, df_daily_return_5]\n",
        "agent_names = ['A2C', 'DDPG', 'PPO', 'SAC', 'TD3']\n",
        "\n",
        "for i, df_return in enumerate(return_dfs):\n",
        "    strat = convert_daily_return_to_pyfolio_ts(df_return)\n",
        "    stats = timeseries.perf_stats(strat)\n",
        "    print(f\"\\nPerformance Stats for {agent_names[i]}:\\n\")\n",
        "    print(stats)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
