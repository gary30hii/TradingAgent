{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "## Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPT0ipYE28wL",
        "outputId": "912ac487-d3c8-467f-ef2a-968fa655b206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-28tnnfnd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-28tnnfnd\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-cuh9bhwl/elegantrl_b0c8be6984e34e1f89cfec0ad9a9d653\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-cuh9bhwl/elegantrl_b0c8be6984e34e1f89cfec0ad9a9d653\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# ## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install pandas_market_calendars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')  \n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Enable Inline Plotting (Jupyter)\n",
        "# ===========================\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Custom Imports (model.py)\n",
        "# ===========================\n",
        "sys.path.append(os.path.abspath(\".\"))  \n",
        "from models import DRLEnsembleAgent\n",
        "\n",
        "sys.path.append(\"../FinRL-Library\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "## `process_csv_to_features(csv_path)`\n",
        "\n",
        "Processes financial data from a CSV by adding technical indicators and turbulence features.\n",
        "\n",
        "### **Parameters**\n",
        "- `csv_path` *(str)*: Path to the raw financial data CSV.\n",
        "\n",
        "### **Workflow**\n",
        "1. Load data.\n",
        "2. Identify 5-day and 7-day tickers.\n",
        "3. Apply technical indicators.\n",
        "4. Combine datasets.\n",
        "5. Add turbulence feature.\n",
        "6. Clean `NaN` and infinite values.\n",
        "\n",
        "### **Returns**\n",
        "- `processed` *(DataFrame)*: Feature-enhanced, cleaned DataFrame for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_to_features(csv_path):\n",
        "    # Step 1: Load Data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 3: Apply Technical Indicators\n",
        "    fe_ti = FeatureEngineer(\n",
        "        use_technical_indicator=True,\n",
        "        use_turbulence=False,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    df_5day_full = fe_ti.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe_ti.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 4: Combine and Clean Index\n",
        "    combined_df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    combined_df.index = range(len(combined_df))\n",
        "\n",
        "    # Step 5: Remove dates with only one ticker\n",
        "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "    combined_df = combined_df[combined_df.groupby('date')['date'].transform('count') > 1]\n",
        "    combined_df = combined_df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Apply Turbulence Feature\n",
        "    fe_turb = FeatureEngineer(\n",
        "        use_technical_indicator=False,\n",
        "        use_turbulence=True,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    processed = fe_turb.preprocess_data(combined_df)\n",
        "\n",
        "    # Step 7: Final Cleaning\n",
        "    processed = processed.copy()\n",
        "    processed = processed.fillna(0)\n",
        "    processed = processed.replace(np.inf, 0)\n",
        "\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "## `setup_drl_ensemble_agent(...)`\n",
        "\n",
        "Initializes a `DRLEnsembleAgent` with dynamic environment and parameter settings.\n",
        "\n",
        "### **Key Parameters**\n",
        "- `processed_df` *(DataFrame)*: Data with features for training and trading.\n",
        "- `indicators` *(list)*: Technical indicators used.\n",
        "- `train_start_date`, `train_end_date`: Training period.\n",
        "- `trade_start_date`, `trade_end_date`: Trading period.\n",
        "- `rebalance_window`, `validation_window`: Rebalancing and validation frequency.\n",
        "- `initial_amount`, `transaction_cost`, `hmax`, `reward_scaling`: Trading environment settings.\n",
        "\n",
        "### **Returns**\n",
        "- `agent`: Configured `DRLEnsembleAgent` ready for training and trading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_drl_ensemble_agent(processed_df,  \n",
        "                              indicators, \n",
        "                              train_start_date, \n",
        "                              train_end_date,\n",
        "                              trade_start_date, \n",
        "                              trade_end_date, \n",
        "                              rebalance_window=63, \n",
        "                              validation_window=63, \n",
        "                              initial_amount=1_000_000,\n",
        "                              transaction_cost=0.001,\n",
        "                              hmax=100,\n",
        "                              reward_scaling=1e-4,\n",
        "                              print_verbosity=5):\n",
        "    \"\"\"\n",
        "    Setup DRLEnsembleAgent with flexible date and parameter configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate dynamic parameters\n",
        "    stock_dimension = len(processed_df.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(indicators) * stock_dimension\n",
        "\n",
        "    # 2. Environment configuration\n",
        "    env_kwargs = {\n",
        "        \"hmax\": hmax,\n",
        "        \"initial_amount\": initial_amount,\n",
        "        \"buy_cost_pct\": transaction_cost,\n",
        "        \"sell_cost_pct\": transaction_cost,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": indicators,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": reward_scaling,\n",
        "        \"print_verbosity\": print_verbosity\n",
        "    }\n",
        "\n",
        "    # 3. Initialize DRLEnsembleAgent\n",
        "    agent = DRLEnsembleAgent(\n",
        "        df=processed_df,\n",
        "        train_period=(train_start_date, train_end_date),\n",
        "        val_test_period=(trade_start_date, trade_end_date),\n",
        "        rebalance_window=rebalance_window,\n",
        "        validation_window=validation_window,\n",
        "        **env_kwargs\n",
        "    )\n",
        "\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "## DRL Model Hyperparameters & Training Timesteps\n",
        "\n",
        "Defines hyperparameters for five DRL algorithms and their training timesteps.\n",
        "\n",
        "### **Model Hyperparameters**\n",
        "- **A2C**:  \n",
        "  `n_steps`, `ent_coef`, `learning_rate`\n",
        "\n",
        "- **PPO**:  \n",
        "  `n_steps`, `ent_coef`, `learning_rate`, `batch_size`\n",
        "\n",
        "- **DDPG**:  \n",
        "  `buffer_size`, `learning_rate`, `batch_size`\n",
        "\n",
        "- **SAC**:  \n",
        "  `batch_size`, `buffer_size`, `learning_rate`, `learning_starts`, `ent_coef`\n",
        "\n",
        "- **TD3**:  \n",
        "  `batch_size`, `buffer_size`, `learning_rate`\n",
        "\n",
        "### **Training Timesteps**\n",
        "- Each model: `10,000` timesteps  \n",
        "  *(Defined in `timesteps_dict`)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.005\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.005,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00001,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      \"buffer_size\": 50_000,\n",
        "                      \"learning_rate\": 0.005,\n",
        "                      \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "                      \"batch_size\": 128,\n",
        "                      \"buffer_size\": 100000,\n",
        "                      \"learning_rate\": 0.005,\n",
        "                      \"learning_starts\": 100,\n",
        "                      \"ent_coef\": 0.1,\n",
        "                    }\n",
        "\n",
        "TD3_model_kwargs = {\n",
        "                      \"batch_size\": 100,\n",
        "                      \"buffer_size\": 1000000,\n",
        "                      \"learning_rate\": 0.001\n",
        "                   }\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 30_000,\n",
        "                 'ppo' : 30_000,\n",
        "                 'ddpg' : 30_000,\n",
        "                  'sac' : 30_000,\n",
        "                 'td3' : 30_000,\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "## `run_ensemble_and_generate_daily_return(...)`\n",
        "\n",
        "Executes a DRL Ensemble Strategy, tracks portfolio performance, calculates daily returns, and organizes output files.\n",
        "\n",
        "### **Key Features**\n",
        "- Runs `ensemble_agent` with specified model hyperparameters and timesteps.\n",
        "- Tracks continuous portfolio value across rebalancing periods.\n",
        "- Generates and saves a portfolio value plot.\n",
        "- Calculates daily returns and exports to CSV.\n",
        "- Organizes trained models, logs, and results into a structured folder.\n",
        "\n",
        "### **Parameters**\n",
        "- `ensemble_agent`: Initialized DRLEnsembleAgent.\n",
        "- Model kwargs: `A2C_kwargs`, `PPO_kwargs`, `DDPG_kwargs`, `SAC_kwargs`, `TD3_kwargs`.\n",
        "- `timesteps_dict`: Training timesteps per model.\n",
        "- `processed_df`: Feature-enhanced DataFrame.\n",
        "- Date ranges, rebalancing configs, and file management options.\n",
        "\n",
        "### **Returns**\n",
        "- `df_daily_return` *(DataFrame)*: Daily return series for the portfolio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ensemble_and_generate_daily_return(ensemble_agent, \n",
        "                                            A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, \n",
        "                                            timesteps_dict, \n",
        "                                            processed_df, \n",
        "                                            trade_start_date, trade_end_date, \n",
        "                                            rebalance_window, validation_window, \n",
        "                                            output_csv_name=\"df_daily_return.csv\",\n",
        "                                            initial_fund=1_000_000,\n",
        "                                            original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Runs DRL Ensemble Strategy, tracks continuous portfolio value, \n",
        "    calculates daily returns, saves outputs, and organizes files into a folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "\n",
        "    # === Step 1: Create Folder Based on CSV Name ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]\n",
        "    target_folder = f\"{base_name}\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "        print(f\"[INFO] Created folder: {target_folder}\")\n",
        "\n",
        "    # === Step 2: Run Ensemble Strategy ===\n",
        "    print(\"[INFO] Running Ensemble Strategy...\")\n",
        "    df_summary = ensemble_agent.run_ensemble_strategy(\n",
        "        A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, timesteps_dict\n",
        "    )\n",
        "\n",
        "    # === Step 3: Prepare Trade Dates ===\n",
        "    unique_trade_date = processed_df[\n",
        "        (processed_df.date >= trade_start_date) & (processed_df.date <= trade_end_date)\n",
        "    ].date.unique()\n",
        "\n",
        "    current_value = initial_fund\n",
        "    portfolio_tracking = []\n",
        "    is_first_file = True\n",
        "\n",
        "    rebalance_points = list(range(rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window))\n",
        "\n",
        "    # === Step 4: Track Portfolio Value Across Rebalances ===\n",
        "    for i in rebalance_points:\n",
        "        file_path = f'results/account_value_trade_ensemble_{i}.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            temp = pd.read_csv(file_path)\n",
        "\n",
        "            if is_first_file:\n",
        "                first_date = temp.loc[0, 'date']\n",
        "                original_value = temp.loc[0, 'account_value']\n",
        "                portfolio_tracking.append({\n",
        "                    'date': first_date,\n",
        "                    'portfolio_value': current_value,\n",
        "                    'original_account_value': original_value\n",
        "                })\n",
        "                start_idx = 1\n",
        "                is_first_file = False\n",
        "            else:\n",
        "                start_idx = 1\n",
        "\n",
        "            for idx in range(start_idx, len(temp)):\n",
        "                daily_return = temp.loc[idx, 'daily_return']\n",
        "                date = temp.loc[idx, 'date']\n",
        "                original_value = temp.loc[idx, 'account_value']\n",
        "                if pd.notna(daily_return):\n",
        "                    current_value *= (1 + daily_return)\n",
        "                    portfolio_tracking.append({\n",
        "                        'date': date,\n",
        "                        'portfolio_value': current_value,\n",
        "                        'original_account_value': original_value\n",
        "                    })\n",
        "        else:\n",
        "            print(f\"[Warning] File does not exist: {file_path}\")\n",
        "\n",
        "    df_portfolio = pd.DataFrame(portfolio_tracking)\n",
        "\n",
        "    # === Step 5: Plot Portfolio Value ===\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['portfolio_value'], label='Continuous Portfolio Value')\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['original_account_value'], label='Original (Resetting) Account Value', linestyle='--')\n",
        "    plt.title('Portfolio Value: Continuous vs Original')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Portfolio Value')\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(target_folder, \"portfolio_value_plot.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"[INFO] Portfolio value plot saved to: {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # === Step 6: Calculate Daily Returns ===\n",
        "    df_daily_return = df_portfolio.copy()\n",
        "    df_daily_return[\"daily_return\"] = df_daily_return[\"portfolio_value\"].pct_change()\n",
        "    df_daily_return = df_daily_return.infer_objects(copy=False)\n",
        "    df_daily_return.loc[0, \"daily_return\"] = 0.0\n",
        "    df_daily_return = df_daily_return[[\"date\", \"daily_return\"]]\n",
        "\n",
        "    # === Step 7: Save Daily Return CSV into Folder ===\n",
        "    csv_full_path = os.path.join(target_folder, output_csv_name)\n",
        "    df_daily_return.to_csv(csv_full_path, index=False)\n",
        "    print(f\"[INFO] Daily return saved to: {csv_full_path}\")\n",
        "    \n",
        "    # === Step 8: Merge Trade Action Files ===\n",
        "    print(\"[INFO] Merging trade action files...\")\n",
        "    action_files = sorted(glob.glob(os.path.join(RESULTS_DIR, \"actions_trade_ensemble_*.csv\")))\n",
        "\n",
        "    if action_files:\n",
        "        df_actions_list = [pd.read_csv(f) for f in action_files]\n",
        "        df_actions_merged = pd.concat(df_actions_list, ignore_index=True)\n",
        "\n",
        "        # Save merged actions\n",
        "        actions_output_path = os.path.join(target_folder, \"merged_trade_actions.csv\")\n",
        "        df_actions_merged.to_csv(actions_output_path, index=False)\n",
        "        print(f\"[INFO] Merged trade actions saved to: {actions_output_path}\")\n",
        "\n",
        "        # Optional: Basic Trade Stats\n",
        "        total_trades = (df_actions_merged.drop(columns=['date'], errors='ignore') != 0).sum().sum()\n",
        "        print(f\"[INFO] Total trades executed: {total_trades}\")\n",
        "\n",
        "    else:\n",
        "        print(\"[Warning] No trade action files found to merge.\")\n",
        "\n",
        "    # === Step 9: Move Directories into the Folder ===\n",
        "    dirs_to_move = [TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR]\n",
        "\n",
        "    for dir_path in dirs_to_move:\n",
        "        if os.path.exists(dir_path):\n",
        "            dest_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(dest_path):\n",
        "                shutil.rmtree(dest_path)  \n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[Warning] Directory not found: {dir_path}\")\n",
        "\n",
        "    return df_daily_return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DRL Ensemble Strategy Workflow\n",
        "\n",
        "This part processes datasets, initializes DRL ensemble agents, runs the ensemble trading strategy, and generates daily returns for three different datasets.\n",
        "\n",
        "### **Workflow Overview**\n",
        "For each dataset:\n",
        "1. **Process Data**  \n",
        "   Apply feature engineering using `process_csv_to_features()`.\n",
        "\n",
        "2. **Setup DRL Ensemble Agent**  \n",
        "   Configure the agent with `setup_drl_ensemble_agent()`.\n",
        "\n",
        "3. **Run Ensemble Strategy & Generate Daily Returns**  \n",
        "   Execute `run_ensemble_and_generate_daily_return()` to:\n",
        "   - Train models (A2C, PPO, DDPG, SAC, TD3)\n",
        "   - Track portfolio value\n",
        "   - Calculate and save daily returns\n",
        "   - Organize outputs\n",
        "\n",
        "---\n",
        "\n",
        "### **Datasets Processed**\n",
        "1. `2007-2025_no_crypto.csv`  \n",
        "   - **Train**: 2007-06-01 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "2. `2015-2025_crypto.csv`  \n",
        "   - **Train**: 2015-02-02 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "3. `2015-2025_no_crypto.csv`  \n",
        "   - **Train**: 2015-02-02 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "---\n",
        "\n",
        "### **Outputs**\n",
        "- Daily return CSV: `df_daily_return_ensemble.csv` (saved in dataset-specific folders)\n",
        "- Portfolio value plots\n",
        "- Organized directories for models, logs, and results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_3\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 495        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 21.7       |\n",
            "|    reward             | -0.8168007 |\n",
            "|    std                | 0.986      |\n",
            "|    value_loss         | 5.9        |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 498          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11          |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | -0.227       |\n",
            "|    reward             | -0.121866554 |\n",
            "|    std                | 0.965        |\n",
            "|    value_loss         | 0.594        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 509        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 7.29       |\n",
            "|    reward             | -0.5989505 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 1.02       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -15.5     |\n",
            "|    reward             | -0.352054 |\n",
            "|    std                | 0.953     |\n",
            "|    value_loss         | 2.23      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -0.00013  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -0.0957   |\n",
            "|    reward             | 0.9252755 |\n",
            "|    std                | 0.969     |\n",
            "|    value_loss         | 0.672     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 526        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 2.22e-05   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 1.19       |\n",
            "|    reward             | 0.10235672 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 0.335      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 0.303     |\n",
            "|    reward             | -0.353732 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.0511    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 530        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.0922    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -8.36      |\n",
            "|    reward             | -0.4325208 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.623      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 531       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0109   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -20.8     |\n",
            "|    reward             | 0.8979843 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.63      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 532        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 22.8       |\n",
            "|    reward             | -1.9638222 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 6.59       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 11.3       |\n",
            "|    reward             | -0.8688026 |\n",
            "|    std                | 0.99       |\n",
            "|    value_loss         | 1.56       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -7.89    |\n",
            "|    reward             | 4.147804 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 0.802    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 0.708    |\n",
            "|    reward             | 0.239192 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 25.5     |\n",
            "|    reward             | 2.889898 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 9.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 537      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -7.7     |\n",
            "|    reward             | 1.533696 |\n",
            "|    std                | 0.978    |\n",
            "|    value_loss         | 1.32     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 14.2      |\n",
            "|    reward             | 1.2948056 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 2.17      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -24.6     |\n",
            "|    reward             | 1.1173104 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 6.5       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -48.1     |\n",
            "|    reward             | -0.916458 |\n",
            "|    std                | 0.951     |\n",
            "|    value_loss         | 27.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 10.1     |\n",
            "|    reward             | 0.688108 |\n",
            "|    std                | 0.942    |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -16.5    |\n",
            "|    reward             | 0.067695 |\n",
            "|    std                | 0.988    |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 15.4      |\n",
            "|    reward             | -1.214845 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 2.01      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -8.15     |\n",
            "|    reward             | -1.937468 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.865     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 525      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 1.77     |\n",
            "|    reward             | 1.460909 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 527         |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 22          |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | -12.9       |\n",
            "|    reward             | -0.23364493 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.68        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 10.8      |\n",
            "|    reward             | 0.6573902 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.6       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 530        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -8.92      |\n",
            "|    reward             | 0.40975598 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.47       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 531         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | -17.8       |\n",
            "|    reward             | -0.47627583 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 3.72        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 3.07      |\n",
            "|    reward             | -0.555239 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.11      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 534       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -2.68     |\n",
            "|    reward             | -0.831044 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.181     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -308     |\n",
            "|    reward             | 1.725951 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 713      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -13.2     |\n",
            "|    reward             | -1.672509 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 5.27      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 537        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.2      |\n",
            "|    explained_variance | 0.0501     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -24        |\n",
            "|    reward             | -1.5474641 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 6.09       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 538         |\n",
            "|    iterations         | 3300        |\n",
            "|    time_elapsed       | 30          |\n",
            "|    total_timesteps    | 16500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3299        |\n",
            "|    policy_loss        | 16.7        |\n",
            "|    reward             | -0.22422262 |\n",
            "|    std                | 1.13        |\n",
            "|    value_loss         | 2.75        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | 11.4       |\n",
            "|    reward             | -1.1243768 |\n",
            "|    std                | 1.16       |\n",
            "|    value_loss         | 1.33       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 540       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 11        |\n",
            "|    reward             | -0.270092 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.775     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -12.6     |\n",
            "|    reward             | -0.761469 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.67      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 541      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -2.74    |\n",
            "|    reward             | 0.175749 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 0.365    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 542      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | -54.6    |\n",
            "|    reward             | 1.067534 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 19.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 543       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 25.7      |\n",
            "|    reward             | -0.665514 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 5.9       |\n",
            "-------------------------------------\n",
            "day: 3925, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1546407.99\n",
            "total_reward: 546407.99\n",
            "total_cost: 5669.83\n",
            "total_trades: 18676\n",
            "Sharpe: 0.243\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 543        |\n",
            "|    iterations         | 4000       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 20000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3999       |\n",
            "|    policy_loss        | -16.4      |\n",
            "|    reward             | -0.8133711 |\n",
            "|    std                | 1.24       |\n",
            "|    value_loss         | 5.31       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 544         |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 37          |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | 41.9        |\n",
            "|    reward             | -0.22282177 |\n",
            "|    std                | 1.21        |\n",
            "|    value_loss         | 8.73        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 544         |\n",
            "|    iterations         | 4200        |\n",
            "|    time_elapsed       | 38          |\n",
            "|    total_timesteps    | 21000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4199        |\n",
            "|    policy_loss        | 0.343       |\n",
            "|    reward             | -0.18384004 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 0.621       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 545      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 3.08     |\n",
            "|    reward             | 0.326665 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 0.116    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 4.62      |\n",
            "|    reward             | -1.856652 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 0.387     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 16.4      |\n",
            "|    reward             | 0.537537  |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 2.02      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 20        |\n",
            "|    reward             | -1.136078 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 3.06      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -14.4    |\n",
            "|    reward             | 0.852399 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 6.01     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 548        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 43         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -21.4      |\n",
            "|    reward             | 0.24843898 |\n",
            "|    std                | 1.34       |\n",
            "|    value_loss         | 4.24       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 548       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 3.24      |\n",
            "|    reward             | 0.2566352 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 0.318     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -14       |\n",
            "|    reward             | 0.953469  |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 1.55      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -4.41    |\n",
            "|    reward             | 0.008354 |\n",
            "|    std                | 1.31     |\n",
            "|    value_loss         | 0.836    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 7.94     |\n",
            "|    reward             | 0.126552 |\n",
            "|    std                | 1.33     |\n",
            "|    value_loss         | 0.704    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 14.1      |\n",
            "|    reward             | 0.071653  |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 1.75      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -33.1     |\n",
            "|    reward             | -0.180485 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 7.2       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -9.22     |\n",
            "|    reward             | 0.9983386 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 0.477     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 551        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | -0.000834  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -18.1      |\n",
            "|    reward             | -1.6777499 |\n",
            "|    std                | 1.3        |\n",
            "|    value_loss         | 1.9        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 551         |\n",
            "|    iterations         | 5700        |\n",
            "|    time_elapsed       | 51          |\n",
            "|    total_timesteps    | 28500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.3       |\n",
            "|    explained_variance | 6.57e-05    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5699        |\n",
            "|    policy_loss        | -15.8       |\n",
            "|    reward             | -0.88663244 |\n",
            "|    std                | 1.31        |\n",
            "|    value_loss         | 2           |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 551         |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 52          |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -24.1       |\n",
            "|    reward             | -0.62000847 |\n",
            "|    std                | 1.26        |\n",
            "|    value_loss         | 4.75        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 552       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 53        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -5.66     |\n",
            "|    reward             | -0.421808 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 0.514     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 552      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 54       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 13.1     |\n",
            "|    reward             | 1.09929  |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.11786474794090772\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_3\n",
            "day: 3925, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2282885.53\n",
            "total_reward: 1282885.53\n",
            "total_cost: 2308.62\n",
            "total_trades: 15676\n",
            "Sharpe: 0.438\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 212      |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 15704    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.72     |\n",
            "|    critic_loss     | 5.74     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 15603    |\n",
            "|    reward          | 0.465912 |\n",
            "---------------------------------\n",
            "day: 3925, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2146791.38\n",
            "total_reward: 1146791.38\n",
            "total_cost: 999.00\n",
            "total_trades: 15648\n",
            "Sharpe: 0.393\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.24719137454413548\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_3\n",
            "day: 3925, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1352912.34\n",
            "total_reward: 352912.34\n",
            "total_cost: 999.00\n",
            "total_trades: 15700\n",
            "Sharpe: 0.285\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 205      |\n",
            "|    time_elapsed    | 76       |\n",
            "|    total_timesteps | 15704    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 108      |\n",
            "|    critic_loss     | 844      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15603    |\n",
            "|    reward          | 0.638582 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.28521353502842023\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_3\n",
            "day: 3925, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1272796.81\n",
            "total_reward: 272796.81\n",
            "total_cost: 3998.85\n",
            "total_trades: 15967\n",
            "Sharpe: 0.190\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 160       |\n",
            "|    time_elapsed    | 97        |\n",
            "|    total_timesteps | 15704     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 798       |\n",
            "|    critic_loss     | 542       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 15603     |\n",
            "|    reward          | -0.482899 |\n",
            "----------------------------------\n",
            "day: 3925, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2221366.66\n",
            "total_reward: 1221366.66\n",
            "total_cost: 998.97\n",
            "total_trades: 19625\n",
            "Sharpe: 0.431\n",
            "=================================\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.13255139220388573\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_3\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 619         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.22843514 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 599           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00070836826 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.014        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.934         |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00159      |\n",
            "|    reward               | -0.34813002   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.23          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 596           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00046697923 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | -0.000492     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.23          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00132      |\n",
            "|    reward               | -0.53134125   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.69          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 591          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004807922 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.00504      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.61         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.000835    |\n",
            "|    reward               | -0.36973515  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.82         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 589           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.7636436e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00486       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.55          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000357     |\n",
            "|    reward               | 0.3615897     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.63          |\n",
            "-------------------------------------------\n",
            "day: 3925, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1352073.85\n",
            "total_reward: 352073.85\n",
            "total_cost: 204273.87\n",
            "total_trades: 30530\n",
            "Sharpe: 0.243\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 589          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007325897 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0339      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.547        |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00157     |\n",
            "|    reward               | -0.009741639 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.27         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 592           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.5367848e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0179       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.4           |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000365     |\n",
            "|    reward               | -0.04268294   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.47          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 593           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029282214 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0381        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.62          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00084      |\n",
            "|    reward               | 1.0296894     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.13          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 595         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 30          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000241008 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.00955     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 15.9        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.000611   |\n",
            "|    reward               | 1.2986859   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 594           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.5074255e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0406       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.72          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000457     |\n",
            "|    reward               | -0.23608105   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.29          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 595           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015536131 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0144        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 8.12          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000681     |\n",
            "|    reward               | 1.5085701     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 21.1          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 595           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 41            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013217857 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00986      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.05          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000544     |\n",
            "|    reward               | 0.19105032    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.11          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 596          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001239983 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0103      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 9.48         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000584    |\n",
            "|    reward               | 1.4563123    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 21.9         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 594           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 48            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016789977 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0866       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.944         |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000865     |\n",
            "|    reward               | -0.101578005  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.08          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 593           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 51            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021366007 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0108       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.89          |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000838     |\n",
            "|    reward               | -1.9053968    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.62          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.15693064256494416\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_3\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 533         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 11.5        |\n",
            "|    reward             | -0.86569816 |\n",
            "|    std                | 0.972       |\n",
            "|    value_loss         | 2.07        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 5.58      |\n",
            "|    reward             | 0.1476913 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 0.99      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 516       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 7.45      |\n",
            "|    reward             | -0.854668 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 1.41      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -20.9     |\n",
            "|    reward             | -0.490571 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 3.48      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 523      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -2.05    |\n",
            "|    reward             | 1.429811 |\n",
            "|    std                | 0.979    |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 526      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -5.6     |\n",
            "|    reward             | 0.546672 |\n",
            "|    std                | 0.987    |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -37.1     |\n",
            "|    reward             | -0.042052 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 14.2      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 525          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -0.425       |\n",
            "|    reward             | -0.053332813 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.00616      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 522         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.0491     |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 18.3        |\n",
            "|    reward             | -0.07363554 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.19        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 522        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 14         |\n",
            "|    reward             | -1.5146188 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.99       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 522        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -29.5      |\n",
            "|    reward             | 0.17814368 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 9.68       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 5.15      |\n",
            "|    reward             | -0.349804 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.15      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 522      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -0.44    |\n",
            "|    reward             | 0.998048 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.362    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -8.49     |\n",
            "|    reward             | -0.986163 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 4.52      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -38.8     |\n",
            "|    reward             | -6.437284 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 14.5      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 521          |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.6        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | -7.18        |\n",
            "|    reward             | -0.013505373 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.408        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -4.69      |\n",
            "|    reward             | 0.15662883 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.954      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 519        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -3.61      |\n",
            "|    reward             | 0.95460695 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 0.755      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -10.6      |\n",
            "|    reward             | 0.48403916 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 1          |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -16.2     |\n",
            "|    reward             | -0.403042 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 2.22      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 519      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -3.32    |\n",
            "|    reward             | 0.297324 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.162    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 520      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | -16.2    |\n",
            "|    reward             | 1.478152 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 520      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 12       |\n",
            "|    reward             | 0.218368 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 520        |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | 2.7        |\n",
            "|    reward             | 0.15910439 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.137      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 520      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -17.5    |\n",
            "|    reward             | 0.182879 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 520        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | 4.58       |\n",
            "|    reward             | 0.57479393 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.324      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 520         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | 5.12        |\n",
            "|    reward             | -0.23860891 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 1.59        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 3.64      |\n",
            "|    reward             | -1.349952 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 0.585     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 521      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 5.54     |\n",
            "|    reward             | 0.091535 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.247    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 2.11      |\n",
            "|    reward             | 0.740466  |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.176     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 6.67      |\n",
            "|    reward             | -0.101591 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.521     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0.167     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 57.3      |\n",
            "|    reward             | -1.639991 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 25.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 522         |\n",
            "|    iterations         | 3300        |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 16500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3299        |\n",
            "|    policy_loss        | 7.97        |\n",
            "|    reward             | -0.24738061 |\n",
            "|    std                | 1.19        |\n",
            "|    value_loss         | 0.669       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -4.03     |\n",
            "|    reward             | 0.3394691 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.692     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 18.8      |\n",
            "|    reward             | 0.2605422 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 2.51      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -18.5     |\n",
            "|    reward             | -0.361103 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 1.96      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 523      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -7.4     |\n",
            "|    reward             | 0.042121 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 0.631    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -8.68     |\n",
            "|    reward             | -0.224509 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 1.68      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 13.8      |\n",
            "|    reward             | -0.046752 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 3.12      |\n",
            "-------------------------------------\n",
            "day: 3988, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1988011.48\n",
            "total_reward: 988011.48\n",
            "total_cost: 1551.18\n",
            "total_trades: 15541\n",
            "Sharpe: 0.376\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 4000       |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 20000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | -0.000243  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3999       |\n",
            "|    policy_loss        | -10.5      |\n",
            "|    reward             | 0.50158536 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 1.15       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 523      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | -0.561   |\n",
            "|    reward             | 0.539944 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.175    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -16.6      |\n",
            "|    reward             | -3.7593472 |\n",
            "|    std                | 1.16       |\n",
            "|    value_loss         | 2.61       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 523         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 41          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | -7.61       |\n",
            "|    reward             | -0.06718002 |\n",
            "|    std                | 1.19        |\n",
            "|    value_loss         | 0.568       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 523      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -11.7    |\n",
            "|    reward             | 0.213855 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 0.853    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 7.26     |\n",
            "|    reward             | 0.199897 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 0.622    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 7.61      |\n",
            "|    reward             | 0.139966  |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 0.565     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -23.6     |\n",
            "|    reward             | -0.392374 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 2.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 45         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.0137    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -14.1      |\n",
            "|    reward             | -0.2903511 |\n",
            "|    std                | 1.26       |\n",
            "|    value_loss         | 1.04       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 13.4      |\n",
            "|    reward             | 0.277321  |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 1.41      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 47         |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | 22.3       |\n",
            "|    reward             | -1.1918243 |\n",
            "|    std                | 1.31       |\n",
            "|    value_loss         | 3.14       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 4.17      |\n",
            "|    reward             | 0.8487432 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 0.548     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 525      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 49       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -6.79    |\n",
            "|    reward             | 0.284531 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 0.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -6.12    |\n",
            "|    reward             | 0.342082 |\n",
            "|    std                | 1.36     |\n",
            "|    value_loss         | 0.211    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 51       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 3.68     |\n",
            "|    reward             | 1.594403 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 0.23     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 24.5      |\n",
            "|    reward             | -0.833653 |\n",
            "|    std                | 1.43      |\n",
            "|    value_loss         | 5.2       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 518         |\n",
            "|    iterations         | 5600        |\n",
            "|    time_elapsed       | 54          |\n",
            "|    total_timesteps    | 28000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.8       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5599        |\n",
            "|    policy_loss        | -3.2        |\n",
            "|    reward             | -0.07052852 |\n",
            "|    std                | 1.42        |\n",
            "|    value_loss         | 0.0954      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 518       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 13.2      |\n",
            "|    reward             | 0.5534219 |\n",
            "|    std                | 1.43      |\n",
            "|    value_loss         | 1.37      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 519      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 24.4     |\n",
            "|    reward             | 1.480709 |\n",
            "|    std                | 1.41     |\n",
            "|    value_loss         | 6.1      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 519        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 56         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | 4.35       |\n",
            "|    reward             | -1.1276224 |\n",
            "|    std                | 1.39       |\n",
            "|    value_loss         | 0.644      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 519      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 57       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -49.2    |\n",
            "|    reward             | -0.35248 |\n",
            "|    std                | 1.4      |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.03778913062590933\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_3\n",
            "day: 3988, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1928681.03\n",
            "total_reward: 928681.03\n",
            "total_cost: 998.99\n",
            "total_trades: 23928\n",
            "Sharpe: 0.422\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 178       |\n",
            "|    time_elapsed    | 89        |\n",
            "|    total_timesteps | 15956     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -227      |\n",
            "|    critic_loss     | 5.47      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 15855     |\n",
            "|    reward          | -0.298411 |\n",
            "----------------------------------\n",
            "day: 3988, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1928681.03\n",
            "total_reward: 928681.03\n",
            "total_cost: 998.99\n",
            "total_trades: 23928\n",
            "Sharpe: 0.422\n",
            "=================================\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.1487358949199093\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_3\n",
            "day: 3988, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 980866.88\n",
            "total_reward: -19133.12\n",
            "total_cost: 999.00\n",
            "total_trades: 11964\n",
            "Sharpe: 0.000\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 100      |\n",
            "|    total_timesteps | 15956    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 229      |\n",
            "|    critic_loss     | 1.3e+04  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15855    |\n",
            "|    reward          | 0.176817 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.22296436268521297\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_3\n",
            "day: 3988, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2086465.27\n",
            "total_reward: 1086465.27\n",
            "total_cost: 5364.81\n",
            "total_trades: 12513\n",
            "Sharpe: 0.465\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 165      |\n",
            "|    time_elapsed    | 96       |\n",
            "|    total_timesteps | 15956    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 678      |\n",
            "|    critic_loss     | 184      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 15855    |\n",
            "|    reward          | 0.474346 |\n",
            "---------------------------------\n",
            "day: 3988, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2046619.92\n",
            "total_reward: 1046619.92\n",
            "total_cost: 999.00\n",
            "total_trades: 11964\n",
            "Sharpe: 0.496\n",
            "=================================\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.007565580562589155\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_3\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 625         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.18161719 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 611           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00042574725 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0213       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.03          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00121      |\n",
            "|    reward               | -1.2561983    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.2           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 602          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004044061 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0196      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.58         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00128     |\n",
            "|    reward               | 1.0219247    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 602          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003253314 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00879      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.000697    |\n",
            "|    reward               | -0.18551724  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 601          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012387638 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00255      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 6.91         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    reward               | 0.5453088    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "day: 3988, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1912018.06\n",
            "total_reward: 912018.06\n",
            "total_cost: 213831.27\n",
            "total_trades: 30824\n",
            "Sharpe: 0.419\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030255684 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0111       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2             |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000762     |\n",
            "|    reward               | -1.1320266    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.5           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027893647 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00134      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.2           |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000543     |\n",
            "|    reward               | -0.57986385   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.9          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 601          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003998185 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00604     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.28         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.000927    |\n",
            "|    reward               | 0.52896166   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.45         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 601           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015862138 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00444      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 12.3          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000541     |\n",
            "|    reward               | 0.099330574   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 20.7          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00096209487 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0221       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.58          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00137      |\n",
            "|    reward               | 0.3631864     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.59          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 602          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006781453 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0107      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 7.1          |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    reward               | 0.5106101    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020416378 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0374       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.2           |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00075      |\n",
            "|    reward               | 0.7522514     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.21          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 44            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017743066 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0127       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 8.75          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000801     |\n",
            "|    reward               | 0.18563436    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 19.2          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 601           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 47            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033329215 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000419      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.96          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000838     |\n",
            "|    reward               | 1.2445203     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4             |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 51            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025001712 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0108       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.86          |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    reward               | -1.0665588    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.81          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.10166054607002481\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_3\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 15.5       |\n",
            "|    reward             | -1.1431308 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 2.9        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 543      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 4.04     |\n",
            "|    reward             | 0.216682 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.605    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 6.01        |\n",
            "|    reward             | -0.78248674 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 0.932       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 539       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -20.5     |\n",
            "|    reward             | -0.394708 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 3.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 541      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -2.22    |\n",
            "|    reward             | 1.242699 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 542      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -3.39    |\n",
            "|    reward             | 0.452548 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 542       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -34.2     |\n",
            "|    reward             | -0.023397 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 9.41      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 542       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 3.51      |\n",
            "|    reward             | -0.172989 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 2.13      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 501       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -17.5     |\n",
            "|    reward             | 1.419852  |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 2.6       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 503         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 13.3        |\n",
            "|    reward             | -0.69894207 |\n",
            "|    std                | 1.15        |\n",
            "|    value_loss         | 1.74        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 507         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -17.9       |\n",
            "|    reward             | -0.12056019 |\n",
            "|    std                | 1.16        |\n",
            "|    value_loss         | 2.28        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 510      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -16.8    |\n",
            "|    reward             | -0.77074 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 513      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 6.76     |\n",
            "|    reward             | 0.803043 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.344    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 515        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.396     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -3.8       |\n",
            "|    reward             | 0.60135096 |\n",
            "|    std                | 1.21       |\n",
            "|    value_loss         | 0.283      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 517      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 21.8     |\n",
            "|    reward             | 0.377227 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 3.74     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -12.6     |\n",
            "|    reward             | -0.824673 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 1.28      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 24.9      |\n",
            "|    reward             | -0.378345 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 5.82      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 522        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -2.15      |\n",
            "|    reward             | 0.26707876 |\n",
            "|    std                | 1.24       |\n",
            "|    value_loss         | 0.34       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 9.63      |\n",
            "|    reward             | 0.1409791 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.692     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -5.8      |\n",
            "|    reward             | -0.960421 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 0.845     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 526      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 11.1     |\n",
            "|    reward             | 0.645923 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -2.56     |\n",
            "|    reward             | -0.316647 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.626     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 19.3     |\n",
            "|    reward             | 1.021168 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 4        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 60.9     |\n",
            "|    reward             | 0.214572 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 21.5     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -51.9      |\n",
            "|    reward             | -1.9057102 |\n",
            "|    std                | 1.2        |\n",
            "|    value_loss         | 15.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 530       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 4.26      |\n",
            "|    reward             | 0.1554673 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.194     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 531         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | -5.23       |\n",
            "|    reward             | -0.21326639 |\n",
            "|    std                | 1.19        |\n",
            "|    value_loss         | 0.324       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 531      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -6.51    |\n",
            "|    reward             | 0.341915 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 0.325    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 532      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -24.6    |\n",
            "|    reward             | 1.211021 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 5.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 532      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -7.26    |\n",
            "|    reward             | 0.072361 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 0.617    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -10.9    |\n",
            "|    reward             | 0.033421 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.896    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -24.2    |\n",
            "|    reward             | 1.218791 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 3.36     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 6.9       |\n",
            "|    reward             | 0.5694417 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.582     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | -15.9      |\n",
            "|    reward             | 0.20751373 |\n",
            "|    std                | 1.28       |\n",
            "|    value_loss         | 1.68       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 534       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | -18       |\n",
            "|    reward             | 0.2545069 |\n",
            "|    std                | 1.37      |\n",
            "|    value_loss         | 2.24      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -0.462    |\n",
            "|    reward             | -0.300561 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.0067    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 23.2     |\n",
            "|    reward             | 1.037963 |\n",
            "|    std                | 1.4      |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | -6.28    |\n",
            "|    reward             | 0.491902 |\n",
            "|    std                | 1.41     |\n",
            "|    value_loss         | 0.237    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 2.19     |\n",
            "|    reward             | 0.60039  |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 0.0622   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -18.3     |\n",
            "|    reward             | 0.2952    |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 6.27      |\n",
            "-------------------------------------\n",
            "day: 4051, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1221056.25\n",
            "total_reward: 221056.25\n",
            "total_cost: 1481.87\n",
            "total_trades: 24402\n",
            "Sharpe: 0.170\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | -8.9      |\n",
            "|    reward             | 0.1376444 |\n",
            "|    std                | 1.47      |\n",
            "|    value_loss         | 0.508     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 9.75      |\n",
            "|    reward             | 1.4630051 |\n",
            "|    std                | 1.48      |\n",
            "|    value_loss         | 0.554     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 40          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | 3.36        |\n",
            "|    reward             | -0.20106585 |\n",
            "|    std                | 1.48        |\n",
            "|    value_loss         | 0.0993      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -5.23     |\n",
            "|    reward             | -0.047381 |\n",
            "|    std                | 1.51      |\n",
            "|    value_loss         | 0.177     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 537      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | -0.684   |\n",
            "|    reward             | 0.166428 |\n",
            "|    std                | 1.51     |\n",
            "|    value_loss         | 0.0981   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | -3.75     |\n",
            "|    reward             | -0.507204 |\n",
            "|    std                | 1.52      |\n",
            "|    value_loss         | 0.128     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 32.6      |\n",
            "|    reward             | -1.181694 |\n",
            "|    std                | 1.54      |\n",
            "|    value_loss         | 7         |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 3.31      |\n",
            "|    reward             | -0.365054 |\n",
            "|    std                | 1.52      |\n",
            "|    value_loss         | 0.456     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 4900       |\n",
            "|    time_elapsed       | 45         |\n",
            "|    total_timesteps    | 24500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4899       |\n",
            "|    policy_loss        | 10.3       |\n",
            "|    reward             | -0.7952797 |\n",
            "|    std                | 1.54       |\n",
            "|    value_loss         | 0.7        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 19.2      |\n",
            "|    reward             | 0.2491549 |\n",
            "|    std                | 1.56      |\n",
            "|    value_loss         | 2.46      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 47        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -6.88     |\n",
            "|    reward             | 0.5308053 |\n",
            "|    std                | 1.58      |\n",
            "|    value_loss         | 0.24      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 538      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 0.684    |\n",
            "|    reward             | 0.635122 |\n",
            "|    std                | 1.6      |\n",
            "|    value_loss         | 0.133    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 538       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -0.428    |\n",
            "|    reward             | -0.675679 |\n",
            "|    std                | 1.62      |\n",
            "|    value_loss         | 0.377     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | 3.51       |\n",
            "|    reward             | -1.3064849 |\n",
            "|    std                | 1.66       |\n",
            "|    value_loss         | 0.266      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 51       |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 4.92     |\n",
            "|    reward             | 0.01912  |\n",
            "|    std                | 1.66     |\n",
            "|    value_loss         | 0.168    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 51       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -14.2    |\n",
            "|    reward             | 1.15259  |\n",
            "|    std                | 1.71     |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -15.2      |\n",
            "|    explained_variance | 4.91e-05   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | 22.3       |\n",
            "|    reward             | -0.9122463 |\n",
            "|    std                | 1.71       |\n",
            "|    value_loss         | 2.29       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 539       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 53        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 12.1      |\n",
            "|    reward             | -0.628392 |\n",
            "|    std                | 1.78      |\n",
            "|    value_loss         | 0.637     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 54         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -15.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -35.5      |\n",
            "|    reward             | -0.5209438 |\n",
            "|    std                | 1.8        |\n",
            "|    value_loss         | 7.49       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -24.1    |\n",
            "|    reward             | 0.259478 |\n",
            "|    std                | 1.83     |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.5260733584130266\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_3\n",
            "day: 4051, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2082692.63\n",
            "total_reward: 1082692.63\n",
            "total_cost: 999.00\n",
            "total_trades: 16204\n",
            "Sharpe: 0.368\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 207       |\n",
            "|    time_elapsed    | 78        |\n",
            "|    total_timesteps | 16208     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -122      |\n",
            "|    critic_loss     | 2.07      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16107     |\n",
            "|    reward          | -1.995189 |\n",
            "----------------------------------\n",
            "day: 4051, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2082692.63\n",
            "total_reward: 1082692.63\n",
            "total_cost: 999.00\n",
            "total_trades: 16204\n",
            "Sharpe: 0.368\n",
            "=================================\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.3476757889444622\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_2\n",
            "day: 4051, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2184398.87\n",
            "total_reward: 1184398.87\n",
            "total_cost: 1827.77\n",
            "total_trades: 12210\n",
            "Sharpe: 0.325\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 198       |\n",
            "|    time_elapsed    | 81        |\n",
            "|    total_timesteps | 16208     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 268       |\n",
            "|    critic_loss     | 3.57e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 16107     |\n",
            "|    reward          | -2.466744 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.42599002588114254\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "day: 4051, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1766459.58\n",
            "total_reward: 766459.58\n",
            "total_cost: 5092.05\n",
            "total_trades: 20570\n",
            "Sharpe: 0.318\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 171      |\n",
            "|    time_elapsed    | 94       |\n",
            "|    total_timesteps | 16208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 901      |\n",
            "|    critic_loss     | 938      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16107    |\n",
            "|    reward          | -1.7553  |\n",
            "---------------------------------\n",
            "day: 4051, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1993068.18\n",
            "total_reward: 993068.18\n",
            "total_cost: 998.99\n",
            "total_trades: 20255\n",
            "Sharpe: 0.398\n",
            "=================================\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.4905244516939333\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 625        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.3321608 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 611           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00070985535 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0171       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.94          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00148      |\n",
            "|    reward               | -0.6084861    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.82          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 117           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031287243 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0034       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.15          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00069      |\n",
            "|    reward               | -1.4854579    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 14.2          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 146           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 55            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065337634 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0313       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.75          |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00137      |\n",
            "|    reward               | 0.11170676    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.52          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 172           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 59            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3182114e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00368      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.08          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000209     |\n",
            "|    reward               | -0.11169788   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 19.2          |\n",
            "-------------------------------------------\n",
            "day: 4051, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2411021.45\n",
            "total_reward: 1411021.45\n",
            "total_cost: 216037.33\n",
            "total_trades: 31427\n",
            "Sharpe: 0.460\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 192          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 63           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005522745 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0122      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.54         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00127     |\n",
            "|    reward               | -0.07127263  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.78         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 213          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 67           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.379666e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00479     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 15           |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.000172    |\n",
            "|    reward               | -0.049177695 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 30.5         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 232           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 70            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019179462 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0221       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.51          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000877     |\n",
            "|    reward               | 0.3990174     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.31          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 246           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 74            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044536314 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00073       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 9.03          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000905     |\n",
            "|    reward               | 0.04460766    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 20.7          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 260           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 78            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038434216 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00939      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.36          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00119      |\n",
            "|    reward               | 1.2179167     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.87          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 273          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.292111e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0114      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 5.99         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000368    |\n",
            "|    reward               | -0.036719836 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 285         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 86          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 6.88805e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0306     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.78        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.000527   |\n",
            "|    reward               | -0.28397402 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.25        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 296          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 89           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.852325e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00203     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 12.4         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000331    |\n",
            "|    reward               | 0.3996391    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 21.1         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 307           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 93            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036201338 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00622       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.26          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.0013       |\n",
            "|    reward               | 0.25962064    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.62          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 317           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 96            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019293404 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0096       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 12            |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000792     |\n",
            "|    reward               | -0.18386546   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 19.8          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.45087883618118835\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 547        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 20.2       |\n",
            "|    reward             | -1.2637798 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 4.3        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 532      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 9.05     |\n",
            "|    reward             | 0.194985 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 11.1       |\n",
            "|    reward             | -0.8221899 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 2.25       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 545         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -30.1       |\n",
            "|    reward             | -0.35466838 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 6.72        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 1.42     |\n",
            "|    reward             | 1.41053  |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 1.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -3.91    |\n",
            "|    reward             | 0.544947 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -47.2     |\n",
            "|    reward             | -0.041919 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 15.5      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 546         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -0.00224    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -2.2        |\n",
            "|    reward             | 0.037485827 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 2.88        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 10.9       |\n",
            "|    reward             | 0.77535176 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.63       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -0.211     |\n",
            "|    reward             | 0.17529966 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 0.0302     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0.404      |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 1.51       |\n",
            "|    reward             | 0.23004286 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 0.0769     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -15.3     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 0.564     |\n",
            "|    reward             | 0.5250165 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 0.329     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 28.3     |\n",
            "|    reward             | 0.922935 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 8.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 548      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 31.5     |\n",
            "|    reward             | 0.049622 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 8.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 548      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -2.93    |\n",
            "|    reward             | 2.28275  |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 8.88     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 53.1      |\n",
            "|    reward             | -0.327525 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 33.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 548       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -2.97     |\n",
            "|    reward             | -1.235616 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 0.171     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 548        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 18.5       |\n",
            "|    reward             | -0.7566659 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 2.69       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 4.74     |\n",
            "|    reward             | 0.350847 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 0.57     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 549         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 8.03        |\n",
            "|    reward             | -0.43064827 |\n",
            "|    std                | 1.19        |\n",
            "|    value_loss         | 0.324       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 8.64     |\n",
            "|    reward             | 0.345278 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 0.774    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 0.191    |\n",
            "|    reward             | 0.764142 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 0.0343   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 46.1      |\n",
            "|    reward             | -0.128087 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 13.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 23.3     |\n",
            "|    reward             | 0.217191 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 9.34     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 549        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -26.1      |\n",
            "|    reward             | -0.4809776 |\n",
            "|    std                | 1.32       |\n",
            "|    value_loss         | 4.44       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 549        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -2.29      |\n",
            "|    reward             | 0.22257185 |\n",
            "|    std                | 1.3        |\n",
            "|    value_loss         | 0.0651     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 13500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | 9.89     |\n",
            "|    reward             | 0.486131 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 0.819    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 12.4      |\n",
            "|    reward             | 0.1704387 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 0.745     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -21.3    |\n",
            "|    reward             | 0.509673 |\n",
            "|    std                | 1.35     |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -1.43     |\n",
            "|    reward             | -0.515473 |\n",
            "|    std                | 1.37      |\n",
            "|    value_loss         | 0.206     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -9.55    |\n",
            "|    reward             | 0.200359 |\n",
            "|    std                | 1.35     |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -38.9    |\n",
            "|    reward             | 1.929594 |\n",
            "|    std                | 1.33     |\n",
            "|    value_loss         | 12.9     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.182     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 4.33      |\n",
            "|    reward             | 1.6540302 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.202     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 31.7      |\n",
            "|    reward             | 1.3795617 |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 5.83      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 549        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.00511    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | -32        |\n",
            "|    reward             | -0.9633519 |\n",
            "|    std                | 1.26       |\n",
            "|    value_loss         | 6.45       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.253   |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -8.29    |\n",
            "|    reward             | 0.809078 |\n",
            "|    std                | 1.27     |\n",
            "|    value_loss         | 1.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.00769  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -7.68    |\n",
            "|    reward             | 0.641359 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 3.15     |\n",
            "|    reward             | 0.375027 |\n",
            "|    std                | 1.31     |\n",
            "|    value_loss         | 0.232    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 13.9     |\n",
            "|    reward             | 0.762282 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -20.6    |\n",
            "|    reward             | 0.279813 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 2.2      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 9.88      |\n",
            "|    reward             | -1.737458 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 1.36      |\n",
            "-------------------------------------\n",
            "day: 4114, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1098205.07\n",
            "total_reward: 98205.07\n",
            "total_cost: 14830.24\n",
            "total_trades: 18187\n",
            "Sharpe: 0.132\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | 2.46       |\n",
            "|    reward             | 0.99038696 |\n",
            "|    std                | 1.34       |\n",
            "|    value_loss         | 0.23       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -16.2     |\n",
            "|    reward             | 1.3787922 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 2.3       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 4400       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 22000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4399       |\n",
            "|    policy_loss        | 9.98       |\n",
            "|    reward             | 0.52457464 |\n",
            "|    std                | 1.44       |\n",
            "|    value_loss         | 0.502      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 1.89      |\n",
            "|    reward             | -0.584184 |\n",
            "|    std                | 1.46      |\n",
            "|    value_loss         | 0.0344    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 8.42      |\n",
            "|    reward             | -0.197816 |\n",
            "|    std                | 1.5       |\n",
            "|    value_loss         | 0.657     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 27.4      |\n",
            "|    reward             | -0.418856 |\n",
            "|    std                | 1.52      |\n",
            "|    value_loss         | 4.54      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 551      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 43       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | 12.8     |\n",
            "|    reward             | 1.181572 |\n",
            "|    std                | 1.53     |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 551      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -6.84    |\n",
            "|    reward             | 0.359161 |\n",
            "|    std                | 1.56     |\n",
            "|    value_loss         | 0.77     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -18.9     |\n",
            "|    reward             | 0.5444055 |\n",
            "|    std                | 1.6       |\n",
            "|    value_loss         | 1.6       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 46         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | -27.6      |\n",
            "|    reward             | 0.18283342 |\n",
            "|    std                | 1.59       |\n",
            "|    value_loss         | 3.68       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 47         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | 4.85       |\n",
            "|    reward             | 0.13606314 |\n",
            "|    std                | 1.64       |\n",
            "|    value_loss         | 0.636      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -17.3    |\n",
            "|    reward             | 0.525993 |\n",
            "|    std                | 1.67     |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 4.68      |\n",
            "|    reward             | -0.230612 |\n",
            "|    std                | 1.74      |\n",
            "|    value_loss         | 0.156     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -4.52     |\n",
            "|    reward             | -0.079966 |\n",
            "|    std                | 1.74      |\n",
            "|    value_loss         | 0.133     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 0.418    |\n",
            "|    reward             | 0.135101 |\n",
            "|    std                | 1.68     |\n",
            "|    value_loss         | 0.219    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 51        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -0.634657 |\n",
            "|    std                | 1.65      |\n",
            "|    value_loss         | 1.69      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -28.6     |\n",
            "|    reward             | 3.1038735 |\n",
            "|    std                | 1.67      |\n",
            "|    value_loss         | 3.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 53        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.7     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 4.55      |\n",
            "|    reward             | 1.0995698 |\n",
            "|    std                | 1.66      |\n",
            "|    value_loss         | 0.319     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 549        |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 54         |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 12.3       |\n",
            "|    reward             | 0.84793276 |\n",
            "|    std                | 1.67       |\n",
            "|    value_loss         | 0.596      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.37838309222169425\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 4114, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1939063.85\n",
            "total_reward: 939063.85\n",
            "total_cost: 999.00\n",
            "total_trades: 12342\n",
            "Sharpe: 0.345\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 218      |\n",
            "|    time_elapsed    | 75       |\n",
            "|    total_timesteps | 16460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.3    |\n",
            "|    critic_loss     | 5.79     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16359    |\n",
            "|    reward          | 0.994973 |\n",
            "---------------------------------\n",
            "day: 4114, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1939063.85\n",
            "total_reward: 939063.85\n",
            "total_cost: 999.00\n",
            "total_trades: 12342\n",
            "Sharpe: 0.345\n",
            "=================================\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.37808554260525445\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "day: 4114, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2279367.27\n",
            "total_reward: 1279367.27\n",
            "total_cost: 999.00\n",
            "total_trades: 12342\n",
            "Sharpe: 0.340\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 201      |\n",
            "|    time_elapsed    | 81       |\n",
            "|    total_timesteps | 16460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 297      |\n",
            "|    critic_loss     | 7.04     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16359    |\n",
            "|    reward          | 1.809024 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3953217188970231\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "day: 4114, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1815013.22\n",
            "total_reward: 815013.22\n",
            "total_cost: 4001.88\n",
            "total_trades: 24778\n",
            "Sharpe: 0.374\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 172      |\n",
            "|    time_elapsed    | 95       |\n",
            "|    total_timesteps | 16460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 706      |\n",
            "|    critic_loss     | 322      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16359    |\n",
            "|    reward          | 1.195867 |\n",
            "---------------------------------\n",
            "day: 4114, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2128991.70\n",
            "total_reward: 1128991.70\n",
            "total_cost: 999.00\n",
            "total_trades: 24684\n",
            "Sharpe: 0.383\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.37016243715768865\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 640         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.18648784 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 626          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010091214 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.086       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.954        |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00158     |\n",
            "|    reward               | 0.011891664  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 620           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00040083006 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00339       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.37          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00105      |\n",
            "|    reward               | -0.09003437   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.29          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 622          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007292748 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0405      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.853        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    reward               | 0.00432374   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.82         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 618           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030274846 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00991      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.66          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000587     |\n",
            "|    reward               | 0.9024823     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.32          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 619           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 19            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065955427 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000475      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.46          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00159      |\n",
            "|    reward               | 0.00951246    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.95          |\n",
            "-------------------------------------------\n",
            "day: 4114, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1556997.04\n",
            "total_reward: 556997.04\n",
            "total_cost: 223039.32\n",
            "total_trades: 31739\n",
            "Sharpe: 0.305\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 617           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.8409834e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00169      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.92          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00029      |\n",
            "|    reward               | -0.6648823    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.9           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 617           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 26            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00059282366 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.114        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.831         |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.0013       |\n",
            "|    reward               | -0.20142417   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.83          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 617          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018018275 |\n",
            "|    clip_fraction        | 0.000244     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0272      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.21         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    reward               | -0.68066096  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.85         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 617         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001862739 |\n",
            "|    clip_fraction        | 0.000439    |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00596     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 0.617       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00235    |\n",
            "|    reward               | -0.20266384 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.53        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 616        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 36         |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 8.75e-05   |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.0217    |\n",
            "|    learning_rate        | 1e-05      |\n",
            "|    loss                 | 7.72       |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.000558  |\n",
            "|    reward               | 0.06937196 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 18         |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 617           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 39            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00088652864 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0142        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.03          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00151      |\n",
            "|    reward               | -0.071786985  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.74          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 617           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 43            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014276535 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00106       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 6.62          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000665     |\n",
            "|    reward               | 1.4405944     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 13.3          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 616         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000413843 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00173    |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.84        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.00111    |\n",
            "|    reward               | 1.5136707   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.55        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 616           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 49            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2548555e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00339       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 21            |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000352     |\n",
            "|    reward               | 1.702107      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 31.4          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.525116708221795\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 13.7      |\n",
            "|    reward             | -0.890796 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.68      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 552      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 3.51     |\n",
            "|    reward             | 0.097173 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.396    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 4.52      |\n",
            "|    reward             | -0.506816 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.508     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -15.2     |\n",
            "|    reward             | -0.331194 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.05      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 551      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -0.376   |\n",
            "|    reward             | 0.962356 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.622    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 553       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -0.612    |\n",
            "|    reward             | 0.29109   |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.767     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -27.6     |\n",
            "|    reward             | -0.115034 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 6.4       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 551      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 1.11     |\n",
            "|    reward             | -0.12776 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -4.71     |\n",
            "|    reward             | 0.5973371 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 0.376     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 5.61      |\n",
            "|    reward             | 0.6921243 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.298     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 552        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 1.06       |\n",
            "|    reward             | 0.14787212 |\n",
            "|    std                | 1.25       |\n",
            "|    value_loss         | 0.099      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 551      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 15.2     |\n",
            "|    reward             | 0.144803 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 551       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -1.33     |\n",
            "|    reward             | -0.156533 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.0311    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 1.13      |\n",
            "|    reward             | -0.780381 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.237     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 17.4      |\n",
            "|    reward             | -1.178699 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 2.41      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 547        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -0.000363  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 65.3       |\n",
            "|    reward             | 0.57661045 |\n",
            "|    std                | 1.2        |\n",
            "|    value_loss         | 37         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 547         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -23         |\n",
            "|    reward             | -0.14180224 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 3.54        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 547        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 4.76       |\n",
            "|    reward             | 0.29601118 |\n",
            "|    std                | 1.21       |\n",
            "|    value_loss         | 0.211      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 11.9     |\n",
            "|    reward             | -1.0501  |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.3     |\n",
            "|    reward             | -0.357248 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.795     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 7.72     |\n",
            "|    reward             | 1.97244  |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 0.549    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 16.8     |\n",
            "|    reward             | 0.095452 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 547      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 13.1     |\n",
            "|    reward             | 1.59078  |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | -17       |\n",
            "|    reward             | -0.365726 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 2.91      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 548       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 20.8      |\n",
            "|    reward             | 0.617508  |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 8.71      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 547        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -5.98      |\n",
            "|    reward             | -1.2800548 |\n",
            "|    std                | 1.25       |\n",
            "|    value_loss         | 0.733      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 548         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | 7.63        |\n",
            "|    reward             | 0.107142895 |\n",
            "|    std                | 1.23        |\n",
            "|    value_loss         | 0.526       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 548       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -8.03     |\n",
            "|    reward             | 0.2988542 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 0.521     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 548       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 11        |\n",
            "|    reward             | -0.497537 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -8.98     |\n",
            "|    reward             | -0.117458 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 1.22      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -25.8     |\n",
            "|    reward             | -0.173161 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 5.19      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 549      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -10.5    |\n",
            "|    reward             | 2.214252 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 5.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -45.4    |\n",
            "|    reward             | 2.647411 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 15.6     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 549       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 5.45      |\n",
            "|    reward             | 0.5113556 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.745     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | 15         |\n",
            "|    reward             | 0.29822448 |\n",
            "|    std                | 1.24       |\n",
            "|    value_loss         | 1.97       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 550         |\n",
            "|    iterations         | 3600        |\n",
            "|    time_elapsed       | 32          |\n",
            "|    total_timesteps    | 18000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3599        |\n",
            "|    policy_loss        | 18.5        |\n",
            "|    reward             | -0.05842723 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 3.69        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 7.33     |\n",
            "|    reward             | 0.317243 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.399    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 3800       |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 19000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3799       |\n",
            "|    policy_loss        | -27.7      |\n",
            "|    reward             | 0.21275263 |\n",
            "|    std                | 1.28       |\n",
            "|    value_loss         | 5.1        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -0.459    |\n",
            "|    reward             | -1.111824 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.0951    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 8.81      |\n",
            "|    reward             | 0.279316  |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 15.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | -67.7     |\n",
            "|    reward             | -5.632424 |\n",
            "|    std                | 1.31      |\n",
            "|    value_loss         | 29.2      |\n",
            "-------------------------------------\n",
            "day: 4177, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2093128.59\n",
            "total_reward: 1093128.59\n",
            "total_cost: 1043.29\n",
            "total_trades: 27065\n",
            "Sharpe: 0.376\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -2.84      |\n",
            "|    reward             | 0.86149645 |\n",
            "|    std                | 1.32       |\n",
            "|    value_loss         | 0.105      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 4300       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 21500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4299       |\n",
            "|    policy_loss        | -9.95      |\n",
            "|    reward             | -1.2073046 |\n",
            "|    std                | 1.33       |\n",
            "|    value_loss         | 0.753      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 4400       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 22000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4399       |\n",
            "|    policy_loss        | 10.6       |\n",
            "|    reward             | -1.1655301 |\n",
            "|    std                | 1.35       |\n",
            "|    value_loss         | 0.985      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 5.22      |\n",
            "|    reward             | 0.5558825 |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 1.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 17.9     |\n",
            "|    reward             | 0.25178  |\n",
            "|    std                | 1.43     |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 8.77     |\n",
            "|    reward             | 0.128184 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 0.573    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0.00525   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -21.1     |\n",
            "|    reward             | -1.803392 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 2.66      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 17.2     |\n",
            "|    reward             | 2.163187 |\n",
            "|    std                | 1.4      |\n",
            "|    value_loss         | 4.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 45       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -74      |\n",
            "|    reward             | 1.035578 |\n",
            "|    std                | 1.43     |\n",
            "|    value_loss         | 34.9     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 550         |\n",
            "|    iterations         | 5100        |\n",
            "|    time_elapsed       | 46          |\n",
            "|    total_timesteps    | 25500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5099        |\n",
            "|    policy_loss        | -14.8       |\n",
            "|    reward             | -0.09672206 |\n",
            "|    std                | 1.45        |\n",
            "|    value_loss         | 1.97        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 550         |\n",
            "|    iterations         | 5200        |\n",
            "|    time_elapsed       | 47          |\n",
            "|    total_timesteps    | 26000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5199        |\n",
            "|    policy_loss        | 3.43        |\n",
            "|    reward             | -0.07636597 |\n",
            "|    std                | 1.45        |\n",
            "|    value_loss         | 0.436       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 550        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 48         |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -5.27      |\n",
            "|    reward             | -0.2939058 |\n",
            "|    std                | 1.49       |\n",
            "|    value_loss         | 0.21       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 49       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -3.1     |\n",
            "|    reward             | 1.417776 |\n",
            "|    std                | 1.5      |\n",
            "|    value_loss         | 0.786    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -3.37     |\n",
            "|    reward             | -0.265831 |\n",
            "|    std                | 1.54      |\n",
            "|    value_loss         | 0.157     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 15.3     |\n",
            "|    reward             | 0.415246 |\n",
            "|    std                | 1.57     |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 51       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 2.55     |\n",
            "|    reward             | 0.821163 |\n",
            "|    std                | 1.54     |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 52        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -13.7     |\n",
            "|    reward             | 1.133271  |\n",
            "|    std                | 1.51      |\n",
            "|    value_loss         | 2.28      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 550      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -3.52    |\n",
            "|    reward             | 0.012139 |\n",
            "|    std                | 1.52     |\n",
            "|    value_loss         | 0.865    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 550         |\n",
            "|    iterations         | 6000        |\n",
            "|    time_elapsed       | 54          |\n",
            "|    total_timesteps    | 30000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5999        |\n",
            "|    policy_loss        | -19.7       |\n",
            "|    reward             | -0.73197854 |\n",
            "|    std                | 1.58        |\n",
            "|    value_loss         | 4.76        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.3322759916915129\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "day: 4177, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2325498.94\n",
            "total_reward: 1325498.94\n",
            "total_cost: 999.00\n",
            "total_trades: 25039\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 219       |\n",
            "|    time_elapsed    | 76        |\n",
            "|    total_timesteps | 16712     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 96.2      |\n",
            "|    critic_loss     | 32        |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16611     |\n",
            "|    reward          | -0.391514 |\n",
            "----------------------------------\n",
            "day: 4177, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2325498.94\n",
            "total_reward: 1325498.94\n",
            "total_cost: 999.00\n",
            "total_trades: 25039\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3726218222104181\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "day: 4177, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2304282.53\n",
            "total_reward: 1304282.53\n",
            "total_cost: 998.98\n",
            "total_trades: 29193\n",
            "Sharpe: 0.448\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 216       |\n",
            "|    time_elapsed    | 77        |\n",
            "|    total_timesteps | 16712     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 202       |\n",
            "|    critic_loss     | 1.46e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 16611     |\n",
            "|    reward          | -0.464567 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.4726266111421579\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "day: 4177, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2158561.44\n",
            "total_reward: 1158561.44\n",
            "total_cost: 5127.92\n",
            "total_trades: 13117\n",
            "Sharpe: 0.321\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 172       |\n",
            "|    time_elapsed    | 97        |\n",
            "|    total_timesteps | 16712     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 864       |\n",
            "|    critic_loss     | 158       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16611     |\n",
            "|    reward          | -0.636724 |\n",
            "----------------------------------\n",
            "day: 4177, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2177957.83\n",
            "total_reward: 1177957.83\n",
            "total_cost: 999.00\n",
            "total_trades: 12531\n",
            "Sharpe: 0.322\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.31641281249909164\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 639         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.43337595 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 624           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034026033 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0244       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.75          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000952     |\n",
            "|    reward               | -0.12105969   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.17          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 610           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00028504943 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | 0.000517      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.12          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00103      |\n",
            "|    reward               | -0.06758183   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.54          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 610          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010248437 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0124       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.501        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    reward               | -0.669144    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.38         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 611           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0949148e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | 0.0204        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.14          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000362     |\n",
            "|    reward               | 0.4801095     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.62          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 602           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035124336 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | -0.0152       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.69          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000994     |\n",
            "|    reward               | 1.286849      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.53          |\n",
            "-------------------------------------------\n",
            "day: 4177, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1591697.99\n",
            "total_reward: 591697.99\n",
            "total_cost: 230287.28\n",
            "total_trades: 32261\n",
            "Sharpe: 0.288\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 604           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00045023617 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | -0.011        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.97          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00106      |\n",
            "|    reward               | -0.1255778    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.48          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 606          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007946825 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0127      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.758        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    reward               | -0.9502348   |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 2.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 602          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008508566 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0226      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.63         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00177     |\n",
            "|    reward               | 0.44791085   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.82         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 604           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2395935e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | 0.0296        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.36          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00033      |\n",
            "|    reward               | -4.189882     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.87          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 605           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011723471 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | 0.00566       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.94          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000589     |\n",
            "|    reward               | -0.6542013    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.2          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 604          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005378376 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0113       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.2          |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00124     |\n",
            "|    reward               | -0.82227826  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.84         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 605           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 43            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014142532 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | -0.00593      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.937         |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000593     |\n",
            "|    reward               | -0.49282697   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.75          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 605           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 47            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014258531 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | -0.0011       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.41          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000569     |\n",
            "|    reward               | 1.9661996     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.9           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 606          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.371576e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0218      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.81         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.000411    |\n",
            "|    reward               | 0.4541583    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8            |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.2657667026607006\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.245      |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 14.8       |\n",
            "|    reward             | -0.8282453 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.71       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 547       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.427    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 4.97      |\n",
            "|    reward             | 0.1718929 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.805     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 549        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -2.38      |\n",
            "|    reward             | 0.20963794 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.0586     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 548        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -7.95      |\n",
            "|    reward             | -0.0615235 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.677      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 2.08      |\n",
            "|    reward             | 0.069607  |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.0391    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 550       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 3.19      |\n",
            "|    reward             | -0.245963 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 0.0904    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -1.02     |\n",
            "|    reward             | -0.257321 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.0497    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -4.95     |\n",
            "|    reward             | -0.139911 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.211     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -3.06      |\n",
            "|    reward             | -0.7027975 |\n",
            "|    std                | 1.19       |\n",
            "|    value_loss         | 0.0757     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 23        |\n",
            "|    reward             | -1.822952 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 5.08      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -1.2      |\n",
            "|    reward             | -2.280618 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 3.97      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 544       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -14.6     |\n",
            "|    reward             | -0.285221 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 2.01      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 545      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -20.6    |\n",
            "|    reward             | 1.334892 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 3.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 40.3     |\n",
            "|    reward             | 0.894112 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 8.55     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -16.9     |\n",
            "|    reward             | -4.749989 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 19.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -15        |\n",
            "|    reward             | -10.162058 |\n",
            "|    std                | 1.15       |\n",
            "|    value_loss         | 31.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.5      |\n",
            "|    explained_variance | -2.6       |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -4.94      |\n",
            "|    reward             | 0.81146467 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 0.202      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -22.9      |\n",
            "|    reward             | -2.0254505 |\n",
            "|    std                | 1.21       |\n",
            "|    value_loss         | 3.16       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -23.6      |\n",
            "|    reward             | -1.1443847 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 5.06       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -41.8      |\n",
            "|    reward             | -1.1873047 |\n",
            "|    std                | 1.23       |\n",
            "|    value_loss         | 11.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -10.6     |\n",
            "|    reward             | -1.063961 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 1.52      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 18.6     |\n",
            "|    reward             | 2.464131 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 7.08      |\n",
            "|    reward             | -1.553403 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 1.8       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 14.6      |\n",
            "|    reward             | 0.368619  |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 30.5      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 545      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 8.03     |\n",
            "|    reward             | 3.793016 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -5.28      |\n",
            "|    reward             | 0.82910156 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 0.209      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -18.9     |\n",
            "|    reward             | -1.311012 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 2.14      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 546         |\n",
            "|    iterations         | 2800        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 14000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2799        |\n",
            "|    policy_loss        | 9.75        |\n",
            "|    reward             | -0.34162652 |\n",
            "|    std                | 1.23        |\n",
            "|    value_loss         | 0.8         |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.81    |\n",
            "|    reward             | -0.00488 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 0.0263   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 12.3      |\n",
            "|    reward             | -0.335284 |\n",
            "|    std                | 1.31      |\n",
            "|    value_loss         | 1.06      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 9.92     |\n",
            "|    reward             | 0.294116 |\n",
            "|    std                | 1.34     |\n",
            "|    value_loss         | 0.601    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 23.3     |\n",
            "|    reward             | 2.157069 |\n",
            "|    std                | 1.35     |\n",
            "|    value_loss         | 4.82     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 546       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -5.89     |\n",
            "|    reward             | -0.878627 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 2.37      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 546        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | -1.32      |\n",
            "|    reward             | -0.8347024 |\n",
            "|    std                | 1.4        |\n",
            "|    value_loss         | 0.0913     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 546         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 32          |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | -4.56       |\n",
            "|    reward             | -0.09977062 |\n",
            "|    std                | 1.43        |\n",
            "|    value_loss         | 0.404       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 535         |\n",
            "|    iterations         | 3600        |\n",
            "|    time_elapsed       | 33          |\n",
            "|    total_timesteps    | 18000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3599        |\n",
            "|    policy_loss        | 14.1        |\n",
            "|    reward             | -0.01972987 |\n",
            "|    std                | 1.43        |\n",
            "|    value_loss         | 1.56        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 18        |\n",
            "|    reward             | 0.6768661 |\n",
            "|    std                | 1.45      |\n",
            "|    value_loss         | 1.87      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -0.148206 |\n",
            "|    std                | 1.42      |\n",
            "|    value_loss         | 1.38      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -12.1     |\n",
            "|    reward             | -0.776631 |\n",
            "|    std                | 1.4       |\n",
            "|    value_loss         | 0.941     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -0.0673   |\n",
            "|    reward             | -0.225636 |\n",
            "|    std                | 1.44      |\n",
            "|    value_loss         | 0.0818    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 3.26      |\n",
            "|    reward             | -1.899628 |\n",
            "|    std                | 1.42      |\n",
            "|    value_loss         | 0.557     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 29.9     |\n",
            "|    reward             | 1.015979 |\n",
            "|    std                | 1.38     |\n",
            "|    value_loss         | 8.25     |\n",
            "------------------------------------\n",
            "day: 4240, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2210706.60\n",
            "total_reward: 1210706.60\n",
            "total_cost: 1098.12\n",
            "total_trades: 22149\n",
            "Sharpe: 0.493\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 40          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | -11.4       |\n",
            "|    reward             | -0.28547958 |\n",
            "|    std                | 1.39        |\n",
            "|    value_loss         | 0.982       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 535         |\n",
            "|    iterations         | 4400        |\n",
            "|    time_elapsed       | 41          |\n",
            "|    total_timesteps    | 22000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.2       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4399        |\n",
            "|    policy_loss        | -9.83       |\n",
            "|    reward             | -0.32164088 |\n",
            "|    std                | 1.39        |\n",
            "|    value_loss         | 1.03        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 535         |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 41          |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | 5.21        |\n",
            "|    reward             | -0.37150005 |\n",
            "|    std                | 1.44        |\n",
            "|    value_loss         | 0.406       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | -6.06     |\n",
            "|    reward             | 0.5658698 |\n",
            "|    std                | 1.48      |\n",
            "|    value_loss         | 0.302     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 11.6      |\n",
            "|    reward             | -0.152373 |\n",
            "|    std                | 1.5       |\n",
            "|    value_loss         | 2.76      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -9.19     |\n",
            "|    reward             | 1.1586463 |\n",
            "|    std                | 1.5       |\n",
            "|    value_loss         | 1.97      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 45       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -30.6    |\n",
            "|    reward             | 0.602748 |\n",
            "|    std                | 1.5      |\n",
            "|    value_loss         | 9.03     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -2.01     |\n",
            "|    reward             | -2.953296 |\n",
            "|    std                | 1.59      |\n",
            "|    value_loss         | 8.76      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 47        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -6.27     |\n",
            "|    reward             | 1.1797106 |\n",
            "|    std                | 1.63      |\n",
            "|    value_loss         | 0.493     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 15.8      |\n",
            "|    reward             | 0.677672  |\n",
            "|    std                | 1.62      |\n",
            "|    value_loss         | 2.07      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 536        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 49         |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -23.6      |\n",
            "|    reward             | -0.7872941 |\n",
            "|    std                | 1.66       |\n",
            "|    value_loss         | 4.53       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 5400        |\n",
            "|    time_elapsed       | 50          |\n",
            "|    total_timesteps    | 27000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.9       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5399        |\n",
            "|    policy_loss        | -7.46       |\n",
            "|    reward             | 0.114867486 |\n",
            "|    std                | 1.58        |\n",
            "|    value_loss         | 0.548       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 51        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -14       |\n",
            "|    reward             | 1.3580002 |\n",
            "|    std                | 1.57      |\n",
            "|    value_loss         | 1.29      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 22.1     |\n",
            "|    reward             | 0.154222 |\n",
            "|    std                | 1.58     |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 53        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 16.8      |\n",
            "|    reward             | -1.194948 |\n",
            "|    std                | 1.64      |\n",
            "|    value_loss         | 1.84      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -17.7     |\n",
            "|    reward             | -1.951382 |\n",
            "|    std                | 1.65      |\n",
            "|    value_loss         | 2.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 66.4      |\n",
            "|    reward             | 3.396968  |\n",
            "|    std                | 1.59      |\n",
            "|    value_loss         | 26.3      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 537        |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 55         |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | -6.11      |\n",
            "|    reward             | 0.44602114 |\n",
            "|    std                | 1.57       |\n",
            "|    value_loss         | 0.343      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.04716263981782176\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "day: 4240, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2422155.54\n",
            "total_reward: 1422155.54\n",
            "total_cost: 998.99\n",
            "total_trades: 29680\n",
            "Sharpe: 0.420\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 213      |\n",
            "|    time_elapsed    | 79       |\n",
            "|    total_timesteps | 16964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 191      |\n",
            "|    critic_loss     | 4.9      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16863    |\n",
            "|    reward          | 2.290356 |\n",
            "---------------------------------\n",
            "day: 4240, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2422155.54\n",
            "total_reward: 1422155.54\n",
            "total_cost: 998.99\n",
            "total_trades: 29680\n",
            "Sharpe: 0.420\n",
            "=================================\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.1891974486414947\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "day: 4240, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2691169.90\n",
            "total_reward: 1691169.90\n",
            "total_cost: 998.99\n",
            "total_trades: 25440\n",
            "Sharpe: 0.416\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 190      |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 16964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 111      |\n",
            "|    critic_loss     | 51.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16863    |\n",
            "|    reward          | 2.71075  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.19963263418792457\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "day: 4240, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1658388.10\n",
            "total_reward: 658388.10\n",
            "total_cost: 6518.57\n",
            "total_trades: 21325\n",
            "Sharpe: 0.252\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 146      |\n",
            "|    time_elapsed    | 116      |\n",
            "|    total_timesteps | 16964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 564      |\n",
            "|    critic_loss     | 9.32     |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16863    |\n",
            "|    reward          | 2.620943 |\n",
            "---------------------------------\n",
            "day: 4240, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2560650.50\n",
            "total_reward: 1560650.50\n",
            "total_cost: 998.99\n",
            "total_trades: 21170\n",
            "Sharpe: 0.419\n",
            "=================================\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.04157653843930078\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 603        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.4063525 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 581           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029385957 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0473       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.686         |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000905     |\n",
            "|    reward               | 0.055646352   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.33          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 574           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019108431 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000312      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 6.66          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000762     |\n",
            "|    reward               | 0.109348476   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.88          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 573           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 14            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00046984787 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0287       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.06          |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000902     |\n",
            "|    reward               | -1.1134242    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.96          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002980504 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0162      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.69         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.000778    |\n",
            "|    reward               | 0.080292694  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 10.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 571         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001364904 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0207     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.35        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00227    |\n",
            "|    reward               | -1.6626071  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.41        |\n",
            "-----------------------------------------\n",
            "day: 4240, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2311280.21\n",
            "total_reward: 1311280.21\n",
            "total_cost: 228283.33\n",
            "total_trades: 32862\n",
            "Sharpe: 0.409\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 569          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.760304e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0161      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 12.4         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.000289    |\n",
            "|    reward               | 0.33561683   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 27.5         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 570           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044456663 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00675      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.86          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.0012       |\n",
            "|    reward               | -2.219001     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 12            |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8459828e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0263        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 15.8          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000354     |\n",
            "|    reward               | -0.16821733   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 25.1          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00087845733 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0115       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 8.25          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00131      |\n",
            "|    reward               | -0.8635105    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 17.2          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 570           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 39            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011669801 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000168      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 9.59          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000484     |\n",
            "|    reward               | 0.07043322    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 20.8          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001504062 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00424    |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 6.99        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00169    |\n",
            "|    reward               | 0.6445962   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 570           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 46            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013603127 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00596       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 8.34          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000549     |\n",
            "|    reward               | 0.22430573    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 19.3          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003855712 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00754      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 7.92         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000875    |\n",
            "|    reward               | 1.6220787    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 17.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005067509 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.03        |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.66         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    reward               | 0.6408213    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.2          |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.18646409267504208\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 503       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 4.81      |\n",
            "|    reward             | -0.656915 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.227     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 368       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 0.0972    |\n",
            "|    reward             | -0.016764 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.0302    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 406        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.022      |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -0.0277    |\n",
            "|    reward             | -0.1336066 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.0417     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 429       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -11.1     |\n",
            "|    reward             | -0.228927 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.24      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 444      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 0.468    |\n",
            "|    reward             | 0.662805 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 0.249    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 453       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -1.43     |\n",
            "|    reward             | 0.2865227 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.604     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 454       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -26.2     |\n",
            "|    reward             | -0.012649 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 6.69      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 458        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -10.8      |\n",
            "|    reward             | -1.8879113 |\n",
            "|    std                | 1.11       |\n",
            "|    value_loss         | 3.25       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 459         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -11.9       |\n",
            "|    reward             | -0.75089693 |\n",
            "|    std                | 1.15        |\n",
            "|    value_loss         | 2.58        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 459         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 11.6        |\n",
            "|    reward             | 0.035283856 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 0.936       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 459         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | 5.96e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 1.17        |\n",
            "|    reward             | -0.09831437 |\n",
            "|    std                | 1.18        |\n",
            "|    value_loss         | 0.0174      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 461        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -0.115     |\n",
            "|    reward             | 0.16389424 |\n",
            "|    std                | 1.23       |\n",
            "|    value_loss         | 0.00905    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 463      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -0.808   |\n",
            "|    reward             | 0.140139 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.0207   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 465       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -9.12     |\n",
            "|    reward             | -0.131799 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 0.573     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 468      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | -0.00163 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -3.66    |\n",
            "|    reward             | -0.04241 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 0.0726   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 470       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -4.95     |\n",
            "|    reward             | -0.536985 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 0.2       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 472        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 3.75       |\n",
            "|    reward             | 0.25450623 |\n",
            "|    std                | 1.34       |\n",
            "|    value_loss         | 0.0971     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 474        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.9      |\n",
            "|    explained_variance | 0.00294    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 12.3       |\n",
            "|    reward             | 0.43715137 |\n",
            "|    std                | 1.39       |\n",
            "|    value_loss         | 1.18       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 476         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 19          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -7.93       |\n",
            "|    reward             | -0.50725055 |\n",
            "|    std                | 1.42        |\n",
            "|    value_loss         | 0.418       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 477        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.4      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -6.15      |\n",
            "|    reward             | 0.09219728 |\n",
            "|    std                | 1.48       |\n",
            "|    value_loss         | 0.218      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 479       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 6.31      |\n",
            "|    reward             | 0.278579  |\n",
            "|    std                | 1.49      |\n",
            "|    value_loss         | 0.299     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 478      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | -3.31    |\n",
            "|    reward             | 0.111286 |\n",
            "|    std                | 1.55     |\n",
            "|    value_loss         | 0.105    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 479       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 7.36      |\n",
            "|    reward             | -0.133729 |\n",
            "|    std                | 1.57      |\n",
            "|    value_loss         | 0.237     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 480      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15      |\n",
            "|    explained_variance | -0.00747 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -6.86    |\n",
            "|    reward             | 0.028864 |\n",
            "|    std                | 1.64     |\n",
            "|    value_loss         | 0.38     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 481       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -1.34     |\n",
            "|    reward             | -0.272729 |\n",
            "|    std                | 1.69      |\n",
            "|    value_loss         | 0.0301    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 481      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 8.53     |\n",
            "|    reward             | 1.225841 |\n",
            "|    std                | 1.7      |\n",
            "|    value_loss         | 0.415    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 482       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -9.26     |\n",
            "|    reward             | 0.3617799 |\n",
            "|    std                | 1.73      |\n",
            "|    value_loss         | 0.689     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 481       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 10.4      |\n",
            "|    reward             | 1.5878799 |\n",
            "|    std                | 1.73      |\n",
            "|    value_loss         | 0.626     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 482      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 7.1      |\n",
            "|    reward             | -2.04015 |\n",
            "|    std                | 1.73     |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 482      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 6.79     |\n",
            "|    reward             | 1.608    |\n",
            "|    std                | 1.76     |\n",
            "|    value_loss         | 0.916    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 482      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 11.3     |\n",
            "|    reward             | 1.0452   |\n",
            "|    std                | 1.75     |\n",
            "|    value_loss         | 0.547    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 482      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 16       |\n",
            "|    reward             | 0.2613   |\n",
            "|    std                | 1.76     |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 482      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 37.4     |\n",
            "|    reward             | 1.2663   |\n",
            "|    std                | 1.73     |\n",
            "|    value_loss         | 7.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 481      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | -7.75    |\n",
            "|    reward             | 0.804    |\n",
            "|    std                | 1.7      |\n",
            "|    value_loss         | 0.608    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 482        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -15.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | 9.11       |\n",
            "|    reward             | 0.84646124 |\n",
            "|    std                | 1.71       |\n",
            "|    value_loss         | 0.576      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 483        |\n",
            "|    iterations         | 3600       |\n",
            "|    time_elapsed       | 37         |\n",
            "|    total_timesteps    | 18000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -15.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3599       |\n",
            "|    policy_loss        | 9.02       |\n",
            "|    reward             | 0.65024245 |\n",
            "|    std                | 1.76       |\n",
            "|    value_loss         | 1.12       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 483         |\n",
            "|    iterations         | 3700        |\n",
            "|    time_elapsed       | 38          |\n",
            "|    total_timesteps    | 18500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -15.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3699        |\n",
            "|    policy_loss        | -5.14       |\n",
            "|    reward             | -0.47413146 |\n",
            "|    std                | 1.69        |\n",
            "|    value_loss         | 0.936       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 483      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | -8.81    |\n",
            "|    reward             | 0.352222 |\n",
            "|    std                | 1.71     |\n",
            "|    value_loss         | 0.525    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 484       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 14.1      |\n",
            "|    reward             | -0.474145 |\n",
            "|    std                | 1.8       |\n",
            "|    value_loss         | 0.793     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 484       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -12       |\n",
            "|    reward             | -0.825481 |\n",
            "|    std                | 1.79      |\n",
            "|    value_loss         | 0.754     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 485      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.4    |\n",
            "|    explained_variance | 0.00143  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 8.81     |\n",
            "|    reward             | 0.489636 |\n",
            "|    std                | 1.75     |\n",
            "|    value_loss         | 0.588    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 485       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 0.533     |\n",
            "|    reward             | -0.470794 |\n",
            "|    std                | 1.79      |\n",
            "|    value_loss         | 0.46      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 486       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -18.6     |\n",
            "|    reward             | -0.472916 |\n",
            "|    std                | 1.81      |\n",
            "|    value_loss         | 1.31      |\n",
            "-------------------------------------\n",
            "day: 4303, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 989941.14\n",
            "total_reward: -10058.86\n",
            "total_cost: 2114.66\n",
            "total_trades: 9311\n",
            "Sharpe: 0.068\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 486       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -4.33     |\n",
            "|    reward             | -0.087784 |\n",
            "|    std                | 1.84      |\n",
            "|    value_loss         | 0.222     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 487      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | -8.26    |\n",
            "|    reward             | 0.898023 |\n",
            "|    std                | 1.88     |\n",
            "|    value_loss         | 0.456    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 487      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -16.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 5.68     |\n",
            "|    reward             | 0.051279 |\n",
            "|    std                | 1.94     |\n",
            "|    value_loss         | 0.209    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 488      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -16.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 9.85     |\n",
            "|    reward             | 0.102042 |\n",
            "|    std                | 1.98     |\n",
            "|    value_loss         | 0.693    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 488       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 49        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -16.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -1.46     |\n",
            "|    reward             | -0.051107 |\n",
            "|    std                | 1.99      |\n",
            "|    value_loss         | 0.0125    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 489      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -16.4    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -9.44    |\n",
            "|    reward             | 0.160536 |\n",
            "|    std                | 2.01     |\n",
            "|    value_loss         | 0.563    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 489       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 51        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -16.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -3.83     |\n",
            "|    reward             | -0.211729 |\n",
            "|    std                | 2.04      |\n",
            "|    value_loss         | 0.126     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 489      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -16.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 0.73     |\n",
            "|    reward             | 0.379308 |\n",
            "|    std                | 2.03     |\n",
            "|    value_loss         | 0.0358   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 489        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 53         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -16.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -16.7      |\n",
            "|    reward             | 0.11431368 |\n",
            "|    std                | 2.09       |\n",
            "|    value_loss         | 1.22       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 489       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 54        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -16.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -11.3     |\n",
            "|    reward             | -0.051464 |\n",
            "|    std                | 2.11      |\n",
            "|    value_loss         | 0.522     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 489       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 55        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -16.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 7.82      |\n",
            "|    reward             | 0.235625  |\n",
            "|    std                | 2.12      |\n",
            "|    value_loss         | 0.36      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 490      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -16.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | -3.09    |\n",
            "|    reward             | 0.1838   |\n",
            "|    std                | 2.1      |\n",
            "|    value_loss         | 0.0457   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 490       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 57        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -16.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 0.445     |\n",
            "|    reward             | 0.684444  |\n",
            "|    std                | 2.13      |\n",
            "|    value_loss         | 0.026     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 490      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 58       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -17.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 3.78     |\n",
            "|    reward             | 0.514286 |\n",
            "|    std                | 2.24     |\n",
            "|    value_loss         | 0.128    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 490       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 59        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -17.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -2.13     |\n",
            "|    reward             | 0.455824  |\n",
            "|    std                | 2.25      |\n",
            "|    value_loss         | 0.111     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 490       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 60        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -17.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -1.39     |\n",
            "|    reward             | -0.544048 |\n",
            "|    std                | 2.36      |\n",
            "|    value_loss         | 0.0405    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 490       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -17.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 5.31      |\n",
            "|    reward             | -0.066699 |\n",
            "|    std                | 2.4       |\n",
            "|    value_loss         | 0.144     |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.36103755085532885\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "day: 4303, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 989376.96\n",
            "total_reward: -10623.04\n",
            "total_cost: 999.00\n",
            "total_trades: 17068\n",
            "Sharpe: 0.030\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 189      |\n",
            "|    time_elapsed    | 91       |\n",
            "|    total_timesteps | 17216    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 32.6     |\n",
            "|    critic_loss     | 0.435    |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17115    |\n",
            "|    reward          | 0.014912 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3620897726469113\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "day: 4303, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2226674.86\n",
            "total_reward: 1226674.86\n",
            "total_cost: 4297.68\n",
            "total_trades: 17600\n",
            "Sharpe: 0.373\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 184      |\n",
            "|    time_elapsed    | 93       |\n",
            "|    total_timesteps | 17216    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.4     |\n",
            "|    critic_loss     | 1.41e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17115    |\n",
            "|    reward          | 0.408957 |\n",
            "---------------------------------\n",
            "day: 4303, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2688369.69\n",
            "total_reward: 1688369.69\n",
            "total_cost: 998.99\n",
            "total_trades: 17181\n",
            "Sharpe: 0.464\n",
            "=================================\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.1189684948942679\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "day: 4303, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2006750.48\n",
            "total_reward: 1006750.48\n",
            "total_cost: 999.00\n",
            "total_trades: 21483\n",
            "Sharpe: 0.345\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 17216    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 811      |\n",
            "|    critic_loss     | 110      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17115    |\n",
            "|    reward          | 0.352094 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.20528269184929301\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 582         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.32157037 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 569           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00058676576 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000491      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.896         |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000717     |\n",
            "|    reward               | 0.042622082   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.37          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 561         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000474633 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0298     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.72        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00113    |\n",
            "|    reward               | -0.5349976  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.52        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 562          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011508729 |\n",
            "|    clip_fraction        | 4.88e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0124       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.13         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00173     |\n",
            "|    reward               | 0.7946721    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.16         |\n",
            "------------------------------------------\n",
            "day: 4303, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1926161.38\n",
            "total_reward: 926161.38\n",
            "total_cost: 241287.47\n",
            "total_trades: 33345\n",
            "Sharpe: 0.458\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 560           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048062683 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00284      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.06          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    reward               | 0.81532156    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.5           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 561           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081866025 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00151       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.68          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00134      |\n",
            "|    reward               | 1.4305559     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.64          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 561           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 25            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023020539 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00236      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.38          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000877     |\n",
            "|    reward               | -0.049894255  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.25          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 560          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006750857 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00524     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.01         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00129     |\n",
            "|    reward               | 0.5651654    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.35         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 543          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.902565e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0379      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.36         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.000524    |\n",
            "|    reward               | -0.43323848  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 544          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.179266e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0273      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.15         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000373    |\n",
            "|    reward               | -1.0083946   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 10.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 545          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003258914 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0139      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.68         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000982    |\n",
            "|    reward               | -1.4767379   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.1          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 546          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013638792 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00743     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.29         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00152     |\n",
            "|    reward               | 1.2332422    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.69         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 547          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.699038e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0154      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.59         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.0003      |\n",
            "|    reward               | 0.45241603   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.44         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 547           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020911498 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00302       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 10.9          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000667     |\n",
            "|    reward               | -0.47166497   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 24.5          |\n",
            "-------------------------------------------\n",
            "day: 4303, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1818892.64\n",
            "total_reward: 818892.64\n",
            "total_cost: 237655.79\n",
            "total_trades: 33078\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 547           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 56            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025711596 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0334       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.725         |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000812     |\n",
            "|    reward               | 0.9676331     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.73          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.3183220157370226\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 16         |\n",
            "|    reward             | -0.8825087 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.44       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 523         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -3.5        |\n",
            "|    reward             | -0.27565667 |\n",
            "|    std                | 0.968       |\n",
            "|    value_loss         | 0.381       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 525        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 1.54       |\n",
            "|    reward             | -0.1305875 |\n",
            "|    std                | 0.965      |\n",
            "|    value_loss         | 0.192      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -8.47     |\n",
            "|    reward             | -0.154864 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 0.876     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 3.04     |\n",
            "|    reward             | 0.297098 |\n",
            "|    std                | 0.95     |\n",
            "|    value_loss         | 0.161    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 527      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -7.1     |\n",
            "|    reward             | 0.003252 |\n",
            "|    std                | 0.959    |\n",
            "|    value_loss         | 0.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -4.15    |\n",
            "|    reward             | 0.016448 |\n",
            "|    std                | 0.949    |\n",
            "|    value_loss         | 0.248    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -1.06     |\n",
            "|    reward             | -0.532328 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 0.064     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 527      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0.019    |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 11.5     |\n",
            "|    reward             | 0.8271   |\n",
            "|    std                | 0.993    |\n",
            "|    value_loss         | 2.84     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -3.48     |\n",
            "|    reward             | -0.515367 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.302     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -11       |\n",
            "|    reward             | 1.2664264 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.59      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 8.94      |\n",
            "|    reward             | -0.260741 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.661     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -11.5    |\n",
            "|    reward             | 0.726415 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 527        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 10.3       |\n",
            "|    reward             | 0.21071577 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 0.746      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 5.73      |\n",
            "|    reward             | -0.520083 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.295     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -13      |\n",
            "|    reward             | 0.6743   |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -4.87     |\n",
            "|    reward             | -0.201876 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 2.06      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 527        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -10.3      |\n",
            "|    reward             | -1.2940245 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 2.06       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 528         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -5.3        |\n",
            "|    reward             | -0.28544766 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.703       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -22.7      |\n",
            "|    reward             | -0.5229055 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 4.56       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -11.9     |\n",
            "|    reward             | 1.000535  |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.987     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 8.73      |\n",
            "|    reward             | -0.01937  |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.939     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -22.8    |\n",
            "|    reward             | 0.233992 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 3.67     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | -7.28     |\n",
            "|    reward             | -0.689939 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.07      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -79       |\n",
            "|    reward             | -0.281907 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 44.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 24.9      |\n",
            "|    reward             | 0.737048  |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 6.36      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -9.68     |\n",
            "|    reward             | 1.2580281 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.77      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 528         |\n",
            "|    iterations         | 2800        |\n",
            "|    time_elapsed       | 26          |\n",
            "|    total_timesteps    | 14000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2799        |\n",
            "|    policy_loss        | -0.437      |\n",
            "|    reward             | -0.22844915 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.109       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 529          |\n",
            "|    iterations         | 2900         |\n",
            "|    time_elapsed       | 27           |\n",
            "|    total_timesteps    | 14500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.1        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 2899         |\n",
            "|    policy_loss        | 5.5          |\n",
            "|    reward             | -0.112356596 |\n",
            "|    std                | 0.999        |\n",
            "|    value_loss         | 0.331        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -7.47     |\n",
            "|    reward             | -1.037766 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 0.643     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -15.7    |\n",
            "|    reward             | 0.603579 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 10.5      |\n",
            "|    reward             | -3.344718 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.45      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -1.61     |\n",
            "|    reward             | -1.336791 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.76      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 10.3     |\n",
            "|    reward             | 0.897975 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 15.2     |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 528          |\n",
            "|    iterations         | 3500         |\n",
            "|    time_elapsed       | 33           |\n",
            "|    total_timesteps    | 17500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.2        |\n",
            "|    explained_variance | -88.2        |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 3499         |\n",
            "|    policy_loss        | 71.1         |\n",
            "|    reward             | -0.073053844 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 57           |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -13.8     |\n",
            "|    reward             | 1.0265712 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.87      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | 9.58       |\n",
            "|    reward             | 0.24835853 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 0.887      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 529         |\n",
            "|    iterations         | 3800        |\n",
            "|    time_elapsed       | 35          |\n",
            "|    total_timesteps    | 19000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0.00171     |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3799        |\n",
            "|    policy_loss        | -18.6       |\n",
            "|    reward             | -0.17898047 |\n",
            "|    std                | 1.12        |\n",
            "|    value_loss         | 8.04        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 7.7       |\n",
            "|    reward             | -0.654235 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.04      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 1.72      |\n",
            "|    reward             | -0.209791 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.0942    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 13.8     |\n",
            "|    reward             | 1.734621 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 2.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 10.1     |\n",
            "|    reward             | 0.445349 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 40       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -11.9    |\n",
            "|    reward             | 0.825104 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "day: 4366, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2382068.50\n",
            "total_reward: 1382068.50\n",
            "total_cost: 3143.34\n",
            "total_trades: 26553\n",
            "Sharpe: 0.356\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -25.4     |\n",
            "|    reward             | 1.1493119 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 6.43      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 42         |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | -1.04      |\n",
            "|    reward             | -0.9161586 |\n",
            "|    std                | 1.16       |\n",
            "|    value_loss         | 0.0639     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 529         |\n",
            "|    iterations         | 4600        |\n",
            "|    time_elapsed       | 43          |\n",
            "|    total_timesteps    | 23000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4599        |\n",
            "|    policy_loss        | 1.05        |\n",
            "|    reward             | -0.33268508 |\n",
            "|    std                | 1.16        |\n",
            "|    value_loss         | 0.0973      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -12      |\n",
            "|    reward             | 0.934053 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 45        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 7.42      |\n",
            "|    reward             | -1.542684 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 0.922     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 46        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 1.5       |\n",
            "|    reward             | -0.409451 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 0.516     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 13.3     |\n",
            "|    reward             | 0.504124 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -21.9     |\n",
            "|    reward             | -3.030477 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 6.98      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 49         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -1.26      |\n",
            "|    reward             | -1.6395984 |\n",
            "|    std                | 1.27       |\n",
            "|    value_loss         | 0.427      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | 10.8       |\n",
            "|    reward             | 0.36052763 |\n",
            "|    std                | 1.29       |\n",
            "|    value_loss         | 1.01       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 51        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 31.2      |\n",
            "|    reward             | 1.3464563 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 7.64      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 51         |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | 11.1       |\n",
            "|    reward             | 0.09466525 |\n",
            "|    std                | 1.34       |\n",
            "|    value_loss         | 0.977      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -1.72    |\n",
            "|    reward             | 0.093047 |\n",
            "|    std                | 1.35     |\n",
            "|    value_loss         | 0.0987   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 23.2     |\n",
            "|    reward             | 0.484357 |\n",
            "|    std                | 1.4      |\n",
            "|    value_loss         | 4.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 54       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 18.9     |\n",
            "|    reward             | 0.970088 |\n",
            "|    std                | 1.48     |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 8.13     |\n",
            "|    reward             | 1.31388  |\n",
            "|    std                | 1.47     |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 529      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 76.7     |\n",
            "|    reward             | 1.285915 |\n",
            "|    std                | 1.54     |\n",
            "|    value_loss         | 27       |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.044430806089588604\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "day: 4366, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2294730.09\n",
            "total_reward: 1294730.09\n",
            "total_cost: 999.00\n",
            "total_trades: 21830\n",
            "Sharpe: 0.435\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 212      |\n",
            "|    time_elapsed    | 82       |\n",
            "|    total_timesteps | 17468    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.65    |\n",
            "|    critic_loss     | 1.39     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17367    |\n",
            "|    reward          | 1.764178 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.054739230333389566\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "day: 4366, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2581298.41\n",
            "total_reward: 1581298.41\n",
            "total_cost: 4889.18\n",
            "total_trades: 26326\n",
            "Sharpe: 0.428\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 197      |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 17468    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 242      |\n",
            "|    critic_loss     | 1.08e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17367    |\n",
            "|    reward          | 1.418911 |\n",
            "---------------------------------\n",
            "day: 4366, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2592917.77\n",
            "total_reward: 1592917.77\n",
            "total_cost: 998.98\n",
            "total_trades: 26196\n",
            "Sharpe: 0.455\n",
            "=================================\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.01376234168834794\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "day: 4366, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3578848.95\n",
            "total_reward: 2578848.95\n",
            "total_cost: 999.00\n",
            "total_trades: 8732\n",
            "Sharpe: 0.457\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 164      |\n",
            "|    time_elapsed    | 106      |\n",
            "|    total_timesteps | 17468    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 548      |\n",
            "|    critic_loss     | 72.1     |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17367    |\n",
            "|    reward          | 3.55329  |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.08159674691883362\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 597         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.13894516 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 580          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010459861 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0933      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.16         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    reward               | -0.17201586  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.34         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 576         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000830728 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00802    |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 3.03        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00156    |\n",
            "|    reward               | 0.027602224 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.78        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 576           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 14            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035715185 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0724       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.86          |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000903     |\n",
            "|    reward               | 2.054648      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.48          |\n",
            "-------------------------------------------\n",
            "day: 4366, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1420404.99\n",
            "total_reward: 420404.99\n",
            "total_cost: 231028.78\n",
            "total_trades: 33461\n",
            "Sharpe: 0.248\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 545           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023344831 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0356       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.4           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00126      |\n",
            "|    reward               | -0.22672494   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.76          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024898087 |\n",
            "|    clip_fraction        | 0.000977     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0278       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.52         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    reward               | -0.31109464  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.21         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004687865 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0348      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.8          |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    reward               | 0.6568907    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.74         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006620697 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0107       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.27         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00125     |\n",
            "|    reward               | 0.4664071    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.82         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 557           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00085417286 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0869       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.12          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.00167      |\n",
            "|    reward               | 0.20871997    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.75          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 558           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 36            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021032192 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.078        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.797         |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000816     |\n",
            "|    reward               | -0.2805019    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.51          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 559           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00057658204 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.192        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.342         |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00158      |\n",
            "|    reward               | 0.15508212    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 0.606         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 561          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018298279 |\n",
            "|    clip_fraction        | 0.000195     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.054       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 0.514        |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    reward               | -0.4759236   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.28         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 561           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 47            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00028547214 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00775      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.751         |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00083      |\n",
            "|    reward               | 0.2461003     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.63          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 562           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 50            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00052241946 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0413       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.768         |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.00125      |\n",
            "|    reward               | -0.019774143  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.52          |\n",
            "-------------------------------------------\n",
            "day: 4366, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1522609.26\n",
            "total_reward: 522609.26\n",
            "total_cost: 228406.33\n",
            "total_trades: 32718\n",
            "Sharpe: 0.379\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 563           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044593748 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0142       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.876         |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000951     |\n",
            "|    reward               | -0.016018867  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.08          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.261551647731181\n",
            "======Best Model Retraining from:  2007-06-01 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  80.58985718091328  minutes\n",
            "[INFO] Portfolio value plot saved to: 2007-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2007-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2007-2025_no_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 914\n",
            "[INFO] Moved trained_models to 2007-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2007-2025_no_crypto/\n",
            "[INFO] Moved results to 2007-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_0 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_0,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2007-06-01',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_0,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_0,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2007-2025_no_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -0.491   |\n",
            "|    reward             | 0.072802 |\n",
            "|    std                | 0.963    |\n",
            "|    value_loss         | 0.211    |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 775          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.5        |\n",
            "|    explained_variance | 1.81e-05     |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | -6.2         |\n",
            "|    reward             | -0.048637986 |\n",
            "|    std                | 0.976        |\n",
            "|    value_loss         | 0.613        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 778        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.5      |\n",
            "|    explained_variance | 0.00147    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 390        |\n",
            "|    reward             | -10.549021 |\n",
            "|    std                | 0.983      |\n",
            "|    value_loss         | 1.11e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 776        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -68.8      |\n",
            "|    reward             | -0.0367594 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 35.3       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 777      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0.0289   |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 4.85     |\n",
            "|    reward             | 0.68243  |\n",
            "|    std                | 0.981    |\n",
            "|    value_loss         | 0.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 778      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    reward             | 1.445057 |\n",
            "|    std                | 0.986    |\n",
            "|    value_loss         | 0.997    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 764        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -0.0049    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -12.8      |\n",
            "|    reward             | -0.0894634 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 1.49       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 765        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -3.85      |\n",
            "|    reward             | -0.0429413 |\n",
            "|    std                | 0.985      |\n",
            "|    value_loss         | 0.11       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 767      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -2.61    |\n",
            "|    reward             | 0.494053 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.115    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 29.2      |\n",
            "|    reward             | -0.422309 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 6.37      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 770      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0.146    |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 4.47     |\n",
            "|    reward             | 2.658682 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 0.49     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 1.43       |\n",
            "|    reward             | 0.15405689 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0175     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 771       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 9.81      |\n",
            "|    reward             | -0.297456 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 0.731     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 772       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -0.0405   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -4.89     |\n",
            "|    reward             | -0.219603 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.483     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 773      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 3.4      |\n",
            "|    reward             | 0.765712 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 773        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.5      |\n",
            "|    explained_variance | 0.00468    |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 0.484      |\n",
            "|    reward             | -0.2317617 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.00633    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 6.14      |\n",
            "|    reward             | -0.676434 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.409     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -9.27     |\n",
            "|    reward             | -0.160532 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.733     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 774      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 20.9     |\n",
            "|    reward             | 0.123248 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1050820.76\n",
            "total_reward: 50820.76\n",
            "total_cost: 4648.22\n",
            "total_trades: 9578\n",
            "Sharpe: 0.126\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 774         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.481       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -2.96       |\n",
            "|    reward             | -0.88946074 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0772      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 775      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -0.515   |\n",
            "|    reward             | 0.209951 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.107    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 775         |\n",
            "|    iterations         | 2200        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 11000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2199        |\n",
            "|    policy_loss        | 1.99        |\n",
            "|    reward             | -0.31627753 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.206       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | -0.00024 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 7.21     |\n",
            "|    reward             | 0.793378 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 768         |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.1       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | -23.4       |\n",
            "|    reward             | -0.05270076 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 3.72        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -5.42    |\n",
            "|    reward             | 0.471038 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.342    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 767       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | -7.82     |\n",
            "|    reward             | -0.918724 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.487     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 766       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -15       |\n",
            "|    reward             | -1.377072 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 2.67      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 766        |\n",
            "|    iterations         | 2800       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 14000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2799       |\n",
            "|    policy_loss        | 0.0162     |\n",
            "|    reward             | -1.7021942 |\n",
            "|    std                | 1.11       |\n",
            "|    value_loss         | 0.242      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -0.000701 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -127      |\n",
            "|    reward             | -7.241643 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 103       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 39.6     |\n",
            "|    reward             | 0.021419 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 21.4     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.03e-05 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 299       |\n",
            "|    reward             | 174.72856 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 9.73e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -16.7      |\n",
            "|    reward             | 0.48705444 |\n",
            "|    std                | 1.19       |\n",
            "|    value_loss         | 1.65       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -90.9     |\n",
            "|    reward             | -5.207549 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 67.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 7.21      |\n",
            "|    reward             | -2.451245 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.84      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 7.99e-06 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 4.17e+03 |\n",
            "|    reward             | 74.45858 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 9.68e+04 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 2.05     |\n",
            "|    reward             | -0.6029  |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.247    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 1.49e-06 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -270     |\n",
            "|    reward             | 6.290522 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 299      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 45.6      |\n",
            "|    reward             | -1.808689 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 84.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | -2.15e-06 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 892       |\n",
            "|    reward             | 30.5138   |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 2.78e+04  |\n",
            "-------------------------------------\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 23279130.18\n",
            "total_reward: 22279130.18\n",
            "total_cost: 1262.11\n",
            "total_trades: 11990\n",
            "Sharpe: 0.947\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -13.4    |\n",
            "|    reward             | 0.64504  |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 1.31     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 151       |\n",
            "|    reward             | 0.211724  |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 119       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 96.5      |\n",
            "|    reward             | -9.555578 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 75.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -3.1e-06  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -4.91e+03 |\n",
            "|    reward             | 164.80042 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 2.02e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 761       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -27       |\n",
            "|    reward             | -0.094645 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 7.25      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 92.4     |\n",
            "|    reward             | 2.357618 |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 61.9     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 758        |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | 907        |\n",
            "|    reward             | -38.402733 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 7.75e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 756        |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | 9.26e+03   |\n",
            "|    reward             | -286.14737 |\n",
            "|    std                | 1.25       |\n",
            "|    value_loss         | 4.93e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 755       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 3.05      |\n",
            "|    reward             | -0.38827  |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 0.568     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 755      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -29.1    |\n",
            "|    reward             | 1.627246 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 13       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 755      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -196     |\n",
            "|    reward             | 7.68699  |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 710      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 755       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -4.83e+03 |\n",
            "|    reward             | -86.93959 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 1.47e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 756       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -27.3     |\n",
            "|    reward             | -1.165046 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 4.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 756       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 70.8      |\n",
            "|    reward             | 11.226506 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 36.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 757      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 293      |\n",
            "|    reward             | 3.813051 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 551      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 757        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.9      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | 6.67e+03   |\n",
            "|    reward             | -360.54993 |\n",
            "|    std                | 1.23       |\n",
            "|    value_loss         | 3.75e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 757       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -26.8     |\n",
            "|    reward             | -0.788578 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 3.55      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 757       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 270       |\n",
            "|    reward             | 22.378922 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 315       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 756       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -357      |\n",
            "|    reward             | 38.274036 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.14e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 756        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -1.1e+04   |\n",
            "|    reward             | -24.361458 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 9.23e+05   |\n",
            "--------------------------------------\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 24233215.96\n",
            "total_reward: 23233215.96\n",
            "total_cost: 1955.93\n",
            "total_trades: 10691\n",
            "Sharpe: 0.954\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 756      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -15.7    |\n",
            "|    reward             | 0.34759  |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.4434359697070404\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1153942.94\n",
            "total_reward: 153942.94\n",
            "total_cost: 998.99\n",
            "total_trades: 5982\n",
            "Sharpe: 0.199\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 216       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -63.3     |\n",
            "|    critic_loss     | 129       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.128611 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 223       |\n",
            "|    time_elapsed    | 71        |\n",
            "|    total_timesteps | 15960     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -48.4     |\n",
            "|    critic_loss     | 91        |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 15859     |\n",
            "|    reward          | -0.128611 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1153942.94\n",
            "total_reward: 153942.94\n",
            "total_cost: 998.99\n",
            "total_trades: 5982\n",
            "Sharpe: 0.199\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 223       |\n",
            "|    time_elapsed    | 107       |\n",
            "|    total_timesteps | 23940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -33.8     |\n",
            "|    critic_loss     | 6.74      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 23839     |\n",
            "|    reward          | -0.128611 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1153942.94\n",
            "total_reward: 153942.94\n",
            "total_cost: 998.99\n",
            "total_trades: 5982\n",
            "Sharpe: 0.199\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.1496402651626706\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 26015723.41\n",
            "total_reward: 25015723.41\n",
            "total_cost: 998.99\n",
            "total_trades: 9954\n",
            "Sharpe: 0.966\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 227       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 63.8      |\n",
            "|    critic_loss     | 5.45e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | 10.369358 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 26015723.41\n",
            "total_reward: 25015723.41\n",
            "total_cost: 998.99\n",
            "total_trades: 9954\n",
            "Sharpe: 0.966\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 227       |\n",
            "|    time_elapsed    | 70        |\n",
            "|    total_timesteps | 15960     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 210       |\n",
            "|    critic_loss     | 1.24e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 15859     |\n",
            "|    reward          | 10.369358 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 228       |\n",
            "|    time_elapsed    | 104       |\n",
            "|    total_timesteps | 23940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 253       |\n",
            "|    critic_loss     | 1.3e+04   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23839     |\n",
            "|    reward          | 10.369358 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 26015723.41\n",
            "total_reward: 25015723.41\n",
            "total_cost: 998.99\n",
            "total_trades: 9954\n",
            "Sharpe: 0.966\n",
            "=================================\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.4452545113750356\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1269670.99\n",
            "total_reward: 269670.99\n",
            "total_cost: 999.00\n",
            "total_trades: 3988\n",
            "Sharpe: 0.306\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 169      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.07e+03 |\n",
            "|    critic_loss     | 9.57e+04 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 0.750152 |\n",
            "---------------------------------\n",
            "day: 1994, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1269670.99\n",
            "total_reward: 269670.99\n",
            "total_cost: 999.00\n",
            "total_trades: 3988\n",
            "Sharpe: 0.306\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 171      |\n",
            "|    time_elapsed    | 92       |\n",
            "|    total_timesteps | 15960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.39e+03 |\n",
            "|    critic_loss     | 486      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 15859    |\n",
            "|    reward          | 0.750152 |\n",
            "---------------------------------\n",
            "day: 1994, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1269670.99\n",
            "total_reward: 269670.99\n",
            "total_cost: 999.00\n",
            "total_trades: 3988\n",
            "Sharpe: 0.306\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 172      |\n",
            "|    time_elapsed    | 138      |\n",
            "|    total_timesteps | 23940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.68e+03 |\n",
            "|    critic_loss     | 5.1e+03  |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 23839    |\n",
            "|    reward          | 0.750152 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.28562023156260713\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3278876.40\n",
            "total_reward: 2278876.40\n",
            "total_cost: 962724.80\n",
            "total_trades: 17188\n",
            "Sharpe: 0.667\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 862        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.20601918 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 676          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004893426 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00196      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 141          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    reward               | 0.31558332   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 295          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 704          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002823018 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.0016       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 320          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    reward               | -0.05336271  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 743          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 715          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003615014 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000514    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 205          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    reward               | -0.6901273   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 414          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 728           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 14            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00039485295 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00238      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 327           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00137      |\n",
            "|    reward               | 0.9917086     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 714           |\n",
            "-------------------------------------------\n",
            "day: 1994, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 975869.25\n",
            "total_reward: -24130.75\n",
            "total_cost: 701570.92\n",
            "total_trades: 17109\n",
            "Sharpe: 0.141\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 748           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013133959 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000193     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 961           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000678     |\n",
            "|    reward               | 1.7501503     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.3e+03       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 760           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043930943 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00132      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 71.1          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00166      |\n",
            "|    reward               | -0.286653     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 138           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 772           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044386584 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000826     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 156           |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00131      |\n",
            "|    reward               | -0.24691002   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 299           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 777          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 23           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005635874 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000312     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 28.6         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    reward               | -0.46398026  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 52.7         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 780           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 26            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00032266503 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000352      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 122           |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000983     |\n",
            "|    reward               | -1.8758936    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 229           |\n",
            "-------------------------------------------\n",
            "day: 1994, episode: 75\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2471717.91\n",
            "total_reward: 1471717.91\n",
            "total_cost: 853782.09\n",
            "total_trades: 17008\n",
            "Sharpe: 0.517\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 784         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 28          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000524034 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000164   |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 122         |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00143    |\n",
            "|    reward               | -3.6143541  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 189         |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 788           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 31            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021653657 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000104      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 117           |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00128      |\n",
            "|    reward               | 14.947791     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 182           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 794           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4876208e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000265      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 497           |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000401     |\n",
            "|    reward               | -0.07231262   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 781           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 798           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.4857367e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000181     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.66e+04      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -7.76e-05     |\n",
            "|    reward               | -30.272808    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.69e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 800          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.492055e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00256      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 304          |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.000525    |\n",
            "|    reward               | 0.5282543    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 342          |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.1223522720865215\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 704       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -7.81     |\n",
            "|    reward             | -0.199931 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.453     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 699       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 9.7       |\n",
            "|    reward             | -0.136365 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.969     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 651       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -0.00193  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 289       |\n",
            "|    reward             | -8.100836 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 584       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 674      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 10.4     |\n",
            "|    reward             | 2.445731 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 0.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 689      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 69.1     |\n",
            "|    reward             | 9.093866 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 30.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -21.5    |\n",
            "|    reward             | 2.705333 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 861      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 698      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 795      |\n",
            "|    reward             | 52.22011 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 9.11e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 696        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -2.56e+03  |\n",
            "|    reward             | -64.478485 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 6.54e+04   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -147     |\n",
            "|    reward             | 1.47931  |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 185      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 703       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 179       |\n",
            "|    reward             | -90.27752 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 2.15e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 705       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 345       |\n",
            "|    reward             | 44.856106 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.35e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 705       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 3.95e+03  |\n",
            "|    reward             | 318.22968 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.54e+05  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 708        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -6         |\n",
            "|    reward             | 0.18016072 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.986      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 712       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -84.8     |\n",
            "|    reward             | 30.220594 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 940       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 712       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 496       |\n",
            "|    reward             | -2.582733 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 2.93e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 714       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -2.27e+03 |\n",
            "|    reward             | -46.07265 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 3.99e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 714      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 85.1     |\n",
            "|    reward             | 0.332154 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 77.7     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 715       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 3.42e+03  |\n",
            "|    reward             | 275.56683 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 9.55e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 1.16e+03  |\n",
            "|    reward             | 34.929665 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.24e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 3.34e-06  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 3.97e+03  |\n",
            "|    reward             | 56.870262 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.29e+05  |\n",
            "-------------------------------------\n",
            "day: 2057, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 73548632.17\n",
            "total_reward: 72548632.17\n",
            "total_cost: 1569.07\n",
            "total_trades: 9275\n",
            "Sharpe: 1.119\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -19.6     |\n",
            "|    reward             | -2.126785 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 3.9       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 2.65e+03  |\n",
            "|    reward             | -6.418661 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 3.78e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 709       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -418      |\n",
            "|    reward             | -7.914295 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 3.84e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 709       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 1.39e+04  |\n",
            "|    reward             | -98.39115 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.11e+06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 710       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -5.06     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -79.8     |\n",
            "|    reward             | -1.260575 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 65.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 710       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.469    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 16.2      |\n",
            "|    reward             | 0.5062653 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.6       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 711       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -4.27     |\n",
            "|    reward             | 1.4697255 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.967     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -12.6    |\n",
            "|    reward             | 0.295573 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.13     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 3.61      |\n",
            "|    reward             | 0.3965384 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.706     |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 714          |\n",
            "|    iterations         | 3000         |\n",
            "|    time_elapsed       | 20           |\n",
            "|    total_timesteps    | 15000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.9        |\n",
            "|    explained_variance | 0.237        |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 2999         |\n",
            "|    policy_loss        | 6.95         |\n",
            "|    reward             | -0.027562257 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 0.381        |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 714      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -22.2    |\n",
            "|    reward             | 0.940809 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 715      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | -0.263   |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 21.7     |\n",
            "|    reward             | 0.735407 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 3.21     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 716        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 14.3       |\n",
            "|    reward             | -0.7841872 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 1.16       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 717         |\n",
            "|    iterations         | 3400        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 17000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3399        |\n",
            "|    policy_loss        | -6.44       |\n",
            "|    reward             | -0.26248068 |\n",
            "|    std                | 1.12        |\n",
            "|    value_loss         | 3.06        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | -9.48     |\n",
            "|    reward             | -0.448161 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.67      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 718      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -53.2    |\n",
            "|    reward             | 1.600456 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 19.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 719       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0.0293    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | -35.5     |\n",
            "|    reward             | -0.618358 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 13.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 719       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0.097     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -9.67     |\n",
            "|    reward             | 3.7404537 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.992     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | -178     |\n",
            "|    reward             | 2.799841 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 2.24e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 722      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 103      |\n",
            "|    reward             | 266.0499 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 7.45e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 1.32e+03  |\n",
            "|    reward             | 83.807785 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.19e+04  |\n",
            "-------------------------------------\n",
            "day: 2057, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 61358137.33\n",
            "total_reward: 60358137.33\n",
            "total_cost: 1841.33\n",
            "total_trades: 7013\n",
            "Sharpe: 1.090\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 724      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 6.27     |\n",
            "|    reward             | 0.636534 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.848    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 725      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 21.6     |\n",
            "|    reward             | 3.253155 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 346      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 1.16e+03  |\n",
            "|    reward             | 23.335571 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 8.44e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 725      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 4.65e+03 |\n",
            "|    reward             | 18.70854 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 1.42e+05 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | 0.0175    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 55.4      |\n",
            "|    reward             | -2.137373 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 21.7      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 726      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -549     |\n",
            "|    reward             | 5.225422 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 3.6e+03  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 301       |\n",
            "|    reward             | -6.096794 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 846       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -1e+04    |\n",
            "|    reward             | 20.618216 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 8.13e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -3.03     |\n",
            "|    reward             | 0.258944  |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 0.294     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 729       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | -0.0001   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -1.44e+03 |\n",
            "|    reward             | 214.06657 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 1.82e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 730       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -610      |\n",
            "|    reward             | -30.70744 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 2.88e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 731       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 6.2e+03   |\n",
            "|    reward             | -341.9163 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 2.54e+05  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 731      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -243     |\n",
            "|    reward             | 1.328309 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 388      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.9     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -1.39e+03 |\n",
            "|    reward             | 22.874292 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 4.52e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 2.94e+03  |\n",
            "|    reward             | 156.39166 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 3.84e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -5.05e+03 |\n",
            "|    reward             | -77.36138 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 2.83e+05  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 733      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 29.8     |\n",
            "|    reward             | 2.636705 |\n",
            "|    std                | 1.38     |\n",
            "|    value_loss         | 4.7      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 734       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 1.06e+03  |\n",
            "|    reward             | -87.48404 |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 1.41e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 734       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15       |\n",
            "|    explained_variance | 6.2e-06   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -64.6     |\n",
            "|    reward             | 59.227543 |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 1.78e+03  |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.09587056070846463\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 219      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.8    |\n",
            "|    critic_loss     | 19       |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | -0.98285 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1481273.66\n",
            "total_reward: 481273.66\n",
            "total_cost: 999.00\n",
            "total_trades: 8228\n",
            "Sharpe: 0.385\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 219      |\n",
            "|    time_elapsed    | 74       |\n",
            "|    total_timesteps | 16464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -35.3    |\n",
            "|    critic_loss     | 15.3     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16363    |\n",
            "|    reward          | -0.98285 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1481273.66\n",
            "total_reward: 481273.66\n",
            "total_cost: 999.00\n",
            "total_trades: 8228\n",
            "Sharpe: 0.385\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 217      |\n",
            "|    time_elapsed    | 113      |\n",
            "|    total_timesteps | 24696    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18      |\n",
            "|    critic_loss     | 8.85     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 24595    |\n",
            "|    reward          | -0.98285 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.20455755725042926\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 224      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 278      |\n",
            "|    critic_loss     | 691      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | -0.0112  |\n",
            "---------------------------------\n",
            "day: 2057, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1423479.36\n",
            "total_reward: 423479.36\n",
            "total_cost: 1989.57\n",
            "total_trades: 6234\n",
            "Sharpe: 0.378\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 16464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 315      |\n",
            "|    critic_loss     | 1.13e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16363    |\n",
            "|    reward          | -0.0112  |\n",
            "---------------------------------\n",
            "day: 2057, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1423479.36\n",
            "total_reward: 423479.36\n",
            "total_cost: 1989.57\n",
            "total_trades: 6234\n",
            "Sharpe: 0.378\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 110      |\n",
            "|    total_timesteps | 24696    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 278      |\n",
            "|    critic_loss     | 320      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 24595    |\n",
            "|    reward          | -0.0112  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.009075819336732571\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 175      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 761      |\n",
            "|    critic_loss     | 2.86e+04 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 65.54251 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 51121026.49\n",
            "total_reward: 50121026.49\n",
            "total_cost: 998.99\n",
            "total_trades: 6171\n",
            "Sharpe: 1.069\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 171      |\n",
            "|    time_elapsed    | 95       |\n",
            "|    total_timesteps | 16464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.24e+03 |\n",
            "|    critic_loss     | 1.33e+04 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16363    |\n",
            "|    reward          | 65.54251 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 51121026.49\n",
            "total_reward: 50121026.49\n",
            "total_cost: 998.99\n",
            "total_trades: 6171\n",
            "Sharpe: 1.069\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 171      |\n",
            "|    time_elapsed    | 144      |\n",
            "|    total_timesteps | 24696    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.73e+03 |\n",
            "|    critic_loss     | 7.59e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 24595    |\n",
            "|    reward          | 65.54251 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.09653606163553047\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    fps             | 918      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "| train/             |          |\n",
            "|    reward          | -28.6617 |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 877          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009114188 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -1.01e-05    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.39e+03     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    reward               | 0.09052244   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.41e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 869          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018140788 |\n",
            "|    clip_fraction        | 0.000195     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00987     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 48.3         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    reward               | -3.0732217   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 765          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003362116 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0013      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 262          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00115     |\n",
            "|    reward               | 0.7412224    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 482          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 782           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021652642 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00615      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 24.7          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00061      |\n",
            "|    reward               | -1.1689051    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 67.9          |\n",
            "-------------------------------------------\n",
            "day: 2057, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4140938.88\n",
            "total_reward: 3140938.88\n",
            "total_cost: 1179298.54\n",
            "total_trades: 17613\n",
            "Sharpe: 0.695\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 791           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034327875 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000649      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 389           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00147      |\n",
            "|    reward               | 1.7829313     |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 636           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 800           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012901568 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000135      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 286           |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000565     |\n",
            "|    reward               | -3.2050607    |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 496           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 806           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034112917 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000327      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 143           |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00135      |\n",
            "|    reward               | 0.50952363    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 281           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 810          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005496304 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0133      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 92.9         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    reward               | 1.5572407    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 235          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 814           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 25            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00040375162 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00155      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 143           |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00124      |\n",
            "|    reward               | -2.7610633    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 266           |\n",
            "-------------------------------------------\n",
            "day: 2057, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3024483.48\n",
            "total_reward: 2024483.48\n",
            "total_cost: 1061021.14\n",
            "total_trades: 17728\n",
            "Sharpe: 0.600\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 817           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067365624 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00214       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 322           |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00181      |\n",
            "|    reward               | 0.048733357   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 603           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 819          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.751416e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00248      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 919          |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.0004      |\n",
            "|    reward               | -0.36687505  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.95e+03     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 821           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00037650173 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00424      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 71.1          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000639     |\n",
            "|    reward               | 1.1438168     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 146           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 823           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010562671 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00223      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 569           |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000478     |\n",
            "|    reward               | 1.6409122     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 942           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 825           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023088718 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.0021       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 690           |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000768     |\n",
            "|    reward               | -1.0387468    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.85e+03      |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.05466920466854454\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -4.78     |\n",
            "|    reward             | -0.371073 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.201     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 741       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.00217  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 11.3      |\n",
            "|    reward             | -0.022823 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.959     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 744       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0118    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 5.15      |\n",
            "|    reward             | -0.125175 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.784     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 21.6     |\n",
            "|    reward             | 2.645252 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -2.02      |\n",
            "|    reward             | 0.47028497 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0945     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 744       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -0.675    |\n",
            "|    reward             | -0.869627 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0927    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 28.5      |\n",
            "|    reward             | -1.284839 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 9.38      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.00404  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -42.5    |\n",
            "|    reward             | 1.717478 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 10       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 745      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -0.532   |\n",
            "|    reward             | 0.005675 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -4.5      |\n",
            "|    reward             | -4.715398 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.325     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -17.8     |\n",
            "|    reward             | -2.452076 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -59.7     |\n",
            "|    reward             | -2.010587 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 28        |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 746         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -6.78       |\n",
            "|    reward             | -0.31867266 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 0.545       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -1.1     |\n",
            "|    reward             | 0.111271 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 0.0492   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -32      |\n",
            "|    reward             | 2.331082 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 6.22     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -23.1     |\n",
            "|    reward             | 0.735907  |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 3.12      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 746        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 1.74       |\n",
            "|    reward             | -0.1294804 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.0388     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -21.6     |\n",
            "|    reward             | 1.668725  |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 6.89      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -75.2     |\n",
            "|    reward             | -1.331362 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 34        |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 3.14e+03 |\n",
            "|    reward             | 6.709658 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 7.08e+04 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 278       |\n",
            "|    reward             | -7.076014 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 667       |\n",
            "-------------------------------------\n",
            "day: 2120, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 27907088.13\n",
            "total_reward: 26907088.13\n",
            "total_cost: 2561.39\n",
            "total_trades: 13130\n",
            "Sharpe: 0.960\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -18.9      |\n",
            "|    reward             | -0.4883898 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 4.32       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -601      |\n",
            "|    reward             | 39.40259  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.79e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 745      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 864      |\n",
            "|    reward             | 95.90266 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 6.7e+03  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -2.13e-05 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -1.85e+03 |\n",
            "|    reward             | -3.377077 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.25e+04  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | 13.1       |\n",
            "|    reward             | -2.3022516 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 3.28       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | 2.38e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | 1.26e+03   |\n",
            "|    reward             | -55.324326 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 4.95e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -141      |\n",
            "|    reward             | -77.15558 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.66e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 2900       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 14500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2899       |\n",
            "|    policy_loss        | 1e+04      |\n",
            "|    reward             | -145.32529 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 1.01e+06   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 745      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -9.09    |\n",
            "|    reward             | 0.060794 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 539       |\n",
            "|    reward             | -87.24971 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.59e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | -57.4     |\n",
            "|    reward             | 14.766847 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 3.28e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -2.59e+03 |\n",
            "|    reward             | 220.72203 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 6.85e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 744      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | -0.00558 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 17.3     |\n",
            "|    reward             | 0.707177 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 3.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 743      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | -49.3    |\n",
            "|    reward             | 0.287568 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 70.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 743      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 4.6e-05  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -317     |\n",
            "|    reward             | 2.163391 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 451      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | -2.15e-06 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | -1.14e+04 |\n",
            "|    reward             | -399.8234 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.26e+06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 3.01e+03  |\n",
            "|    reward             | -69.08581 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.01e+05  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 744      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | -27.4    |\n",
            "|    reward             | 0.37308  |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 739       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -1.35e+03 |\n",
            "|    reward             | 30.107002 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.09e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 740       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 1.05e+03  |\n",
            "|    reward             | 19.665302 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 7.73e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -2.4e+03   |\n",
            "|    reward             | -34.146595 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 4.39e+04   |\n",
            "--------------------------------------\n",
            "day: 2120, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 56742121.21\n",
            "total_reward: 55742121.21\n",
            "total_cost: 1692.74\n",
            "total_trades: 8990\n",
            "Sharpe: 1.066\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 740      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -19.8    |\n",
            "|    reward             | -0.55952 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 740       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -1.59e+03 |\n",
            "|    reward             | -32.60964 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 2.31e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 741       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -1.87e+03 |\n",
            "|    reward             | 10.83922  |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 2.36e+04  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 741         |\n",
            "|    iterations         | 4600        |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 23000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4599        |\n",
            "|    policy_loss        | -9.74e+03   |\n",
            "|    reward             | -127.077095 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 4.89e+05    |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 741      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -5.04    |\n",
            "|    reward             | 0.439093 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 0.524    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 741       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 238       |\n",
            "|    reward             | 48.667732 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 2.16e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 741        |\n",
            "|    iterations         | 4900       |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 24500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4899       |\n",
            "|    policy_loss        | -221       |\n",
            "|    reward             | -187.01308 |\n",
            "|    std                | 1.12       |\n",
            "|    value_loss         | 548        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 742       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -6.2e+03  |\n",
            "|    reward             | -72.34234 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 2.75e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 742       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 17.1      |\n",
            "|    reward             | -2.218021 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.55      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 742       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 71.8      |\n",
            "|    reward             | -2.117664 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 89.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 743      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -159     |\n",
            "|    reward             | 152.4235 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 208      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 743      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 2.37e+03 |\n",
            "|    reward             | 746.168  |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 6.7e+04  |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 743        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 37         |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | -2.36e-05  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | 5.77e+03   |\n",
            "|    reward             | -176.55063 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 3.15e+05   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 742      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -9.35    |\n",
            "|    reward             | 6.094644 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.555    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 71        |\n",
            "|    reward             | 12.211285 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 1.14e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 743        |\n",
            "|    iterations         | 5800       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 29000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5799       |\n",
            "|    policy_loss        | 410        |\n",
            "|    reward             | -42.124992 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 6.78e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 742        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -4.01e+03  |\n",
            "|    reward             | -40.071926 |\n",
            "|    std                | 1.2        |\n",
            "|    value_loss         | 7.48e+04   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 742      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 40       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 1.21     |\n",
            "|    reward             | 1.245112 |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 0.447    |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.1378666940886819\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 236      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8484     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 237      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8383     |\n",
            "|    reward          | 0.032775 |\n",
            "---------------------------------\n",
            "day: 2120, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 999811.11\n",
            "total_reward: -188.89\n",
            "total_cost: 998.94\n",
            "total_trades: 2120\n",
            "Sharpe: -0.002\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 234      |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 16968    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.4    |\n",
            "|    critic_loss     | 2.34e+03 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 16867    |\n",
            "|    reward          | 0.032775 |\n",
            "---------------------------------\n",
            "day: 2120, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 999811.11\n",
            "total_reward: -188.89\n",
            "total_cost: 998.94\n",
            "total_trades: 2120\n",
            "Sharpe: -0.002\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 235      |\n",
            "|    time_elapsed    | 108      |\n",
            "|    total_timesteps | 25452    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -92.9    |\n",
            "|    critic_loss     | 2.52     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 25351    |\n",
            "|    reward          | 0.032775 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.140076203662405\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 217       |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 686       |\n",
            "|    critic_loss     | 6.85e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -66.88978 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 34045727.60\n",
            "total_reward: 33045727.60\n",
            "total_cost: 999.00\n",
            "total_trades: 14840\n",
            "Sharpe: 0.993\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 220       |\n",
            "|    time_elapsed    | 77        |\n",
            "|    total_timesteps | 16968     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 663       |\n",
            "|    critic_loss     | 2.49e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 16867     |\n",
            "|    reward          | -66.88978 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 34045727.60\n",
            "total_reward: 33045727.60\n",
            "total_cost: 999.00\n",
            "total_trades: 14840\n",
            "Sharpe: 0.993\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 221       |\n",
            "|    time_elapsed    | 115       |\n",
            "|    total_timesteps | 25452     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 719       |\n",
            "|    critic_loss     | 6.49e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 25351     |\n",
            "|    reward          | -66.88978 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.14569409708912928\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 181       |\n",
            "|    time_elapsed    | 46        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.53e+03  |\n",
            "|    critic_loss     | 4.36e+04  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -2.090478 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1074745.93\n",
            "total_reward: 74745.93\n",
            "total_cost: 998.98\n",
            "total_trades: 2120\n",
            "Sharpe: 0.143\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 181       |\n",
            "|    time_elapsed    | 93        |\n",
            "|    total_timesteps | 16968     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.29e+03  |\n",
            "|    critic_loss     | 1.53e+04  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16867     |\n",
            "|    reward          | -2.090478 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1074745.93\n",
            "total_reward: 74745.93\n",
            "total_cost: 998.98\n",
            "total_trades: 2120\n",
            "Sharpe: 0.143\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 180       |\n",
            "|    time_elapsed    | 141       |\n",
            "|    total_timesteps | 25452     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.34e+03  |\n",
            "|    critic_loss     | 3.43e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 25351     |\n",
            "|    reward          | -2.090478 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.4152453845004008\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 885        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -3.5301256 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 857           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018848496 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -6.63e-05     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 159           |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000659     |\n",
            "|    reward               | -4.9733424    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 341           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 847          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004045249 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000874     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 165          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    reward               | 2.2354958    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 360          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 840           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020274214 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.0021        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 174           |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000754     |\n",
            "|    reward               | -0.6481631    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 557           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 839           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021912652 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000609      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 605           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000914     |\n",
            "|    reward               | -10.48631     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 970           |\n",
            "-------------------------------------------\n",
            "day: 2120, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1869894.51\n",
            "total_reward: 869894.51\n",
            "total_cost: 668154.88\n",
            "total_trades: 17869\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 834          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022066545 |\n",
            "|    clip_fraction        | 0.000732     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00251      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 52.8         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    reward               | -7.3529367   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 224          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 832          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.171888e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000643     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 618          |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.000455    |\n",
            "|    reward               | 7.3696327    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.37e+03     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 830           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 19            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00083014613 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.0017        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 397           |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00197      |\n",
            "|    reward               | 0.38974833    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 689           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 829           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012799224 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000123     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 364           |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000576     |\n",
            "|    reward               | -4.276348     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 539           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 827           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00046230288 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000294      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 357           |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00108      |\n",
            "|    reward               | -11.856177    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 774           |\n",
            "-------------------------------------------\n",
            "day: 2120, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5816892.06\n",
            "total_reward: 4816892.06\n",
            "total_cost: 1372049.70\n",
            "total_trades: 18026\n",
            "Sharpe: 0.841\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 826          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004482103 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00164      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 215          |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000977    |\n",
            "|    reward               | -0.89893585  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 506          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 825           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 29            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015172805 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00349       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 738           |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000884     |\n",
            "|    reward               | -4.3603463    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 949           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 794          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.785414e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -9.8e-05     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.25e+03     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000425    |\n",
            "|    reward               | -0.19676004  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.69e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 795          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.093746e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000721    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.63e+04     |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000168    |\n",
            "|    reward               | 18.889727    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.33e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 796           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 38            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00096467516 |\n",
            "|    clip_fraction        | 4.88e-05      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00218       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 218           |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00199      |\n",
            "|    reward               | 1.1947309     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 530           |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.16086811641938475\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 723      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 165      |\n",
            "|    reward             | 4.631983 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 233      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 730       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0.000126  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -575      |\n",
            "|    reward             | -5.190722 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 3.34e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 707       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 1.83e+04  |\n",
            "|    reward             | -512.4239 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 2.46e+06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 715       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 602       |\n",
            "|    reward             | 107.33739 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 2.25e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | -0.00271  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -60.9     |\n",
            "|    reward             | -1.425257 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 34.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 3.76e+03   |\n",
            "|    reward             | 126.176414 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 8.65e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 364       |\n",
            "|    reward             | 10.131974 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 9.24e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -2.14e+04 |\n",
            "|    reward             | 275.10254 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 3.63e+06  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 724      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -21.8    |\n",
            "|    reward             | 1.372454 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 452       |\n",
            "|    reward             | 19.670334 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.19e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -1.74e+03 |\n",
            "|    reward             | 89.14199  |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 1.91e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -8.86e+03 |\n",
            "|    reward             | 617.706   |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 3.9e+05   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 725        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -1.93e+03  |\n",
            "|    reward             | -204.52905 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 3.35e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 143       |\n",
            "|    reward             | 2.4890428 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 149       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -502      |\n",
            "|    reward             | 14.791528 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 1.43e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 2.42e+03  |\n",
            "|    reward             | 30.430925 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 7.28e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -731      |\n",
            "|    reward             | 188.06085 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 4.33e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 724      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -24.3    |\n",
            "|    reward             | 0.109708 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 8.45     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | -0.000361  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -3.08e+03  |\n",
            "|    reward             | -71.193405 |\n",
            "|    std                | 1.12       |\n",
            "|    value_loss         | 1.21e+05   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 723      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 19.9     |\n",
            "|    reward             | 9.333921 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 1.75e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | -4.86e+03  |\n",
            "|    reward             | -363.50208 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 3.14e+05   |\n",
            "--------------------------------------\n",
            "day: 2183, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 56891348.25\n",
            "total_reward: 55891348.25\n",
            "total_cost: 3169.35\n",
            "total_trades: 10303\n",
            "Sharpe: 1.045\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | -15.9    |\n",
            "|    reward             | 0.023282 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -5.11     |\n",
            "|    reward             | 32.70731  |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 148       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | 1.3e+03    |\n",
            "|    reward             | -114.37399 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 1.35e+04   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 723        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | 6.48e+03   |\n",
            "|    reward             | -1608.1893 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 3.92e+05   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 723        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | -1.66e+03  |\n",
            "|    reward             | -408.14957 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 3.96e+04   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 724      |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 13500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | -13.5    |\n",
            "|    reward             | 0.352048 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 2800       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 14000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2799       |\n",
            "|    policy_loss        | -31.1      |\n",
            "|    reward             | -18.708584 |\n",
            "|    std                | 1.16       |\n",
            "|    value_loss         | 532        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 725        |\n",
            "|    iterations         | 2900       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 14500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2899       |\n",
            "|    policy_loss        | -618       |\n",
            "|    reward             | -56.832172 |\n",
            "|    std                | 1.16       |\n",
            "|    value_loss         | 3.27e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 470       |\n",
            "|    reward             | 76.612785 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 5.39e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -0.914    |\n",
            "|    reward             | -0.059856 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.998     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 2.08e+03  |\n",
            "|    reward             | 27.245092 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 3.04e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | -581      |\n",
            "|    reward             | -49.98279 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.84e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 1.57e+03  |\n",
            "|    reward             | 36.280136 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.91e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | -28.1     |\n",
            "|    reward             | -2.195288 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 6.62      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 725        |\n",
            "|    iterations         | 3600       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 18000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3599       |\n",
            "|    policy_loss        | 117        |\n",
            "|    reward             | -15.326732 |\n",
            "|    std                | 1.19       |\n",
            "|    value_loss         | 87.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | -300      |\n",
            "|    reward             | 24.963106 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 388       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 726         |\n",
            "|    iterations         | 3800        |\n",
            "|    time_elapsed       | 26          |\n",
            "|    total_timesteps    | 19000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.9       |\n",
            "|    explained_variance | 2.38e-07    |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3799        |\n",
            "|    policy_loss        | 1.37e+04    |\n",
            "|    reward             | -127.986664 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 1.26e+06    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 3900       |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 19500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3899       |\n",
            "|    policy_loss        | -312       |\n",
            "|    reward             | -47.106358 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 1.44e+04   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 727      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 59       |\n",
            "|    reward             | -3.79044 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 35.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | 78.2       |\n",
            "|    reward             | -11.343018 |\n",
            "|    std                | 1.19       |\n",
            "|    value_loss         | 173        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 727      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 872      |\n",
            "|    reward             | 21.01524 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 6.53e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -1.19e+03 |\n",
            "|    reward             | -97.7137  |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 2.15e+04  |\n",
            "-------------------------------------\n",
            "day: 2183, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 49177187.64\n",
            "total_reward: 48177187.64\n",
            "total_cost: 998.99\n",
            "total_trades: 8883\n",
            "Sharpe: 1.026\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -27       |\n",
            "|    reward             | -1.204529 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 4.85      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | -32.3      |\n",
            "|    reward             | -105.12879 |\n",
            "|    std                | 1.21       |\n",
            "|    value_loss         | 974        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 1.63e+03  |\n",
            "|    reward             | 12.053439 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 1.81e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 1.14e+03  |\n",
            "|    reward             | 129.92085 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 4.33e+04  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -245       |\n",
            "|    reward             | -239.04776 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 1.97e+03   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 728      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 1.225505 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 728      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -710     |\n",
            "|    reward             | 44.2622  |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 3.53e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 1.3e+03    |\n",
            "|    reward             | -43.926216 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 2.8e+04    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 729       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 391       |\n",
            "|    reward             | 17.973604 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 2.58e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 729      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -14.6    |\n",
            "|    reward             | 0.699545 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -63.3      |\n",
            "|    reward             | -116.62886 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 4.41e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 730       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -4.07e+03 |\n",
            "|    reward             | 1.918165  |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 1.31e+05  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 730        |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 38         |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -5.24e+03  |\n",
            "|    reward             | -56.200546 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 3.27e+05   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 730      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 16.3     |\n",
            "|    reward             | 0.33455  |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 2        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 730      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 108      |\n",
            "|    reward             | -11.2219 |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 216      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 730        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | 1.22e+03   |\n",
            "|    reward             | -265.06613 |\n",
            "|    std                | 1.24       |\n",
            "|    value_loss         | 1.17e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 730       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 6.22e+03  |\n",
            "|    reward             | 139.28818 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 3.21e+05  |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.5239218904119848\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 2183, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 25508803.98\n",
            "total_reward: 24508803.98\n",
            "total_cost: 6462.79\n",
            "total_trades: 11328\n",
            "Sharpe: 0.932\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 236       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 8736      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -18.4     |\n",
            "|    critic_loss     | 2.75e+04  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8635      |\n",
            "|    reward          | 44.587795 |\n",
            "----------------------------------\n",
            "day: 2183, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 35179818.42\n",
            "total_reward: 34179818.42\n",
            "total_cost: 998.98\n",
            "total_trades: 10915\n",
            "Sharpe: 0.980\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 236       |\n",
            "|    time_elapsed    | 74        |\n",
            "|    total_timesteps | 17472     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -107      |\n",
            "|    critic_loss     | 1.13e+04  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 17371     |\n",
            "|    reward          | 44.587795 |\n",
            "----------------------------------\n",
            "day: 2183, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 35179818.42\n",
            "total_reward: 34179818.42\n",
            "total_cost: 998.98\n",
            "total_trades: 10915\n",
            "Sharpe: 0.980\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 110       |\n",
            "|    total_timesteps | 26208     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -83.6     |\n",
            "|    critic_loss     | 1.48e+04  |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 26107     |\n",
            "|    reward          | 44.587795 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.5247944089547831\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "day: 2183, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1582143.15\n",
            "total_reward: 582143.15\n",
            "total_cost: 998.99\n",
            "total_trades: 10915\n",
            "Sharpe: 0.476\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.7    |\n",
            "|    critic_loss     | 251      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.765262 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1582143.15\n",
            "total_reward: 582143.15\n",
            "total_cost: 998.99\n",
            "total_trades: 10915\n",
            "Sharpe: 0.476\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 225      |\n",
            "|    time_elapsed    | 77       |\n",
            "|    total_timesteps | 17472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -9.81    |\n",
            "|    critic_loss     | 9.05     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17371    |\n",
            "|    reward          | 0.765262 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1582143.15\n",
            "total_reward: 582143.15\n",
            "total_cost: 998.99\n",
            "total_trades: 10915\n",
            "Sharpe: 0.476\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 117      |\n",
            "|    total_timesteps | 26208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.26    |\n",
            "|    critic_loss     | 2.85     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 26107    |\n",
            "|    reward          | 0.765262 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.40276138115271315\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "day: 2183, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 42758798.33\n",
            "total_reward: 41758798.33\n",
            "total_cost: 998.99\n",
            "total_trades: 8732\n",
            "Sharpe: 1.008\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 187      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 895      |\n",
            "|    critic_loss     | 1.45e+04 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 54.1581  |\n",
            "---------------------------------\n",
            "day: 2183, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 42758798.33\n",
            "total_reward: 41758798.33\n",
            "total_cost: 998.99\n",
            "total_trades: 8732\n",
            "Sharpe: 1.008\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 185      |\n",
            "|    time_elapsed    | 93       |\n",
            "|    total_timesteps | 17472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.39e+03 |\n",
            "|    critic_loss     | 1.83e+04 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17371    |\n",
            "|    reward          | 54.1581  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 186      |\n",
            "|    time_elapsed    | 140      |\n",
            "|    total_timesteps | 26208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.93e+03 |\n",
            "|    critic_loss     | 6.61e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 26107    |\n",
            "|    reward          | 54.1581  |\n",
            "---------------------------------\n",
            "day: 2183, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 42758798.33\n",
            "total_reward: 41758798.33\n",
            "total_cost: 998.99\n",
            "total_trades: 8732\n",
            "Sharpe: 1.008\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.5233073162167494\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 855        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -22.097488 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 817          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003620568 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000341    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.89e+03     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00094     |\n",
            "|    reward               | -1.8941059   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.29e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 807          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015201875 |\n",
            "|    clip_fraction        | 0.000244     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00635      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 38.3         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    reward               | -0.52748716  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 93.1         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 809           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00026242228 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00276       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 223           |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000919     |\n",
            "|    reward               | 19.209906     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 430           |\n",
            "-------------------------------------------\n",
            "day: 2183, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3630855.76\n",
            "total_reward: 2630855.76\n",
            "total_cost: 1283325.45\n",
            "total_trades: 18766\n",
            "Sharpe: 0.608\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 809           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017951534 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000128      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 503           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000723     |\n",
            "|    reward               | 2.9072742     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 999           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 804           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031231996 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00262       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 164           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00116      |\n",
            "|    reward               | -2.412815     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 416           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 804           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081499014 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00733      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 61.3          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00159      |\n",
            "|    reward               | 19.087706     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 108           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 804           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7905655e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000528      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.9e+03       |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000201     |\n",
            "|    reward               | -0.00167435   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.92e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 803          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.305846e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000766     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.16e+04     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -6.92e-05    |\n",
            "|    reward               | 0.6004997    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.53e+04     |\n",
            "------------------------------------------\n",
            "day: 2183, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5461015.40\n",
            "total_reward: 4461015.40\n",
            "total_cost: 1188733.77\n",
            "total_trades: 18831\n",
            "Sharpe: 0.967\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 802          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010595998 |\n",
            "|    clip_fraction        | 4.88e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000996    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 51.6         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00187     |\n",
            "|    reward               | -9.309983    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 74.5         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 801           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010448834 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00426      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 447           |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000548     |\n",
            "|    reward               | -10.424288    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.16e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 801           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016051883 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00304       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 852           |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000683     |\n",
            "|    reward               | 0.4415705     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.62e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 800          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008688563 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00104      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 183          |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00163     |\n",
            "|    reward               | -0.13672647  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 397          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 799          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013983779 |\n",
            "|    clip_fraction        | 9.77e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00314     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 53.4         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    reward               | -0.25976765  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 147          |\n",
            "------------------------------------------\n",
            "day: 2183, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1782885.72\n",
            "total_reward: 782885.72\n",
            "total_cost: 717139.82\n",
            "total_trades: 18536\n",
            "Sharpe: 0.418\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 798          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014781941 |\n",
            "|    clip_fraction        | 9.77e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00912     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 28.9         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    reward               | 0.7269323    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 73.3         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.041556365290237804\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 669      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.00213  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 75.3     |\n",
            "|    reward             | 1.913646 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 39.1     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 604         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13         |\n",
            "|    explained_variance | -0.0586     |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -92         |\n",
            "|    reward             | -0.26677623 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 114         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 636       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 116       |\n",
            "|    reward             | -4.120465 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 133       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 645      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 61.7     |\n",
            "|    reward             | 8.725607 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 39       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 646       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -0.332439 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.52      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 655       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 2.78e-05  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -1.95e+03 |\n",
            "|    reward             | 5.283913  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 5.53e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 664       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 756       |\n",
            "|    reward             | 5.501852  |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 6.91e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 666        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -4.28e+03  |\n",
            "|    reward             | -111.00309 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.85e+05   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 668        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0.0163     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 2.84e+03   |\n",
            "|    reward             | 0.08927979 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 7.64e+04   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 673      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 30.7     |\n",
            "|    reward             | 1.76812  |\n",
            "|    std                | 0.982    |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 673       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -2.62e-06 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -66.8     |\n",
            "|    reward             | -3.548012 |\n",
            "|    std                | 0.942     |\n",
            "|    value_loss         | 51.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 660       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -969      |\n",
            "|    reward             | 38.071285 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 8.05e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 664       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 3.28e+03  |\n",
            "|    reward             | 244.73686 |\n",
            "|    std                | 0.933     |\n",
            "|    value_loss         | 6.09e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 667       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 17.9      |\n",
            "|    reward             | -0.480045 |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 3.03      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 669       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -1.8e+03  |\n",
            "|    reward             | 114.92046 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 2.63e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 667       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 535       |\n",
            "|    reward             | -7.601661 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 2.2e+03   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 670        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -2.39e+03  |\n",
            "|    reward             | -21.166538 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.07e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 670       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -16       |\n",
            "|    reward             | 0.1514513 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.74      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 669      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 90.7     |\n",
            "|    reward             | 0.236334 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 64.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 672       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -26.1     |\n",
            "|    reward             | 41.459427 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 94.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 674       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -4.63e+03 |\n",
            "|    reward             | 340.9032  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.99e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 675       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 3.87e+03  |\n",
            "|    reward             | -40.00858 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 9.86e+04  |\n",
            "-------------------------------------\n",
            "day: 2246, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 70352306.55\n",
            "total_reward: 69352306.55\n",
            "total_cost: 1140.20\n",
            "total_trades: 11309\n",
            "Sharpe: 1.074\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 677      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 8.88     |\n",
            "|    reward             | 3.418756 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 679       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | -1.63e+03 |\n",
            "|    reward             | 161.30544 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 5.98e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 680       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -417      |\n",
            "|    reward             | 71.013535 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.29e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 646      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -667     |\n",
            "|    reward             | 67.50129 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 7.58e+04 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 648      |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 13500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | 3.54     |\n",
            "|    reward             | 2.657659 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.32     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 651       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -15.9     |\n",
            "|    reward             | -0.031079 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 18.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 653      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 312      |\n",
            "|    reward             | 48.38551 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 665      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 655       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 4.18e+03  |\n",
            "|    reward             | 472.48505 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 1.5e+05   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 657      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 448      |\n",
            "|    reward             | 72.4946  |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 1.53e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 656      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -5.92    |\n",
            "|    reward             | 0.794722 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 654      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 1.8e+03  |\n",
            "|    reward             | 35.83369 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 2.16e+04 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 654      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 721      |\n",
            "|    reward             | 0.96322  |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 3.37e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 655        |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | 5.9e+03    |\n",
            "|    reward             | -127.36199 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 1.7e+05    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 650      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 28.5     |\n",
            "|    reward             | 2.087696 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 652       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 158       |\n",
            "|    reward             | 3.842862  |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 171       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 652       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 460       |\n",
            "|    reward             | -0.087478 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.23e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 653        |\n",
            "|    iterations         | 3900       |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 19500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3899       |\n",
            "|    policy_loss        | 3.5e+03    |\n",
            "|    reward             | -203.67206 |\n",
            "|    std                | 1.15       |\n",
            "|    value_loss         | 2.19e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 655       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -626      |\n",
            "|    reward             | 233.88727 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 3.14e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 656      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | -5.21    |\n",
            "|    reward             | 0.434451 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 0.499    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 657       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | -113      |\n",
            "|    reward             | 42.220226 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 2.87e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 657        |\n",
            "|    iterations         | 4300       |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 21500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4299       |\n",
            "|    policy_loss        | -527       |\n",
            "|    reward             | -11.950439 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 2.59e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 658        |\n",
            "|    iterations         | 4400       |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 22000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4399       |\n",
            "|    policy_loss        | -1.77e+03  |\n",
            "|    reward             | -191.51392 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 2.13e+04   |\n",
            "--------------------------------------\n",
            "day: 2246, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 49757868.91\n",
            "total_reward: 48757868.91\n",
            "total_cost: 998.98\n",
            "total_trades: 15775\n",
            "Sharpe: 1.031\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 659       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -15.5     |\n",
            "|    reward             | -0.682349 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 2.43      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 660       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 110       |\n",
            "|    reward             | -9.499018 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 69.6      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 659      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -113     |\n",
            "|    reward             | 0.129888 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 107      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 660        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -5.57e+03  |\n",
            "|    reward             | -124.80548 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 2.27e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 661       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 1.7e+03   |\n",
            "|    reward             | -84.24815 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 2.77e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 662      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -3.52    |\n",
            "|    reward             | 0.747596 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 0.404    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 663      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -192     |\n",
            "|    reward             | 2.474159 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 1.82e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 664        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -206       |\n",
            "|    reward             | -140.08484 |\n",
            "|    std                | 1.28       |\n",
            "|    value_loss         | 2.14e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 663        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -6.66e+03  |\n",
            "|    reward             | -56.502907 |\n",
            "|    std                | 1.27       |\n",
            "|    value_loss         | 2.79e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 664       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -31.5     |\n",
            "|    reward             | -1.098604 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 4.5       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 665       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 47.4      |\n",
            "|    reward             | -1.704277 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 28.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 665      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -3.48    |\n",
            "|    reward             | 3.404647 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 666       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 1.81e+03  |\n",
            "|    reward             | 64.961235 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 2.6e+04   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 667      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 43       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -505     |\n",
            "|    reward             | 2.979911 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 2.74e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 668      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -6.26    |\n",
            "|    reward             | 1.166674 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 0.363    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 668       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -1.68e+03 |\n",
            "|    reward             | -0.814905 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 1.37e+04  |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.3888237175302008\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "day: 2246, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3755251.43\n",
            "total_reward: 2755251.43\n",
            "total_cost: 7055.77\n",
            "total_trades: 9512\n",
            "Sharpe: 0.693\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 204      |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 8988     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 49.7     |\n",
            "|    critic_loss     | 1.08e+04 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8887     |\n",
            "|    reward          | 208.7493 |\n",
            "---------------------------------\n",
            "day: 2246, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 70461978.21\n",
            "total_reward: 69461978.21\n",
            "total_cost: 999.00\n",
            "total_trades: 8984\n",
            "Sharpe: 1.077\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 206      |\n",
            "|    time_elapsed    | 87       |\n",
            "|    total_timesteps | 17976    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -152     |\n",
            "|    critic_loss     | 1.48e+04 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17875    |\n",
            "|    reward          | 208.7493 |\n",
            "---------------------------------\n",
            "day: 2246, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 70461978.21\n",
            "total_reward: 69461978.21\n",
            "total_cost: 999.00\n",
            "total_trades: 8984\n",
            "Sharpe: 1.077\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 208      |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 26964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -111     |\n",
            "|    critic_loss     | 1.15e+04 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 26863    |\n",
            "|    reward          | 208.7493 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.388320578080452\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "day: 2246, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 75991105.33\n",
            "total_reward: 74991105.33\n",
            "total_cost: 998.92\n",
            "total_trades: 8984\n",
            "Sharpe: 1.086\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 216       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 741       |\n",
            "|    critic_loss     | 3.18e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 226.16757 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 75991105.33\n",
            "total_reward: 74991105.33\n",
            "total_cost: 998.92\n",
            "total_trades: 8984\n",
            "Sharpe: 1.086\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 215       |\n",
            "|    time_elapsed    | 83        |\n",
            "|    total_timesteps | 17976     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 454       |\n",
            "|    critic_loss     | 2.01e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 17875     |\n",
            "|    reward          | 226.16757 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 75991105.33\n",
            "total_reward: 74991105.33\n",
            "total_cost: 998.92\n",
            "total_trades: 8984\n",
            "Sharpe: 1.086\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 214       |\n",
            "|    time_elapsed    | 125       |\n",
            "|    total_timesteps | 26964     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 680       |\n",
            "|    critic_loss     | 1.2e+04   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 26863     |\n",
            "|    reward          | 226.16757 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.38841168951984056\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "day: 2246, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 58368937.87\n",
            "total_reward: 57368937.87\n",
            "total_cost: 999.00\n",
            "total_trades: 13476\n",
            "Sharpe: 1.050\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 173       |\n",
            "|    time_elapsed    | 51        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 802       |\n",
            "|    critic_loss     | 1.4e+04   |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 172.96902 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 58368937.87\n",
            "total_reward: 57368937.87\n",
            "total_cost: 999.00\n",
            "total_trades: 13476\n",
            "Sharpe: 1.050\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 173       |\n",
            "|    time_elapsed    | 103       |\n",
            "|    total_timesteps | 17976     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.07e+03  |\n",
            "|    critic_loss     | 1.32e+04  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 17875     |\n",
            "|    reward          | 172.96902 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 174       |\n",
            "|    time_elapsed    | 154       |\n",
            "|    total_timesteps | 26964     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.36e+03  |\n",
            "|    critic_loss     | 8.56e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 26863     |\n",
            "|    reward          | 172.96902 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 58368937.87\n",
            "total_reward: 57368937.87\n",
            "total_cost: 999.00\n",
            "total_trades: 13476\n",
            "Sharpe: 1.050\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.38766966601969416\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 863        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.1595701 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 831           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021306542 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00132      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 104           |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000653     |\n",
            "|    reward               | 0.3078851     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 339           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 820          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004290962 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.0021       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 73.6         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00176     |\n",
            "|    reward               | 9.22038      |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 109          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 820           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015619738 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00089       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 254           |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00089      |\n",
            "|    reward               | 1.8510684     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 696           |\n",
            "-------------------------------------------\n",
            "day: 2246, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2811836.45\n",
            "total_reward: 1811836.45\n",
            "total_cost: 995631.01\n",
            "total_trades: 19069\n",
            "Sharpe: 0.537\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 813           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016879619 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000638     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 156           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00103      |\n",
            "|    reward               | 0.03659349    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 279           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 807           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036221195 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000548      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 249           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00146      |\n",
            "|    reward               | -0.18267308   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 439           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 805           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016587449 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000605      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 465           |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000811     |\n",
            "|    reward               | -1.7567428    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 928           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 806           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00073141104 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00446       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 83.7          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00192      |\n",
            "|    reward               | 0.46786642    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 128           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 807          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002144929 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00278      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 81.4         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    reward               | -0.2016754   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 148          |\n",
            "------------------------------------------\n",
            "day: 2246, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3133701.80\n",
            "total_reward: 2133701.80\n",
            "total_cost: 1000220.82\n",
            "total_trades: 19174\n",
            "Sharpe: 0.622\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 807          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.383057e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000859     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 524          |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000461    |\n",
            "|    reward               | -0.11803341  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.16e+03     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 808           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043614532 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00138       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 53.1          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00162      |\n",
            "|    reward               | -0.14468579   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 180           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 808          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004107686 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00305      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 47.8         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    reward               | -0.11975397  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 126          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 808           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022293392 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00154       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 76.8          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.0012       |\n",
            "|    reward               | -0.57941693   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 183           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 810           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010438336 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000967      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 88            |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000618     |\n",
            "|    reward               | 0.1465792     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 288           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 810           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035118638 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.0022        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 125           |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00145      |\n",
            "|    reward               | -1.6727463    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 219           |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.08783155820801954\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 719      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -10.3    |\n",
            "|    reward             | -0.23295 |\n",
            "|    std                | 0.966    |\n",
            "|    value_loss         | 0.574    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 9.24     |\n",
            "|    reward             | 0.144093 |\n",
            "|    std                | 0.965    |\n",
            "|    value_loss         | 0.62     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 703       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 34        |\n",
            "|    reward             | -1.164862 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 9.35      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 708       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 11.5      |\n",
            "|    reward             | 1.729935  |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 1.36      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 710      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 39.5     |\n",
            "|    reward             | 7.248392 |\n",
            "|    std                | 0.977    |\n",
            "|    value_loss         | 9.95     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0.000866  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 228       |\n",
            "|    reward             | 26.886185 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 647       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 715      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -844     |\n",
            "|    reward             | -96.9207 |\n",
            "|    std                | 0.973    |\n",
            "|    value_loss         | 5.61e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 8.62e+03 |\n",
            "|    reward             | 61.6844  |\n",
            "|    std                | 0.94     |\n",
            "|    value_loss         | 8.13e+05 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 1.17e+03  |\n",
            "|    reward             | 19.372026 |\n",
            "|    std                | 0.932     |\n",
            "|    value_loss         | 1.13e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -5.53     |\n",
            "|    reward             | 2.4492025 |\n",
            "|    std                | 0.93      |\n",
            "|    value_loss         | 0.849     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 719      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 1.56e+03 |\n",
            "|    reward             | -74.1709 |\n",
            "|    std                | 0.927    |\n",
            "|    value_loss         | 2.47e+04 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -53.9     |\n",
            "|    reward             | 253.36563 |\n",
            "|    std                | 0.928     |\n",
            "|    value_loss         | 80.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 720        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -61.5      |\n",
            "|    reward             | -11.316192 |\n",
            "|    std                | 0.955      |\n",
            "|    value_loss         | 2.47e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -3.81e-06 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -17.4     |\n",
            "|    reward             | -0.71206  |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.72      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 181      |\n",
            "|    reward             | 20.29234 |\n",
            "|    std                | 0.961    |\n",
            "|    value_loss         | 257      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -294      |\n",
            "|    reward             | 34.615486 |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 902       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.2      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -8.43e+03  |\n",
            "|    reward             | -22.175875 |\n",
            "|    std                | 0.965      |\n",
            "|    value_loss         | 7.51e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -980      |\n",
            "|    reward             | 33.070282 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 8.88e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -3.58e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -5.29     |\n",
            "|    reward             | 1.343161  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.11      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | -7.95e-05  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -1.27e+03  |\n",
            "|    reward             | -13.403093 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 1.26e+04   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | 1.1e+03    |\n",
            "|    reward             | -12.441623 |\n",
            "|    std                | 0.969      |\n",
            "|    value_loss         | 8.15e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -2.64e+03 |\n",
            "|    reward             | 39.90287  |\n",
            "|    std                | 0.963     |\n",
            "|    value_loss         | 6.56e+04  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 723        |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -4.37e+03  |\n",
            "|    reward             | -15.070726 |\n",
            "|    std                | 0.975      |\n",
            "|    value_loss         | 1.65e+05   |\n",
            "--------------------------------------\n",
            "day: 2309, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 80551921.95\n",
            "total_reward: 79551921.95\n",
            "total_cost: 1348.97\n",
            "total_trades: 15798\n",
            "Sharpe: 1.096\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 723      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -15      |\n",
            "|    reward             | 0.23188  |\n",
            "|    std                | 0.978    |\n",
            "|    value_loss         | 7.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 723      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -75.7    |\n",
            "|    reward             | 7.162289 |\n",
            "|    std                | 0.97     |\n",
            "|    value_loss         | 82.3     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 1.31e+03  |\n",
            "|    reward             | 53.506805 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 1.23e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -108      |\n",
            "|    reward             | 34.836643 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 478       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -7.71     |\n",
            "|    reward             | -1.407181 |\n",
            "|    std                | 0.961     |\n",
            "|    value_loss         | 0.711     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 342       |\n",
            "|    reward             | 24.870163 |\n",
            "|    std                | 0.96      |\n",
            "|    value_loss         | 2.58e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | -0.000721  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | 1.15e+03   |\n",
            "|    reward             | -62.002277 |\n",
            "|    std                | 0.947      |\n",
            "|    value_loss         | 1.98e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 3.81e-06  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 5.79e+03  |\n",
            "|    reward             | 231.96013 |\n",
            "|    std                | 0.967     |\n",
            "|    value_loss         | 3.1e+05   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 722      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -355     |\n",
            "|    reward             | 0.843768 |\n",
            "|    std                | 0.942    |\n",
            "|    value_loss         | 2.43e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 722      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -10.8    |\n",
            "|    reward             | 0.426534 |\n",
            "|    std                | 0.955    |\n",
            "|    value_loss         | 0.751    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -1.09e+03 |\n",
            "|    reward             | 19.6958   |\n",
            "|    std                | 0.944     |\n",
            "|    value_loss         | 1.01e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 1.51e+03  |\n",
            "|    reward             | 52.962772 |\n",
            "|    std                | 0.958     |\n",
            "|    value_loss         | 1.45e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 719       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -796      |\n",
            "|    reward             | -654.1156 |\n",
            "|    std                | 0.966     |\n",
            "|    value_loss         | 1.48e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | -542     |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 138      |\n",
            "|    reward             | 0.468986 |\n",
            "|    std                | 0.968    |\n",
            "|    value_loss         | 193      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 40.3     |\n",
            "|    reward             | 0.213192 |\n",
            "|    std                | 0.958    |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 101      |\n",
            "|    reward             | -4.60848 |\n",
            "|    std                | 0.968    |\n",
            "|    value_loss         | 194      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -0.000126 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 1.14e+04  |\n",
            "|    reward             | 178.81708 |\n",
            "|    std                | 0.972     |\n",
            "|    value_loss         | 1.05e+06  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | 467        |\n",
            "|    reward             | -120.08138 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 2.22e+03   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 717      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 1.73     |\n",
            "|    reward             | 0.851712 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 0.281    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 717      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 24       |\n",
            "|    reward             | 0.149378 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -5.91     |\n",
            "|    reward             | -0.540102 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.509     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 185       |\n",
            "|    reward             | -1.048206 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 167       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 67.2      |\n",
            "|    reward             | 3.917347  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 41.2      |\n",
            "-------------------------------------\n",
            "day: 2309, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3709022.61\n",
            "total_reward: 2709022.61\n",
            "total_cost: 3081.56\n",
            "total_trades: 15558\n",
            "Sharpe: 0.800\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 718      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -6.97    |\n",
            "|    reward             | 0.004302 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.361    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 15        |\n",
            "|    reward             | -0.129881 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 1.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 14.6      |\n",
            "|    reward             | 0.713774  |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 2.13      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 29.2     |\n",
            "|    reward             | 0.258304 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 6.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 715      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -14.2    |\n",
            "|    reward             | 0.392518 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -1.79     |\n",
            "|    reward             | -0.401972 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.101     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -12.9     |\n",
            "|    reward             | -0.028224 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -5.89    |\n",
            "|    reward             | 0.617599 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 714      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | -1.65    |\n",
            "|    reward             | 0.392179 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 0.248    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 14.4      |\n",
            "|    reward             | -0.410699 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 1.99      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -29.1     |\n",
            "|    reward             | 0.292726  |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 5.66      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 40       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -78.6    |\n",
            "|    reward             | 4.086306 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 36.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -22.6     |\n",
            "|    reward             | 2.849045  |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 2.8       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 9.96     |\n",
            "|    reward             | 0.308069 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.36371832378547825\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "day: 2309, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 145210714.21\n",
            "total_reward: 144210714.21\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 1.163\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 227        |\n",
            "|    time_elapsed    | 40         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -119       |\n",
            "|    critic_loss     | 4.9e+04    |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -141.74176 |\n",
            "-----------------------------------\n",
            "day: 2309, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 145210714.21\n",
            "total_reward: 144210714.21\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 1.163\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 8          |\n",
            "|    fps             | 226        |\n",
            "|    time_elapsed    | 81         |\n",
            "|    total_timesteps | 18480      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -310       |\n",
            "|    critic_loss     | 4.88e+04   |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 18379      |\n",
            "|    reward          | -141.74176 |\n",
            "-----------------------------------\n",
            "day: 2309, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 145210714.21\n",
            "total_reward: 144210714.21\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 1.163\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 12         |\n",
            "|    fps             | 226        |\n",
            "|    time_elapsed    | 122        |\n",
            "|    total_timesteps | 27720      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -348       |\n",
            "|    critic_loss     | 3.64e+04   |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 27619      |\n",
            "|    reward          | -141.74176 |\n",
            "-----------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.22854894938893947\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "day: 2309, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 142967941.65\n",
            "total_reward: 141967941.65\n",
            "total_cost: 999.00\n",
            "total_trades: 9236\n",
            "Sharpe: 1.160\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 233        |\n",
            "|    time_elapsed    | 39         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 482        |\n",
            "|    critic_loss     | 6.22e+04   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -140.41393 |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 8          |\n",
            "|    fps             | 233        |\n",
            "|    time_elapsed    | 79         |\n",
            "|    total_timesteps | 18480      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 401        |\n",
            "|    critic_loss     | 6.02e+04   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 18379      |\n",
            "|    reward          | -140.41393 |\n",
            "-----------------------------------\n",
            "day: 2309, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 142967941.65\n",
            "total_reward: 141967941.65\n",
            "total_cost: 999.00\n",
            "total_trades: 9236\n",
            "Sharpe: 1.160\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 12         |\n",
            "|    fps             | 233        |\n",
            "|    time_elapsed    | 118        |\n",
            "|    total_timesteps | 27720      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 580        |\n",
            "|    critic_loss     | 8.92e+04   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 27619      |\n",
            "|    reward          | -140.41393 |\n",
            "-----------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  -0.22715538047799558\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "day: 2309, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 120643763.99\n",
            "total_reward: 119643763.99\n",
            "total_cost: 6769.88\n",
            "total_trades: 7527\n",
            "Sharpe: 1.140\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 187        |\n",
            "|    time_elapsed    | 49         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 709        |\n",
            "|    critic_loss     | 4.49e+04   |\n",
            "|    ent_coef        | 0.1        |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -147.21646 |\n",
            "-----------------------------------\n",
            "day: 2309, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 150602434.00\n",
            "total_reward: 149602434.00\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 1.168\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 8          |\n",
            "|    fps             | 186        |\n",
            "|    time_elapsed    | 98         |\n",
            "|    total_timesteps | 18480      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 1.55e+03   |\n",
            "|    critic_loss     | 4.97e+04   |\n",
            "|    ent_coef        | 0.1        |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 18379      |\n",
            "|    reward          | -147.21646 |\n",
            "-----------------------------------\n",
            "day: 2309, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 150602434.00\n",
            "total_reward: 149602434.00\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 1.168\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 12         |\n",
            "|    fps             | 183        |\n",
            "|    time_elapsed    | 151        |\n",
            "|    total_timesteps | 27720      |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 2.39e+03   |\n",
            "|    critic_loss     | 4.31e+04   |\n",
            "|    ent_coef        | 0.1        |\n",
            "|    learning_rate   | 0.005      |\n",
            "|    n_updates       | 27619      |\n",
            "|    reward          | -147.21646 |\n",
            "-----------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  -0.2296851631179197\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 829       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -3.278229 |\n",
            "----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 798           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 5             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035510413 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 2.5e-06       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 165           |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000924     |\n",
            "|    reward               | -40.943127    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 380           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 793          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.449694e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 6.82e-05     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 6.62e+03     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -8.79e-05    |\n",
            "|    reward               | -0.8692839   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.03e+04     |\n",
            "------------------------------------------\n",
            "day: 2309, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2230360.32\n",
            "total_reward: 1230360.32\n",
            "total_cost: 733042.03\n",
            "total_trades: 19847\n",
            "Sharpe: 0.484\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 784           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022458975 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -7.38e-05     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.18e+03      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000514     |\n",
            "|    reward               | 0.6800957     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.46e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 782           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031437224 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00176       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 203           |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00103      |\n",
            "|    reward               | -0.67125386   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 562           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 783           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015227869 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00265      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 200           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000667     |\n",
            "|    reward               | 0.4813201     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 495           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 783          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003485557 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00202     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 240          |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    reward               | 1.3580556    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 510          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 783           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013441592 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000577     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 353           |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00065      |\n",
            "|    reward               | 1.7820032     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 694           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 782           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6993727e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000457     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.38e+04      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -6.24e-05     |\n",
            "|    reward               | 1.016599      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.19e+04      |\n",
            "-------------------------------------------\n",
            "day: 2309, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2187655.74\n",
            "total_reward: 1187655.74\n",
            "total_cost: 807610.31\n",
            "total_trades: 19714\n",
            "Sharpe: 0.486\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 780          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013756944 |\n",
            "|    clip_fraction        | 9.77e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00153      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 41.8         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00257     |\n",
            "|    reward               | 1.2891434    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 100          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 779           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018461893 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00349      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 93.9          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000729     |\n",
            "|    reward               | -2.5820909    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 146           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 779           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 31            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4940597e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000355     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.85e+03      |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000185     |\n",
            "|    reward               | 0.17899448    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.83e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 778          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 34           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.454585e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000934    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.5e+03      |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000171    |\n",
            "|    reward               | 1.1036642    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.95e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 778          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.135302e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00131      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 577          |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000424    |\n",
            "|    reward               | -6.9324      |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.42e+03     |\n",
            "------------------------------------------\n",
            "day: 2309, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 23977382.50\n",
            "total_reward: 22977382.50\n",
            "total_cost: 2105117.27\n",
            "total_trades: 20186\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 778           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 39            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4743942e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000677      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 9.77e+03      |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -9.3e-05      |\n",
            "|    reward               | 0.48964277    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.74e+04      |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  -0.22069396408371847\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 677       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 35.5      |\n",
            "|    reward             | 0.8707799 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 12.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 685       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 12.9      |\n",
            "|    reward             | -0.005983 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.57      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 670       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.00478  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 105       |\n",
            "|    reward             | -2.351185 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 101       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 676      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 46.7     |\n",
            "|    reward             | 5.31131  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 19.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 678       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -1.75     |\n",
            "|    reward             | -0.155204 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.18      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 680       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 1.96      |\n",
            "|    reward             | -0.077192 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.0649    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 683       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -0.0177   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -9.56     |\n",
            "|    reward             | -0.960023 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.809     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 684       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -18       |\n",
            "|    reward             | 3.895512  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.35      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 686      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 8.59     |\n",
            "|    reward             | 0.914142 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.43     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 686       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 1.18      |\n",
            "|    reward             | 0.438777  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.384     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 688       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 21.7      |\n",
            "|    reward             | -1.337298 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.81      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 689       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 18.6      |\n",
            "|    reward             | -2.735242 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.43      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 691       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 8.35      |\n",
            "|    reward             | -1.294226 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 3.74      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 692      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -2.87    |\n",
            "|    reward             | -1.49132 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.976    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 692        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 0.505      |\n",
            "|    reward             | 0.91959226 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.0368     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 693       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 13.7      |\n",
            "|    reward             | -0.397593 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.998     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 694       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 19.4      |\n",
            "|    reward             | -1.093945 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 2.68      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 694       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.73      |\n",
            "|    reward             | -1.707794 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 3.92      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 695      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | -0.87    |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -76.6    |\n",
            "|    reward             | 0.298216 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 91.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0.00792   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -8.44     |\n",
            "|    reward             | -0.221796 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.709     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 4.61     |\n",
            "|    reward             | 0.844388 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.774    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 8.84     |\n",
            "|    reward             | 1.397497 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 20.6      |\n",
            "|    reward             | -0.387085 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 6.48      |\n",
            "-------------------------------------\n",
            "day: 2372, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1855875.77\n",
            "total_reward: 855875.77\n",
            "total_cost: 2869.61\n",
            "total_trades: 11597\n",
            "Sharpe: 0.443\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -13.7    |\n",
            "|    reward             | 0.381964 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -6.97    |\n",
            "|    reward             | -0.32741 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.315    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -1.07    |\n",
            "|    reward             | 1.525994 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 0.151    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 698       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 32        |\n",
            "|    reward             | -0.715924 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 8.27      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 698       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -4.55     |\n",
            "|    reward             | -1.642204 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 0.611     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -26      |\n",
            "|    reward             | 1.211531 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 4.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -48.4    |\n",
            "|    reward             | 1.231197 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -8.19     |\n",
            "|    reward             | 0.093587  |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 1.62      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 698      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -46.6    |\n",
            "|    reward             | 3.445075 |\n",
            "|    std                | 1.21     |\n",
            "|    value_loss         | 24.6     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 698       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 25.5      |\n",
            "|    reward             | 1.463297  |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 4.4       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 698      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | -0.339   |\n",
            "|    reward             | 0.050401 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.0945   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 699       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 13.2      |\n",
            "|    reward             | -0.524385 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.916     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 699      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 10.1     |\n",
            "|    reward             | 0.119067 |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 0.968    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 699       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 26.2      |\n",
            "|    reward             | 1.968489  |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 3.71      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 699        |\n",
            "|    iterations         | 3800       |\n",
            "|    time_elapsed       | 27         |\n",
            "|    total_timesteps    | 19000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3799       |\n",
            "|    policy_loss        | 3.7        |\n",
            "|    reward             | 0.01262938 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 0.119      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 699       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 10.4      |\n",
            "|    reward             | 0.138792  |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.518     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 699      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -8.69    |\n",
            "|    reward             | -0.0579  |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 0.564    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 28        |\n",
            "|    reward             | 0.246411  |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 5.91      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 21.3      |\n",
            "|    reward             | -1.797946 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 2.84      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -10.7    |\n",
            "|    reward             | 2.282952 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.635    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 0.569    |\n",
            "|    reward             | 0.070398 |\n",
            "|    std                | 1.27     |\n",
            "|    value_loss         | 0.702    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 5.86     |\n",
            "|    reward             | 1.618063 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | -9.39     |\n",
            "|    reward             | 0.643561  |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 0.594     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 0.382     |\n",
            "|    reward             | -0.810007 |\n",
            "|    std                | 1.38      |\n",
            "|    value_loss         | 0.502     |\n",
            "-------------------------------------\n",
            "day: 2372, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1939398.25\n",
            "total_reward: 939398.25\n",
            "total_cost: 1358.26\n",
            "total_trades: 11932\n",
            "Sharpe: 0.533\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 23.5      |\n",
            "|    reward             | -0.418735 |\n",
            "|    std                | 1.42      |\n",
            "|    value_loss         | 3.21      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 29       |\n",
            "|    reward             | 0.030097 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 6.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 11.5     |\n",
            "|    reward             | 0.571418 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 0.832    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -7.99     |\n",
            "|    reward             | -2.758392 |\n",
            "|    std                | 1.43      |\n",
            "|    value_loss         | 10.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 1.23     |\n",
            "|    reward             | 0.599366 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -1.88    |\n",
            "|    reward             | 0.528345 |\n",
            "|    std                | 1.43     |\n",
            "|    value_loss         | 0.0888   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -5.47     |\n",
            "|    reward             | -0.403277 |\n",
            "|    std                | 1.43      |\n",
            "|    value_loss         | 0.537     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 0.11      |\n",
            "|    reward             | -0.565592 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 0.0783    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 14.8     |\n",
            "|    reward             | 0.145025 |\n",
            "|    std                | 1.43     |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 701        |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -15.3      |\n",
            "|    explained_variance | -76        |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | 59.5       |\n",
            "|    reward             | 0.29087502 |\n",
            "|    std                | 1.41       |\n",
            "|    value_loss         | 19.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 0.567     |\n",
            "|    reward             | -0.828067 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 0.0633    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 14.6     |\n",
            "|    reward             | 0.021852 |\n",
            "|    std                | 1.44     |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 16.3      |\n",
            "|    reward             | -1.444994 |\n",
            "|    std                | 1.46      |\n",
            "|    value_loss         | 1.7       |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.16895132497638787\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "day: 2372, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1732873.44\n",
            "total_reward: 732873.44\n",
            "total_cost: 999.00\n",
            "total_trades: 7116\n",
            "Sharpe: 0.513\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 222      |\n",
            "|    time_elapsed    | 42       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 66.5     |\n",
            "|    critic_loss     | 91.8     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | -0.75227 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1732873.44\n",
            "total_reward: 732873.44\n",
            "total_cost: 999.00\n",
            "total_trades: 7116\n",
            "Sharpe: 0.513\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 222      |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 18984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 41       |\n",
            "|    critic_loss     | 81.8     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 18883    |\n",
            "|    reward          | -0.75227 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1732873.44\n",
            "total_reward: 732873.44\n",
            "total_cost: 999.00\n",
            "total_trades: 7116\n",
            "Sharpe: 0.513\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 220      |\n",
            "|    time_elapsed    | 129      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 29.3     |\n",
            "|    critic_loss     | 4.23     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 28375    |\n",
            "|    reward          | -0.75227 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3036851216897835\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "day: 2372, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1190781.78\n",
            "total_reward: 190781.78\n",
            "total_cost: 998.99\n",
            "total_trades: 11860\n",
            "Sharpe: 0.220\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 207      |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 142      |\n",
            "|    critic_loss     | 295      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.216555 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 203      |\n",
            "|    time_elapsed    | 93       |\n",
            "|    total_timesteps | 18984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 152      |\n",
            "|    critic_loss     | 95.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 18883    |\n",
            "|    reward          | 0.216555 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1190781.78\n",
            "total_reward: 190781.78\n",
            "total_cost: 998.99\n",
            "total_trades: 11860\n",
            "Sharpe: 0.220\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 206      |\n",
            "|    time_elapsed    | 137      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 106      |\n",
            "|    critic_loss     | 46.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28375    |\n",
            "|    reward          | 0.216555 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.2086237095310019\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "day: 2372, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1739006.78\n",
            "total_reward: 739006.78\n",
            "total_cost: 7558.88\n",
            "total_trades: 12243\n",
            "Sharpe: 0.469\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 180      |\n",
            "|    time_elapsed    | 52       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 926      |\n",
            "|    critic_loss     | 3.21e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | -0.29662 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1588871.37\n",
            "total_reward: 588871.37\n",
            "total_cost: 998.99\n",
            "total_trades: 11860\n",
            "Sharpe: 0.463\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 179      |\n",
            "|    time_elapsed    | 105      |\n",
            "|    total_timesteps | 18984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.08e+03 |\n",
            "|    critic_loss     | 1.27e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 18883    |\n",
            "|    reward          | -0.29662 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1588871.37\n",
            "total_reward: 588871.37\n",
            "total_cost: 998.99\n",
            "total_trades: 11860\n",
            "Sharpe: 0.463\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 179      |\n",
            "|    time_elapsed    | 158      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.2e+03  |\n",
            "|    critic_loss     | 393      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 28375    |\n",
            "|    reward          | -0.29662 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.28238217956050826\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 839       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -6.541518 |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 810          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005635492 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0016      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 383          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00115     |\n",
            "|    reward               | -0.8495689   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 812          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 799           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038382076 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000568      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 426           |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00102      |\n",
            "|    reward               | 0.003269794   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.02e+03      |\n",
            "-------------------------------------------\n",
            "day: 2372, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1636450.36\n",
            "total_reward: 636450.36\n",
            "total_cost: 959429.70\n",
            "total_trades: 20215\n",
            "Sharpe: 0.326\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 787          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020851037 |\n",
            "|    clip_fraction        | 0.00127      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00285      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 77.7         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    reward               | 0.6729811    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 239          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 783           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00069235894 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00663       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 67.8          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00189      |\n",
            "|    reward               | -2.8578808    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 173           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 783          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 15           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.393552e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00121      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 311          |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000614    |\n",
            "|    reward               | 0.26821187   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 763          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 782           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010001706 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000248      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.61e+03      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000677     |\n",
            "|    reward               | -0.20741194   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.31e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 781          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010236192 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00662     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 29.4         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    reward               | -0.055998832 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 63.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 781         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000681591 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.0109      |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 23.4        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0014     |\n",
            "|    reward               | -5.9341793  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 45.4        |\n",
            "-----------------------------------------\n",
            "day: 2372, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1726201.55\n",
            "total_reward: 726201.55\n",
            "total_cost: 761477.86\n",
            "total_trades: 20217\n",
            "Sharpe: 0.376\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 744           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00047998095 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00181       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 72.8          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00167      |\n",
            "|    reward               | -0.62288624   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 208           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 746           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020375772 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.0096        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 29.5          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000552     |\n",
            "|    reward               | -0.13322207   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 55.8          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 749           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017854242 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00327       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 80.8          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00075      |\n",
            "|    reward               | -0.21327911   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 192           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 751          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003185975 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00338      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 65.4         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00119     |\n",
            "|    reward               | 0.17271776   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 180          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 752          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004113011 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.015       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 19.2         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00129     |\n",
            "|    reward               | -1.5126387   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 40.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 754          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003228753 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00973     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 39.6         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    reward               | -0.37565994  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 81.2         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.03730147213440311\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 691       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 3.2       |\n",
            "|    reward             | -0.753194 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.502     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0.137     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -5.48     |\n",
            "|    reward             | 0.5582708 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.495     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | 0.0291    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 196       |\n",
            "|    reward             | -5.638998 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 272       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 42.1     |\n",
            "|    reward             | 1.4036   |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 9.96     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 695        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -16.7      |\n",
            "|    reward             | -0.5482496 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 1.87       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 294      |\n",
            "|    reward             | 1.729104 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 424      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 696        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.5      |\n",
            "|    explained_variance | -0.026     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 527        |\n",
            "|    reward             | -122.07819 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 3.3e+03    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | -0.000477 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 2.43e+04  |\n",
            "|    reward             | 272.33984 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 3.64e+06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -311      |\n",
            "|    reward             | 21.266054 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 5.65e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -2.87    |\n",
            "|    reward             | -0.0735  |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 0.46     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 696         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.9       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 2.93        |\n",
            "|    reward             | 0.082330704 |\n",
            "|    std                | 1.15        |\n",
            "|    value_loss         | 0.119       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -4.57    |\n",
            "|    reward             | 0.089101 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 0.157    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 697         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14.4       |\n",
            "|    explained_variance | 0.00396     |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 11.7        |\n",
            "|    reward             | -0.15206409 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 1.31        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 18.4     |\n",
            "|    reward             | 0.35244  |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 36.8     |\n",
            "|    reward             | 0.447253 |\n",
            "|    std                | 1.22     |\n",
            "|    value_loss         | 7.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 94.4     |\n",
            "|    reward             | 15.11594 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 87.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.8     |\n",
            "|    explained_variance | -3.58e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -117      |\n",
            "|    reward             | 53.82086  |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 111       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 692       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.08e+03  |\n",
            "|    reward             | 78.040115 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.18e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 692       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -112      |\n",
            "|    reward             | -5.045313 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 170       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 692       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -38.2     |\n",
            "|    reward             | -3.187861 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 11.3      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 692        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | -64.3      |\n",
            "|    reward             | -215.51602 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 3.5e+03    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 693        |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -408       |\n",
            "|    reward             | -21.118608 |\n",
            "|    std                | 1.22       |\n",
            "|    value_loss         | 838        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 693       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -156      |\n",
            "|    reward             | -801.4884 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 2.72e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 693      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 1.44e+03 |\n",
            "|    reward             | 53.4142  |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 3.3e+04  |\n",
            "------------------------------------\n",
            "day: 2435, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 99593481.59\n",
            "total_reward: 98593481.59\n",
            "total_cost: 1484041.96\n",
            "total_trades: 10036\n",
            "Sharpe: 1.084\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 693        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14.2      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | 2.72       |\n",
            "|    reward             | 0.08906408 |\n",
            "|    std                | 1.27       |\n",
            "|    value_loss         | 0.335      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 693      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 17.1     |\n",
            "|    reward             | 1.145328 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 693       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -17.8     |\n",
            "|    reward             | -1.511833 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 4.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 692       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -20.8     |\n",
            "|    reward             | -0.138143 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 2.42      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 693      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -64.5    |\n",
            "|    reward             | 1.022806 |\n",
            "|    std                | 1.33     |\n",
            "|    value_loss         | 22.3     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 693       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -9.54     |\n",
            "|    reward             | -0.203585 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 0.831     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 693      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -13.6    |\n",
            "|    reward             | 0.240254 |\n",
            "|    std                | 1.37     |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -18.3    |\n",
            "|    reward             | 0.651996 |\n",
            "|    std                | 1.34     |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 9.58     |\n",
            "|    reward             | 1.167493 |\n",
            "|    std                | 1.34     |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 694       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 62.6      |\n",
            "|    reward             | -1.856106 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 23.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | -3.94    |\n",
            "|    reward             | -0.68509 |\n",
            "|    std                | 1.35     |\n",
            "|    value_loss         | 0.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 3.84     |\n",
            "|    reward             | 1.229456 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 0.753    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 694       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 1.78      |\n",
            "|    reward             | -0.488627 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 0.259     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | -4.6     |\n",
            "|    reward             | 0.032335 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 695         |\n",
            "|    iterations         | 3900        |\n",
            "|    time_elapsed       | 28          |\n",
            "|    total_timesteps    | 19500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3899        |\n",
            "|    policy_loss        | -4.79       |\n",
            "|    reward             | -0.27185404 |\n",
            "|    std                | 1.29        |\n",
            "|    value_loss         | 0.144       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 13.5      |\n",
            "|    reward             | -0.181728 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 1.14      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 695      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 12.6     |\n",
            "|    reward             | 0.160551 |\n",
            "|    std                | 1.36     |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 695      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -25.3    |\n",
            "|    reward             | -0.16338 |\n",
            "|    std                | 1.36     |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 9.31      |\n",
            "|    reward             | -1.155153 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.862     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | -0.0306   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 11.8      |\n",
            "|    reward             | -0.171918 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 0.757     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -13       |\n",
            "|    reward             | -0.174955 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 1         |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 9.2       |\n",
            "|    reward             | -1.282435 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 0.6       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 7.96     |\n",
            "|    reward             | 0.850509 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 0.822    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -27.8     |\n",
            "|    reward             | -1.104375 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 4.42      |\n",
            "-------------------------------------\n",
            "day: 2435, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1610188.94\n",
            "total_reward: 610188.94\n",
            "total_cost: 1796.45\n",
            "total_trades: 9428\n",
            "Sharpe: 0.395\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 3.76     |\n",
            "|    reward             | -0.71063 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 0.169    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -3.31    |\n",
            "|    reward             | 0.382395 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 0.0625   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -5.89    |\n",
            "|    reward             | 0.264151 |\n",
            "|    std                | 1.41     |\n",
            "|    value_loss         | 0.896    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 3.77      |\n",
            "|    reward             | -0.903758 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 0.262     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -22       |\n",
            "|    reward             | -0.403287 |\n",
            "|    std                | 1.44      |\n",
            "|    value_loss         | 2.18      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -35.7     |\n",
            "|    reward             | -0.230102 |\n",
            "|    std                | 1.46      |\n",
            "|    value_loss         | 17.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 401       |\n",
            "|    reward             | 37.45863  |\n",
            "|    std                | 1.48      |\n",
            "|    value_loss         | 664       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 40       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 2.55     |\n",
            "|    reward             | 4.25524  |\n",
            "|    std                | 1.48     |\n",
            "|    value_loss         | 36.7     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 40        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 387       |\n",
            "|    reward             | -2.455342 |\n",
            "|    std                | 1.47      |\n",
            "|    value_loss         | 1.56e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 41        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 476       |\n",
            "|    reward             | 0.717387  |\n",
            "|    std                | 1.47      |\n",
            "|    value_loss         | 2.48e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 73.9     |\n",
            "|    reward             | 0.414859 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 30       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 43        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.9     |\n",
            "|    explained_variance | -0.00433  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 2.12e+03  |\n",
            "|    reward             | 127.72176 |\n",
            "|    std                | 1.44      |\n",
            "|    value_loss         | 2.5e+04   |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.15793428838925172\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "day: 2435, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 69626176.77\n",
            "total_reward: 68626176.77\n",
            "total_cost: 998.98\n",
            "total_trades: 17045\n",
            "Sharpe: 1.034\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 222      |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 137      |\n",
            "|    critic_loss     | 1.52e+04 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 148.5279 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 69626176.77\n",
            "total_reward: 68626176.77\n",
            "total_cost: 998.98\n",
            "total_trades: 17045\n",
            "Sharpe: 1.034\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 219      |\n",
            "|    time_elapsed    | 88       |\n",
            "|    total_timesteps | 19488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 15.1     |\n",
            "|    critic_loss     | 1.09e+04 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 19387    |\n",
            "|    reward          | 148.5279 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 69626176.77\n",
            "total_reward: 68626176.77\n",
            "total_cost: 998.98\n",
            "total_trades: 17045\n",
            "Sharpe: 1.034\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 218      |\n",
            "|    time_elapsed    | 133      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -132     |\n",
            "|    critic_loss     | 8.47e+03 |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 29131    |\n",
            "|    reward          | 148.5279 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.48136377732126484\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "day: 2435, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 82244940.44\n",
            "total_reward: 81244940.44\n",
            "total_cost: 998.96\n",
            "total_trades: 14596\n",
            "Sharpe: 1.056\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 210       |\n",
            "|    time_elapsed    | 46        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.93e+03  |\n",
            "|    critic_loss     | 1.74e+05  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 176.23227 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 200       |\n",
            "|    time_elapsed    | 97        |\n",
            "|    total_timesteps | 19488     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.51e+03  |\n",
            "|    critic_loss     | 2.62e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 19387     |\n",
            "|    reward          | 176.23227 |\n",
            "----------------------------------\n",
            "day: 2435, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 82244940.44\n",
            "total_reward: 81244940.44\n",
            "total_cost: 998.96\n",
            "total_trades: 14596\n",
            "Sharpe: 1.056\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 200       |\n",
            "|    time_elapsed    | 145       |\n",
            "|    total_timesteps | 29232     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.15e+03  |\n",
            "|    critic_loss     | 1.71e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 29131     |\n",
            "|    reward          | 176.23227 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.48168133862191004\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "day: 2435, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1377260.07\n",
            "total_reward: 377260.07\n",
            "total_cost: 7264.83\n",
            "total_trades: 10228\n",
            "Sharpe: 0.314\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 169      |\n",
            "|    time_elapsed    | 57       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 656      |\n",
            "|    critic_loss     | 907      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 0.505484 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1405525.54\n",
            "total_reward: 405525.54\n",
            "total_cost: 999.00\n",
            "total_trades: 9740\n",
            "Sharpe: 0.328\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 168      |\n",
            "|    time_elapsed    | 115      |\n",
            "|    total_timesteps | 19488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 869      |\n",
            "|    critic_loss     | 1.1e+03  |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 19387    |\n",
            "|    reward          | 0.505484 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1405525.54\n",
            "total_reward: 405525.54\n",
            "total_cost: 999.00\n",
            "total_trades: 9740\n",
            "Sharpe: 0.328\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 168      |\n",
            "|    time_elapsed    | 173      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 925      |\n",
            "|    critic_loss     | 257      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 29131    |\n",
            "|    reward          | 0.505484 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.22763525678493385\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 789       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -48.78801 |\n",
            "----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 761           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 5             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00076573144 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000373     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.22e+04      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00184      |\n",
            "|    reward               | -0.022224354  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.03e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 750          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005043397 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00105      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.78e+03     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00102     |\n",
            "|    reward               | -0.32362878  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.46e+03     |\n",
            "------------------------------------------\n",
            "day: 2435, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3178839.44\n",
            "total_reward: 2178839.44\n",
            "total_cost: 1092661.08\n",
            "total_trades: 20742\n",
            "Sharpe: 0.583\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 746          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012147466 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000257     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 35.2         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    reward               | -0.7196699   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 65.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 749          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002660311 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00393      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 78.1         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    reward               | -0.13056277  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 126          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 746          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006286788 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00116      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 286          |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00194     |\n",
            "|    reward               | -0.14267065  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 546          |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 749            |\n",
            "|    iterations           | 7              |\n",
            "|    time_elapsed         | 19             |\n",
            "|    total_timesteps      | 14336          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000121388526 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -12.8          |\n",
            "|    explained_variance   | -0.000996      |\n",
            "|    learning_rate        | 1e-05          |\n",
            "|    loss                 | 365            |\n",
            "|    n_updates            | 60             |\n",
            "|    policy_gradient_loss | -0.000737      |\n",
            "|    reward               | -3.876823      |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 988            |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 751           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5273632e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.000183     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.44e+04      |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000155     |\n",
            "|    reward               | 0.5105502     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.44e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 752           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015576978 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.00119       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.5e+03       |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000812     |\n",
            "|    reward               | 15.394338     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.3e+03       |\n",
            "-------------------------------------------\n",
            "day: 2435, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2538379.07\n",
            "total_reward: 1538379.07\n",
            "total_cost: 786257.20\n",
            "total_trades: 20815\n",
            "Sharpe: 0.536\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 753         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 27          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000659758 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00773    |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 48.1        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00157    |\n",
            "|    reward               | 0.1917422   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 72.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 754          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010945823 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0228      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 19.6         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    reward               | -0.7325686   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 39.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 755          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.452889e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0062      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 197          |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.000413    |\n",
            "|    reward               | -0.40537193  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 500          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 756           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012965026 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | 0.000644      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 535           |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000677     |\n",
            "|    reward               | -3.1870377    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 769           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 756          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.484824e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00533     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 431          |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000459    |\n",
            "|    reward               | 1.7132657    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 932          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 757           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034510053 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -12.8         |\n",
            "|    explained_variance   | -0.00235      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 169           |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00136      |\n",
            "|    reward               | 2.7049959     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 241           |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.31353548336107395\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  69.69593753019969  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2015-2025_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 259\n",
            "[INFO] Moved trained_models to 2015-2025_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_crypto/\n",
            "[INFO] Moved results to 2015-2025_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "\n",
        "ensemble_agent_1 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_1,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_1,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_1,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 824         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.72       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -6.24       |\n",
            "|    reward             | -0.32220104 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.322       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 4.69      |\n",
            "|    reward             | -0.004026 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.262     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 828        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 7.15e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 45.3       |\n",
            "|    reward             | -1.3419733 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 17.1       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 826         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -5.55       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -17.3       |\n",
            "|    reward             | -0.08900253 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 3.74        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 828      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | -0.0909  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 13.7     |\n",
            "|    reward             | 0.164664 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 829       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -3.37     |\n",
            "|    reward             | 1.2447677 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.28      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 829       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -6.52     |\n",
            "|    reward             | -0.601641 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 2.09      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 828       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -1.84     |\n",
            "|    reward             | 0.0385985 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 0.0288    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 829      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -3.81    |\n",
            "|    reward             | 0.727195 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.462    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 830       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 15.9      |\n",
            "|    reward             | -0.183286 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 2.55      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 831       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 3.8       |\n",
            "|    reward             | 3.111469  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1         |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 831         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 3.52        |\n",
            "|    reward             | 0.020140724 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.169       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 831      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 5.41     |\n",
            "|    reward             | 0.222496 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 831      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 3.6      |\n",
            "|    reward             | 0.46099  |\n",
            "|    std                | 0.99     |\n",
            "|    value_loss         | 0.605    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 832      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -3.21    |\n",
            "|    reward             | 2.362255 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 5.19     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 832         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 2.16        |\n",
            "|    reward             | -0.38033852 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.0378      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 832      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 5.61     |\n",
            "|    reward             | 0.07673  |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.591    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 833       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 19.8      |\n",
            "|    reward             | -0.464641 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 2.9       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 833       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 52.1      |\n",
            "|    reward             | -0.696955 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 31.6      |\n",
            "-------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1624724.41\n",
            "total_reward: 624724.41\n",
            "total_cost: 2045.96\n",
            "total_trades: 9009\n",
            "Sharpe: 0.408\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 833        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -4.93      |\n",
            "|    reward             | -1.0237727 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.147      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 833       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 6.01      |\n",
            "|    reward             | -0.252246 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.551     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 833      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | -1.43    |\n",
            "|    reward             | 0.03123  |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.209    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 834      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -6.59    |\n",
            "|    reward             | 0.284263 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.441    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 833         |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0.647       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | -2.97       |\n",
            "|    reward             | -0.15577604 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.256       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 834       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -4.68     |\n",
            "|    reward             | 0.192468  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.323     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | -5.86     |\n",
            "|    reward             | -0.98003  |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.389     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -19.5     |\n",
            "|    reward             | -1.954192 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 3.37      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 786        |\n",
            "|    iterations         | 2800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 14000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -0.488     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2799       |\n",
            "|    policy_loss        | 6.02       |\n",
            "|    reward             | -0.2634779 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.373      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 788       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -2.21     |\n",
            "|    reward             | -0.509884 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.321     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 789       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -0.0821   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -22.1     |\n",
            "|    reward             | -0.588781 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 3.13      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 787      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -29.1    |\n",
            "|    reward             | 2.817685 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 16.9     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 788         |\n",
            "|    iterations         | 3200        |\n",
            "|    time_elapsed       | 20          |\n",
            "|    total_timesteps    | 16000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.1       |\n",
            "|    explained_variance | -0.347      |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3199        |\n",
            "|    policy_loss        | 6.16        |\n",
            "|    reward             | -0.17400472 |\n",
            "|    std                | 1.12        |\n",
            "|    value_loss         | 0.709       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 789      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 12.6     |\n",
            "|    reward             | 0.123197 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 791       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | -2.25     |\n",
            "|    reward             | -0.329741 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 0.311     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 792      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | -0.0016  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 9.22     |\n",
            "|    reward             | 0.12603  |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 2.75     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 793         |\n",
            "|    iterations         | 3600        |\n",
            "|    time_elapsed       | 22          |\n",
            "|    total_timesteps    | 18000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.3       |\n",
            "|    explained_variance | -0.059      |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3599        |\n",
            "|    policy_loss        | -5.58       |\n",
            "|    reward             | -0.14229222 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 0.275       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 794      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 4.41     |\n",
            "|    reward             | 0.136794 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 0.289    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 795       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 3.99      |\n",
            "|    reward             | -1.001094 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 0.218     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 796      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 3.49     |\n",
            "|    reward             | -0.68205 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 0.141    |\n",
            "------------------------------------\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1317571.66\n",
            "total_reward: 317571.66\n",
            "total_cost: 2505.05\n",
            "total_trades: 6675\n",
            "Sharpe: 0.402\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 797      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 4.13     |\n",
            "|    reward             | 0.458469 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 0.198    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 797         |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | 0.949       |\n",
            "|    reward             | 0.003744842 |\n",
            "|    std                | 1.16        |\n",
            "|    value_loss         | 0.0411      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 798       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | -10.8     |\n",
            "|    reward             | 0.165108  |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.814     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -11.1     |\n",
            "|    reward             | -1.718916 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 0.818     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 800       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -0.984    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 0.98      |\n",
            "|    reward             | -0.664584 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.147     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 0.55      |\n",
            "|    reward             | -0.200376 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.0306    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 2.21      |\n",
            "|    reward             | -0.867779 |\n",
            "|    std                | 1.27      |\n",
            "|    value_loss         | 0.0356    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -30.2     |\n",
            "|    reward             | -0.256036 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 5.91      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -6.01     |\n",
            "|    reward             | -0.429292 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 0.257     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 3.89     |\n",
            "|    reward             | 0.096373 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 0.225    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 804      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | -0.00014 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -1.04    |\n",
            "|    reward             | 0.038514 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 0.0728   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 6.3      |\n",
            "|    reward             | 0.125353 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 805       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -7.42     |\n",
            "|    reward             | -0.001656 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 0.341     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 806       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 1.93      |\n",
            "|    reward             | -0.089559 |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 0.0348    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -9.73    |\n",
            "|    reward             | 0.702826 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 0.674    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 807       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 2.3       |\n",
            "|    reward             | 1.926468  |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 0.117     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 807      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -11.3    |\n",
            "|    reward             | 0.196195 |\n",
            "|    std                | 1.23     |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 808       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -8.73     |\n",
            "|    reward             | -0.43053  |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 0.727     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 808      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 12       |\n",
            "|    reward             | 0.651932 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 0.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 809      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 0.233628 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1281986.04\n",
            "total_reward: 281986.04\n",
            "total_cost: 3657.78\n",
            "total_trades: 8265\n",
            "Sharpe: 0.325\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 809      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 5.66     |\n",
            "|    reward             | 0.05963  |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 0.251    |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.15200920796311673\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1429615.16\n",
            "total_reward: 429615.16\n",
            "total_cost: 999.00\n",
            "total_trades: 11964\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 242       |\n",
            "|    time_elapsed    | 32        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.6      |\n",
            "|    critic_loss     | 117       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.375037 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 238       |\n",
            "|    time_elapsed    | 66        |\n",
            "|    total_timesteps | 15960     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 0.108     |\n",
            "|    critic_loss     | 1.7       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 15859     |\n",
            "|    reward          | -0.375037 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1429615.16\n",
            "total_reward: 429615.16\n",
            "total_cost: 999.00\n",
            "total_trades: 11964\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 236       |\n",
            "|    time_elapsed    | 101       |\n",
            "|    total_timesteps | 23940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.319    |\n",
            "|    critic_loss     | 2.04      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 23839     |\n",
            "|    reward          | -0.375037 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1429615.16\n",
            "total_reward: 429615.16\n",
            "total_cost: 999.00\n",
            "total_trades: 11964\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.0504582612405649\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1395069.63\n",
            "total_reward: 395069.63\n",
            "total_cost: 1457.70\n",
            "total_trades: 8012\n",
            "Sharpe: 0.385\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 253      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 39.5     |\n",
            "|    critic_loss     | 3.55e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | -0.14363 |\n",
            "---------------------------------\n",
            "day: 1994, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1395069.63\n",
            "total_reward: 395069.63\n",
            "total_cost: 1457.70\n",
            "total_trades: 8012\n",
            "Sharpe: 0.385\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 253      |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 15960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 132      |\n",
            "|    critic_loss     | 4.7e+03  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15859    |\n",
            "|    reward          | -0.14363 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 254      |\n",
            "|    time_elapsed    | 94       |\n",
            "|    total_timesteps | 23940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 63.3     |\n",
            "|    critic_loss     | 6.4      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23839    |\n",
            "|    reward          | -0.14363 |\n",
            "---------------------------------\n",
            "day: 1994, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1395069.63\n",
            "total_reward: 395069.63\n",
            "total_cost: 1457.70\n",
            "total_trades: 8012\n",
            "Sharpe: 0.385\n",
            "=================================\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.14998001383090714\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1380606.57\n",
            "total_reward: 380606.57\n",
            "total_cost: 998.97\n",
            "total_trades: 11964\n",
            "Sharpe: 0.397\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 194       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 492       |\n",
            "|    critic_loss     | 1.17e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.092984 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1380606.57\n",
            "total_reward: 380606.57\n",
            "total_cost: 998.97\n",
            "total_trades: 11964\n",
            "Sharpe: 0.397\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 190       |\n",
            "|    time_elapsed    | 83        |\n",
            "|    total_timesteps | 15960     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 682       |\n",
            "|    critic_loss     | 3.17e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 15859     |\n",
            "|    reward          | -0.092984 |\n",
            "----------------------------------\n",
            "day: 1994, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1380606.57\n",
            "total_reward: 380606.57\n",
            "total_cost: 998.97\n",
            "total_trades: 11964\n",
            "Sharpe: 0.397\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 190       |\n",
            "|    time_elapsed    | 125       |\n",
            "|    total_timesteps | 23940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 815       |\n",
            "|    critic_loss     | 131       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 23839     |\n",
            "|    reward          | -0.092984 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.15039964752105664\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1273504.45\n",
            "total_reward: 273504.45\n",
            "total_cost: 124750.85\n",
            "total_trades: 15330\n",
            "Sharpe: 0.292\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 1015       |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.10225992 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 961           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030298906 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00677      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.25          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000747     |\n",
            "|    reward               | -0.094306126  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.63          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 945           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00052443775 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00131      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.43          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00119      |\n",
            "|    reward               | -0.87102276   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.12          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 935          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001018343 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0141       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 6.49         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.000456    |\n",
            "|    reward               | -0.83296835  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.77         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 936           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035193976 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0139        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.01          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00119      |\n",
            "|    reward               | 0.31654522    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.18          |\n",
            "-------------------------------------------\n",
            "day: 1994, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1167356.64\n",
            "total_reward: 167356.64\n",
            "total_cost: 125701.40\n",
            "total_trades: 15472\n",
            "Sharpe: 0.228\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 935           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031425466 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00148      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.33          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000957     |\n",
            "|    reward               | 0.40207627    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.11          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 933           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048702545 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0144       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.51          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00128      |\n",
            "|    reward               | 0.22582833    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.18          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 938          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001921253 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00177     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.05         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.000832    |\n",
            "|    reward               | 0.010041144  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 10.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 940          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004949096 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00241      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.28         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    reward               | -0.30379215  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.87         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 938           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029674175 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0457       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.63          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000773     |\n",
            "|    reward               | 0.34800732    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.85          |\n",
            "-------------------------------------------\n",
            "day: 1994, episode: 75\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1359149.63\n",
            "total_reward: 359149.63\n",
            "total_cost: 124156.37\n",
            "total_trades: 15433\n",
            "Sharpe: 0.370\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 936          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005572785 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0126      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.99         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00106     |\n",
            "|    reward               | 0.40182477   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.54         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 935          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002846795 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00551     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.49         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.000765    |\n",
            "|    reward               | -0.8799246   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.34         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 934           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011194806 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0469       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.43          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000623     |\n",
            "|    reward               | -0.42457432   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11            |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 933           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013554085 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.024        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.91          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000577     |\n",
            "|    reward               | 0.54223627    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11.5          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 934          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.460524e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0068       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.29         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.000571    |\n",
            "|    reward               | 1.0484699    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.25         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.05006955412670363\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 787         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | -12.5       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 0.55        |\n",
            "|    reward             | -0.28952065 |\n",
            "|    std                | 0.985       |\n",
            "|    value_loss         | 0.103       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 798      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 0.672    |\n",
            "|    reward             | 0.077086 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 0.0681   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 806       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -40.7     |\n",
            "|    reward             | -1.741529 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 16        |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 808      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 32.8     |\n",
            "|    reward             | 0.021329 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 8.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 813      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 4.71     |\n",
            "|    reward             | 0.06074  |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.129    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 812         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -4.1        |\n",
            "|    reward             | -0.44604775 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.396       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 810       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -6.12     |\n",
            "|    reward             | -0.616592 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.05      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 816      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -38.9    |\n",
            "|    reward             | 0.209261 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 16.9     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 819       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 4.49      |\n",
            "|    reward             | 0.39296   |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.551     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 817       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -5.29     |\n",
            "|    reward             | -0.422406 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.233     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 819      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | -0.008   |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 11.7     |\n",
            "|    reward             | 0.41871  |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 819      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 14.8     |\n",
            "|    reward             | 2.562302 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 819       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 0.528     |\n",
            "|    reward             | -0.664214 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.0821    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 818       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 4.06      |\n",
            "|    reward             | -0.090507 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.293     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -26.6    |\n",
            "|    reward             | 2.267919 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 809      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -85.1    |\n",
            "|    reward             | 0.867038 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 53.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 808      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 23.1     |\n",
            "|    reward             | 0.760977 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 808       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -1.77     |\n",
            "|    reward             | -0.933935 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.22      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 810      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 1.94     |\n",
            "|    reward             | 0.201405 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 1.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 812      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 48.7     |\n",
            "|    reward             | 1.626085 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 21.4     |\n",
            "------------------------------------\n",
            "day: 2057, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1553064.93\n",
            "total_reward: 553064.93\n",
            "total_cost: 2335.49\n",
            "total_trades: 13818\n",
            "Sharpe: 0.428\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 813      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 3.52     |\n",
            "|    reward             | 0.678118 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.412    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 814       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 2.92      |\n",
            "|    reward             | -0.012156 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.204     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 815       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -4.76     |\n",
            "|    reward             | -0.225773 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.258     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 816      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 20       |\n",
            "|    reward             | -0.03679 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 816       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -0.00348  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -15.3     |\n",
            "|    reward             | -1.045536 |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 4.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 816      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 9.72     |\n",
            "|    reward             | 0.481949 |\n",
            "|    std                | 0.987    |\n",
            "|    value_loss         | 0.724    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 817       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 5.1       |\n",
            "|    reward             | 1.184264  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.77      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 818      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -19.9    |\n",
            "|    reward             | 1.534417 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 4.12     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 818       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.000834 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | 3.62      |\n",
            "|    reward             | 0.31184   |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.545     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 819      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 6.66     |\n",
            "|    reward             | -0.12399 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.379    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 820      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -1.24    |\n",
            "|    reward             | 0.712766 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.0927   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 821      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 14.9     |\n",
            "|    reward             | 0.22727  |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 821       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 18.8      |\n",
            "|    reward             | -1.388982 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 3.15      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 822        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | 0.825      |\n",
            "|    reward             | 0.03107914 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.33       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 822      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | -17.6    |\n",
            "|    reward             | 0.716609 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 823       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -24.9     |\n",
            "|    reward             | 0.819889  |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 9.48      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 824       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | -16.8     |\n",
            "|    reward             | -0.672981 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 3.67      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 823       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -19.8     |\n",
            "|    reward             | -0.681276 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 3.2       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 824        |\n",
            "|    iterations         | 3900       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 19500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3899       |\n",
            "|    policy_loss        | 1.53       |\n",
            "|    reward             | -1.1051799 |\n",
            "|    std                | 1.18       |\n",
            "|    value_loss         | 0.318      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 36.1      |\n",
            "|    reward             | -0.748274 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 12.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 825      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 48.3     |\n",
            "|    reward             | 0.510018 |\n",
            "|    std                | 1.2      |\n",
            "|    value_loss         | 15.3     |\n",
            "------------------------------------\n",
            "day: 2057, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1538778.69\n",
            "total_reward: 538778.69\n",
            "total_cost: 3178.08\n",
            "total_trades: 8012\n",
            "Sharpe: 0.509\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 825      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 8.71     |\n",
            "|    reward             | 0.326519 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | 7.39      |\n",
            "|    reward             | -0.723686 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.398     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 23.6      |\n",
            "|    reward             | -0.893121 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 3.21      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | 11.2      |\n",
            "|    reward             | -0.356587 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.862     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 30.7      |\n",
            "|    reward             | -0.420682 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 6.98      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 827      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -27.9    |\n",
            "|    reward             | 0.188884 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 827      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | 32.1     |\n",
            "|    reward             | 0.228614 |\n",
            "|    std                | 1.36     |\n",
            "|    value_loss         | 5.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 826      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -5.05    |\n",
            "|    reward             | 0.747255 |\n",
            "|    std                | 1.34     |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 827      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 21.5     |\n",
            "|    reward             | 0.04125  |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 827       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 0.318     |\n",
            "|    reward             | -0.767827 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 0.143     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -28       |\n",
            "|    reward             | -0.205516 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 5.93      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | 17.2      |\n",
            "|    reward             | -0.804703 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 2.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -23.4     |\n",
            "|    reward             | -0.084298 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 2.48      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -2.16     |\n",
            "|    reward             | -0.048244 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 0.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 20.3      |\n",
            "|    reward             | -0.304908 |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 3.26      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 12.2      |\n",
            "|    reward             | -0.712655 |\n",
            "|    std                | 1.31      |\n",
            "|    value_loss         | 0.833     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 826      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 1.84     |\n",
            "|    reward             | -0.67895 |\n",
            "|    std                | 1.31     |\n",
            "|    value_loss         | 0.0847   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 825      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -5.19    |\n",
            "|    reward             | -0.24941 |\n",
            "|    std                | 1.33     |\n",
            "|    value_loss         | 0.345    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 2.78      |\n",
            "|    reward             | -0.034231 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.0769    |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.16943776494649904\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 244       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -56.8     |\n",
            "|    critic_loss     | 6.94      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -0.896023 |\n",
            "----------------------------------\n",
            "day: 2057, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1452868.08\n",
            "total_reward: 452868.08\n",
            "total_cost: 998.98\n",
            "total_trades: 10285\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 220       |\n",
            "|    time_elapsed    | 74        |\n",
            "|    total_timesteps | 16464     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -40.2     |\n",
            "|    critic_loss     | 2.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16363     |\n",
            "|    reward          | -0.896023 |\n",
            "----------------------------------\n",
            "day: 2057, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1452868.08\n",
            "total_reward: 452868.08\n",
            "total_cost: 998.98\n",
            "total_trades: 10285\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 228       |\n",
            "|    time_elapsed    | 107       |\n",
            "|    total_timesteps | 24696     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -28       |\n",
            "|    critic_loss     | 1.84      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 24595     |\n",
            "|    reward          | -0.896023 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.20514641903763844\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 246      |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 116      |\n",
            "|    critic_loss     | 42.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.401996 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1537056.41\n",
            "total_reward: 537056.41\n",
            "total_cost: 998.98\n",
            "total_trades: 6171\n",
            "Sharpe: 0.471\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 247      |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 16464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 49       |\n",
            "|    critic_loss     | 1.09e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16363    |\n",
            "|    reward          | 0.401996 |\n",
            "---------------------------------\n",
            "day: 2057, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1537056.41\n",
            "total_reward: 537056.41\n",
            "total_cost: 998.98\n",
            "total_trades: 6171\n",
            "Sharpe: 0.471\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 247      |\n",
            "|    time_elapsed    | 99       |\n",
            "|    total_timesteps | 24696    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52       |\n",
            "|    critic_loss     | 5.07     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 24595    |\n",
            "|    reward          | 0.401996 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.0026120440081914384\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 726       |\n",
            "|    critic_loss     | 625       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -0.290555 |\n",
            "----------------------------------\n",
            "day: 2057, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1512169.48\n",
            "total_reward: 512169.48\n",
            "total_cost: 998.98\n",
            "total_trades: 10285\n",
            "Sharpe: 0.497\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 83        |\n",
            "|    total_timesteps | 16464     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.12e+03  |\n",
            "|    critic_loss     | 1.07e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16363     |\n",
            "|    reward          | -0.290555 |\n",
            "----------------------------------\n",
            "day: 2057, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1512169.48\n",
            "total_reward: 512169.48\n",
            "total_cost: 998.98\n",
            "total_trades: 10285\n",
            "Sharpe: 0.497\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 125       |\n",
            "|    total_timesteps | 24696     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.46e+03  |\n",
            "|    critic_loss     | 124       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 24595     |\n",
            "|    reward          | -0.290555 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.1581897158524395\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 1010       |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.1226537 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 983           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00094289135 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0207       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.9           |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00195      |\n",
            "|    reward               | 0.06338753    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.35          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 975          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005563906 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0134       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    reward               | -0.45985755  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 963          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005285592 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00762     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.7          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00135     |\n",
            "|    reward               | 0.7951824    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.21         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 947          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008271992 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0057       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00143     |\n",
            "|    reward               | -0.08381558  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.44         |\n",
            "------------------------------------------\n",
            "day: 2057, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1068059.10\n",
            "total_reward: 68059.10\n",
            "total_cost: 131320.68\n",
            "total_trades: 15948\n",
            "Sharpe: 0.128\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 951           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00045213287 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00136      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.13          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00103      |\n",
            "|    reward               | 0.15974614    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.99          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 870           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025480057 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00358       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.98          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000783     |\n",
            "|    reward               | -1.8319741    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.06          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 878           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00028926216 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00382      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.62          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000839     |\n",
            "|    reward               | 0.6489948     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.55          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 887          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004946316 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0072      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.97         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00152     |\n",
            "|    reward               | 0.5943653    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.3          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 890          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.982796e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00933     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.68         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000588    |\n",
            "|    reward               | -2.183343    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "day: 2057, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1334924.30\n",
            "total_reward: 334924.30\n",
            "total_cost: 127059.37\n",
            "total_trades: 15865\n",
            "Sharpe: 0.312\n",
            "=================================\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 894            |\n",
            "|    iterations           | 11             |\n",
            "|    time_elapsed         | 25             |\n",
            "|    total_timesteps      | 22528          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000108616136 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -11.4          |\n",
            "|    explained_variance   | 0.00895        |\n",
            "|    learning_rate        | 1e-05          |\n",
            "|    loss                 | 4.99           |\n",
            "|    n_updates            | 100            |\n",
            "|    policy_gradient_loss | -0.000672      |\n",
            "|    reward               | 0.12685989     |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 10.1           |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 900          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007025058 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0147      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.86         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    reward               | -0.6383957   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.53         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 900           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 29            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2797724e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0191       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.07          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.000544     |\n",
            "|    reward               | 2.7742233     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11.5          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 902           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 31            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022950329 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0016        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.92          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000866     |\n",
            "|    reward               | -0.016014958  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 13.9          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 904         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000261162 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0202     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 3.44        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.000781   |\n",
            "|    reward               | -1.039215   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.68        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.11546242465655607\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 792       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 1.37      |\n",
            "|    reward             | -0.924276 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 0.545     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 817       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 0.132     |\n",
            "|    reward             | -0.023067 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 0.0549    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 826       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -52.5     |\n",
            "|    reward             | -1.767895 |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 27.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 823       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 30        |\n",
            "|    reward             | -0.029179 |\n",
            "|    std                | 0.973     |\n",
            "|    value_loss         | 8.86      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 822        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 6.4        |\n",
            "|    reward             | 0.25810274 |\n",
            "|    std                | 0.977      |\n",
            "|    value_loss         | 0.442      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 825       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -2.9      |\n",
            "|    reward             | -0.157722 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 0.205     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 828      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -2.02    |\n",
            "|    reward             | 0.591443 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.421    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 830       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -11.5     |\n",
            "|    reward             | -0.918119 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.02      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 830       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 3.98      |\n",
            "|    reward             | 0.2108475 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 1.12      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 831       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -6.53     |\n",
            "|    reward             | -3.039282 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.48      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 831       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -14.5     |\n",
            "|    reward             | -1.495858 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 2.11      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 832      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -29.2    |\n",
            "|    reward             | -1.12942 |\n",
            "|    std                | 0.976    |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 832       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 1.18      |\n",
            "|    reward             | -0.284202 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 0.162     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 833      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -3.67    |\n",
            "|    reward             | 0.296304 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.142    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 833      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -24.4    |\n",
            "|    reward             | 2.390799 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 6.65     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 834       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -2.99     |\n",
            "|    reward             | 1.04629   |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.791     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 834       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -1.7      |\n",
            "|    reward             | 0.0973466 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.0418    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 835      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 7.58     |\n",
            "|    reward             | 0.052072 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.607    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 835      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -3.33    |\n",
            "|    reward             | 0.060459 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.215    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 836      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 19.2     |\n",
            "|    reward             | 0.522127 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 5.33     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 836       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 15.8      |\n",
            "|    reward             | -2.785639 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.47      |\n",
            "-------------------------------------\n",
            "day: 2120, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1631741.98\n",
            "total_reward: 631741.98\n",
            "total_cost: 2001.24\n",
            "total_trades: 9019\n",
            "Sharpe: 0.460\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 836         |\n",
            "|    iterations         | 2200        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 11000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 2199        |\n",
            "|    policy_loss        | 2.65        |\n",
            "|    reward             | 0.003399444 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.0917      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 836      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -2.85    |\n",
            "|    reward             | 0.514351 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.265    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 836       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 3.23      |\n",
            "|    reward             | 1.2231653 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.812     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 836       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 9.37      |\n",
            "|    reward             | 0.045573  |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.812     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 836      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 7.75     |\n",
            "|    reward             | 0.12872  |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 0.644    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 836       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -5.87     |\n",
            "|    reward             | -0.651744 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.911     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 837       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 8.38      |\n",
            "|    reward             | -2.988763 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 0.687     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 837      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 37.7     |\n",
            "|    reward             | 0.554728 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 9.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 837      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -2.87    |\n",
            "|    reward             | 0.172934 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.556    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 837       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 8.98      |\n",
            "|    reward             | -0.138728 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 0.442     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 837       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | -13.8     |\n",
            "|    reward             | -0.083904 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 1.11      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 837      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -28      |\n",
            "|    reward             | 0.318681 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 837         |\n",
            "|    iterations         | 3400        |\n",
            "|    time_elapsed       | 20          |\n",
            "|    total_timesteps    | 17000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3399        |\n",
            "|    policy_loss        | -0.439      |\n",
            "|    reward             | -0.20714638 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 0.632       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | -18.7     |\n",
            "|    reward             | -0.516417 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 2.4       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -11      |\n",
            "|    reward             | 1.308562 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 0.969    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | -46.9     |\n",
            "|    reward             | -3.341384 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 16.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -58.2     |\n",
            "|    reward             | -0.126616 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 22.8      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | -17      |\n",
            "|    reward             | 0.385764 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -12.5    |\n",
            "|    reward             | 0.227423 |\n",
            "|    std                | 1.27     |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 19.4     |\n",
            "|    reward             | 1.940495 |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | -43.8     |\n",
            "|    reward             | -0.566779 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 12.8      |\n",
            "-------------------------------------\n",
            "day: 2120, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1640553.22\n",
            "total_reward: 640553.22\n",
            "total_cost: 2284.24\n",
            "total_trades: 9806\n",
            "Sharpe: 0.438\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 5.92     |\n",
            "|    reward             | 1.244919 |\n",
            "|    std                | 1.3      |\n",
            "|    value_loss         | 0.622    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -12.2     |\n",
            "|    reward             | -2.074826 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 0.924     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -142      |\n",
            "|    reward             | -6.396314 |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 88.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | -16.7    |\n",
            "|    reward             | -1.87012 |\n",
            "|    std                | 1.27     |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -22       |\n",
            "|    reward             | 1.372493  |\n",
            "|    std                | 1.32      |\n",
            "|    value_loss         | 2.95      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.1    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | -4.52    |\n",
            "|    reward             | 0.074868 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 0.145    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 4.22      |\n",
            "|    reward             | -0.840761 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.228     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -27.2     |\n",
            "|    reward             | -0.798982 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 5.2       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 838        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 13.4       |\n",
            "|    reward             | -0.3399302 |\n",
            "|    std                | 1.26       |\n",
            "|    value_loss         | 1.03       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 5.08      |\n",
            "|    reward             | -0.315175 |\n",
            "|    std                | 1.24      |\n",
            "|    value_loss         | 0.194     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -1.25    |\n",
            "|    reward             | 0.049934 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.473    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -13.9    |\n",
            "|    reward             | 1.910653 |\n",
            "|    std                | 1.25     |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -21.5     |\n",
            "|    reward             | -1.293943 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 5.3       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -20.6     |\n",
            "|    reward             | -1.148503 |\n",
            "|    std                | 1.23      |\n",
            "|    value_loss         | 2.76      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -8.07     |\n",
            "|    reward             | -0.450397 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 0.713     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 838      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -37      |\n",
            "|    reward             | 1.201394 |\n",
            "|    std                | 1.31     |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -39.1     |\n",
            "|    reward             | -0.239357 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 9.93      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 838       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 19        |\n",
            "|    reward             | -0.316832 |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 2.13      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.4839701112399273\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 249       |\n",
            "|    time_elapsed    | 34        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -35.5     |\n",
            "|    critic_loss     | 8.8e+03   |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.413903 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1587462.54\n",
            "total_reward: 587462.54\n",
            "total_cost: 998.99\n",
            "total_trades: 10600\n",
            "Sharpe: 0.416\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 244       |\n",
            "|    time_elapsed    | 69        |\n",
            "|    total_timesteps | 16968     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 10.3      |\n",
            "|    critic_loss     | 3.22      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16867     |\n",
            "|    reward          | -1.413903 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1587462.54\n",
            "total_reward: 587462.54\n",
            "total_cost: 998.99\n",
            "total_trades: 10600\n",
            "Sharpe: 0.416\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 244       |\n",
            "|    time_elapsed    | 104       |\n",
            "|    total_timesteps | 25452     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 6.67      |\n",
            "|    critic_loss     | 1.47      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 25351     |\n",
            "|    reward          | -1.413903 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.38613753980584575\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 104       |\n",
            "|    critic_loss     | 9.65      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.263593 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1583604.62\n",
            "total_reward: 583604.62\n",
            "total_cost: 998.99\n",
            "total_trades: 8480\n",
            "Sharpe: 0.429\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 228       |\n",
            "|    time_elapsed    | 74        |\n",
            "|    total_timesteps | 16968     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 60.3      |\n",
            "|    critic_loss     | 34.7      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 16867     |\n",
            "|    reward          | -1.263593 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1583604.62\n",
            "total_reward: 583604.62\n",
            "total_cost: 998.99\n",
            "total_trades: 8480\n",
            "Sharpe: 0.429\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 230       |\n",
            "|    time_elapsed    | 110       |\n",
            "|    total_timesteps | 25452     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 64.7      |\n",
            "|    critic_loss     | 79.9      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 25351     |\n",
            "|    reward          | -1.263593 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.38745640792690955\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 183       |\n",
            "|    time_elapsed    | 46        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 507       |\n",
            "|    critic_loss     | 59.5      |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.271124 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1518145.60\n",
            "total_reward: 518145.60\n",
            "total_cost: 998.99\n",
            "total_trades: 12720\n",
            "Sharpe: 0.435\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 182       |\n",
            "|    time_elapsed    | 92        |\n",
            "|    total_timesteps | 16968     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 777       |\n",
            "|    critic_loss     | 36        |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 16867     |\n",
            "|    reward          | -1.271124 |\n",
            "----------------------------------\n",
            "day: 2120, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1518145.60\n",
            "total_reward: 518145.60\n",
            "total_cost: 998.99\n",
            "total_trades: 12720\n",
            "Sharpe: 0.435\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 182       |\n",
            "|    time_elapsed    | 139       |\n",
            "|    total_timesteps | 25452     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.09e+03  |\n",
            "|    critic_loss     | 2.22e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 25351     |\n",
            "|    reward          | -1.271124 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.4002993988978573\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 982        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.6238596 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 935           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00085211446 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0225        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.82          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00187      |\n",
            "|    reward               | -2.6202826    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.23          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 923           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016778763 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00665      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.47          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000793     |\n",
            "|    reward               | -2.4565716    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.03          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 924           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 8             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035797898 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0187        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.83          |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    reward               | -0.893363     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.92          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007850458 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0272      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.78         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    reward               | -1.3540062   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.86         |\n",
            "------------------------------------------\n",
            "day: 2120, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1132528.01\n",
            "total_reward: 132528.01\n",
            "total_cost: 137165.14\n",
            "total_trades: 16384\n",
            "Sharpe: 0.182\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006846131 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00805      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.71         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00164     |\n",
            "|    reward               | 0.75122267   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.2          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 923           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018908145 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00769      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.51          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000681     |\n",
            "|    reward               | 0.7619669     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.75          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010764515 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00142     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.45         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00171     |\n",
            "|    reward               | 1.5414311    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.65         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 927          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.893217e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00095     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.89         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.000528    |\n",
            "|    reward               | 0.72912234   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 10.3         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 928           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00017524586 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00518       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.42          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000745     |\n",
            "|    reward               | -0.980083     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.76          |\n",
            "-------------------------------------------\n",
            "day: 2120, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1551537.78\n",
            "total_reward: 551537.78\n",
            "total_cost: 130418.46\n",
            "total_trades: 16219\n",
            "Sharpe: 0.407\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 927           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011200315 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00203       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.52          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000635     |\n",
            "|    reward               | 0.4213736     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11            |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 927           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 26            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015948236 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0123       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.86          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000619     |\n",
            "|    reward               | 0.9583959     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.5          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 924           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033108165 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0261       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.86          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00115      |\n",
            "|    reward               | -0.7744461    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.16          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 926          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001652913 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00759     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.79         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000959    |\n",
            "|    reward               | -0.7084615   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 12.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 926          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003147592 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.028       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.96         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00101     |\n",
            "|    reward               | 0.5236502    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.55         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.42211415245362527\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 787       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -6.23     |\n",
            "|    reward             | -0.259678 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.298     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 807       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 6.02      |\n",
            "|    reward             | 0.5499505 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.568     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 785        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 40.9       |\n",
            "|    reward             | -1.4597538 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 13.4       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 774      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 25.5     |\n",
            "|    reward             | 1.04991  |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 4.52     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 781       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 10.6      |\n",
            "|    reward             | -0.530216 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 0.807     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 787       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -11.2     |\n",
            "|    reward             | -1.054028 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.25      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 786      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 15.3     |\n",
            "|    reward             | 1.462055 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 4.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 791      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -32.3    |\n",
            "|    reward             | 3.278611 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 14.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 793      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -4.18    |\n",
            "|    reward             | -0.32016 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.144    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 797      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -1.31    |\n",
            "|    reward             | 0.039302 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 0.0253   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 797       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.398    |\n",
            "|    reward             | -0.711812 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 0.378     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -6.32     |\n",
            "|    reward             | -0.853205 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 4.56      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 797      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 14.8     |\n",
            "|    reward             | 0.070523 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 799        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -7.14      |\n",
            "|    reward             | 0.49711388 |\n",
            "|    std                | 1.19       |\n",
            "|    value_loss         | 0.318      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 798      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -18.1    |\n",
            "|    reward             | 0.313117 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 800       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 0.955     |\n",
            "|    reward             | -0.772904 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 0.233     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 799      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 9.09     |\n",
            "|    reward             | -0.49862 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 800       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -3.28     |\n",
            "|    reward             | -0.065713 |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 0.436     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 801      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -30.8    |\n",
            "|    reward             | 0.275811 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 9.8      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -3.61     |\n",
            "|    reward             | 0.214808  |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 0.175     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -19.1    |\n",
            "|    reward             | 0.334558 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "day: 2183, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1387976.87\n",
            "total_reward: 387976.87\n",
            "total_cost: 3382.10\n",
            "total_trades: 10108\n",
            "Sharpe: 0.500\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 0.51      |\n",
            "|    reward             | -0.055234 |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 0.11      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 3.62      |\n",
            "|    reward             | 0.369413  |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.0898    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | -7.36     |\n",
            "|    reward             | -0.587384 |\n",
            "|    std                | 1.36      |\n",
            "|    value_loss         | 0.886     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -9.85    |\n",
            "|    reward             | -2.5071  |\n",
            "|    std                | 1.34     |\n",
            "|    value_loss         | 0.732    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 803        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | 7.19       |\n",
            "|    reward             | -0.0184293 |\n",
            "|    std                | 1.31       |\n",
            "|    value_loss         | 1.39       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -2.9      |\n",
            "|    reward             | -0.157309 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.0613    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -15.1    |\n",
            "|    reward             | 0.004636 |\n",
            "|    std                | 1.38     |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -29.6    |\n",
            "|    reward             | 0.441333 |\n",
            "|    std                | 1.33     |\n",
            "|    value_loss         | 4.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 799      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -1.26    |\n",
            "|    reward             | 2.129449 |\n",
            "|    std                | 1.37     |\n",
            "|    value_loss         | 0.428    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 1.53      |\n",
            "|    reward             | -0.681647 |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 0.0283    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 800      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 3.5      |\n",
            "|    reward             | 0.619446 |\n",
            "|    std                | 1.43     |\n",
            "|    value_loss         | 0.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 800      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 6.44     |\n",
            "|    reward             | 0.069259 |\n",
            "|    std                | 1.45     |\n",
            "|    value_loss         | 0.199    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 801         |\n",
            "|    iterations         | 3400        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 17000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3399        |\n",
            "|    policy_loss        | -5.1        |\n",
            "|    reward             | 0.011854763 |\n",
            "|    std                | 1.45        |\n",
            "|    value_loss         | 0.451       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 800         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | -21.1       |\n",
            "|    reward             | -0.30501068 |\n",
            "|    std                | 1.45        |\n",
            "|    value_loss         | 2.83        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 801      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 8.88     |\n",
            "|    reward             | -0.25492 |\n",
            "|    std                | 1.49     |\n",
            "|    value_loss         | 0.371    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 1.62      |\n",
            "|    reward             | -0.055633 |\n",
            "|    std                | 1.56      |\n",
            "|    value_loss         | 0.05      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | -14       |\n",
            "|    reward             | -0.794884 |\n",
            "|    std                | 1.61      |\n",
            "|    value_loss         | 1.05      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 2.42      |\n",
            "|    reward             | -0.137619 |\n",
            "|    std                | 1.58      |\n",
            "|    value_loss         | 1.01      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 801         |\n",
            "|    iterations         | 4000        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 20000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -14.7       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3999        |\n",
            "|    policy_loss        | 5.36        |\n",
            "|    reward             | -0.12888516 |\n",
            "|    std                | 1.59        |\n",
            "|    value_loss         | 0.198       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 6.28     |\n",
            "|    reward             | 0.249631 |\n",
            "|    std                | 1.61     |\n",
            "|    value_loss         | 0.438    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 21.4      |\n",
            "|    reward             | -0.321542 |\n",
            "|    std                | 1.65      |\n",
            "|    value_loss         | 2.92      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -44.8    |\n",
            "|    reward             | 3.287003 |\n",
            "|    std                | 1.64     |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "day: 2183, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1674338.17\n",
            "total_reward: 674338.17\n",
            "total_cost: 1995.86\n",
            "total_trades: 10396\n",
            "Sharpe: 0.499\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -0.73    |\n",
            "|    reward             | 0.278485 |\n",
            "|    std                | 1.64     |\n",
            "|    value_loss         | 0.227    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -2.55     |\n",
            "|    reward             | -0.019917 |\n",
            "|    std                | 1.62      |\n",
            "|    value_loss         | 0.0797    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 14.1     |\n",
            "|    reward             | 0.669184 |\n",
            "|    std                | 1.64     |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 13.2      |\n",
            "|    reward             | -0.068355 |\n",
            "|    std                | 1.67      |\n",
            "|    value_loss         | 0.744     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -12.9     |\n",
            "|    reward             | -0.177316 |\n",
            "|    std                | 1.7       |\n",
            "|    value_loss         | 1.11      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -2.35    |\n",
            "|    reward             | 0.128432 |\n",
            "|    std                | 1.69     |\n",
            "|    value_loss         | 0.184    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 5000      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | -37.1     |\n",
            "|    reward             | -1.552459 |\n",
            "|    std                | 1.75      |\n",
            "|    value_loss         | 8.66      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 800      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 14       |\n",
            "|    reward             | 0.318308 |\n",
            "|    std                | 1.74     |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -14.9     |\n",
            "|    reward             | -1.334236 |\n",
            "|    std                | 1.71      |\n",
            "|    value_loss         | 1.91      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 800      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -16.1    |\n",
            "|    reward             | 0.27586  |\n",
            "|    std                | 1.75     |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 800      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -16.3    |\n",
            "|    reward             | 1.753859 |\n",
            "|    std                | 1.74     |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -99.6     |\n",
            "|    reward             | -1.463337 |\n",
            "|    std                | 1.77      |\n",
            "|    value_loss         | 78.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 799      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -15.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -46.3    |\n",
            "|    reward             | 1.416098 |\n",
            "|    std                | 1.81     |\n",
            "|    value_loss         | 13       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 798       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -16.5     |\n",
            "|    reward             | -0.435707 |\n",
            "|    std                | 1.84      |\n",
            "|    value_loss         | 1.54      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 798       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -5.69     |\n",
            "|    reward             | -0.034798 |\n",
            "|    std                | 1.85      |\n",
            "|    value_loss         | 0.145     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 797        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -16        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | 18.6       |\n",
            "|    reward             | 0.35451776 |\n",
            "|    std                | 1.89       |\n",
            "|    value_loss         | 1.66       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 797       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -15.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | -16.1     |\n",
            "|    reward             | -0.566718 |\n",
            "|    std                | 1.84      |\n",
            "|    value_loss         | 1.09      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.3765728887696429\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 2183, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1213658.89\n",
            "total_reward: 213658.89\n",
            "total_cost: 6389.79\n",
            "total_trades: 7087\n",
            "Sharpe: 0.242\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 239      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24      |\n",
            "|    critic_loss     | 1.54     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.462813 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1144407.38\n",
            "total_reward: 144407.38\n",
            "total_cost: 999.00\n",
            "total_trades: 6549\n",
            "Sharpe: 0.212\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 240      |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 17472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.3    |\n",
            "|    critic_loss     | 1.78     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17371    |\n",
            "|    reward          | 0.462813 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1144407.38\n",
            "total_reward: 144407.38\n",
            "total_cost: 999.00\n",
            "total_trades: 6549\n",
            "Sharpe: 0.212\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 239      |\n",
            "|    time_elapsed    | 109      |\n",
            "|    total_timesteps | 26208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -2.14    |\n",
            "|    critic_loss     | 0.378    |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 26107    |\n",
            "|    reward          | 0.462813 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.387235316655183\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "day: 2183, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1512456.04\n",
            "total_reward: 512456.04\n",
            "total_cost: 1218.26\n",
            "total_trades: 13123\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 233      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 90.7     |\n",
            "|    critic_loss     | 2.48e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.792553 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1512456.04\n",
            "total_reward: 512456.04\n",
            "total_cost: 1218.26\n",
            "total_trades: 13123\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 231      |\n",
            "|    time_elapsed    | 75       |\n",
            "|    total_timesteps | 17472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 97.4     |\n",
            "|    critic_loss     | 1.09e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17371    |\n",
            "|    reward          | 0.792553 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1512456.04\n",
            "total_reward: 512456.04\n",
            "total_cost: 1218.26\n",
            "total_trades: 13123\n",
            "Sharpe: 0.404\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 231      |\n",
            "|    time_elapsed    | 113      |\n",
            "|    total_timesteps | 26208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 67.4     |\n",
            "|    critic_loss     | 6.17     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 26107    |\n",
            "|    reward          | 0.792553 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3839012207484067\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "day: 2183, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 895474.23\n",
            "total_reward: -104525.77\n",
            "total_cost: 998.98\n",
            "total_trades: 4366\n",
            "Sharpe: 0.027\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 196      |\n",
            "|    time_elapsed    | 44       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 515      |\n",
            "|    critic_loss     | 2.02e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.582115 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 895474.23\n",
            "total_reward: -104525.77\n",
            "total_cost: 998.98\n",
            "total_trades: 4366\n",
            "Sharpe: 0.027\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 195      |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 17472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 700      |\n",
            "|    critic_loss     | 1.65e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 17371    |\n",
            "|    reward          | 0.582115 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 194      |\n",
            "|    time_elapsed    | 134      |\n",
            "|    total_timesteps | 26208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 763      |\n",
            "|    critic_loss     | 109      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 26107    |\n",
            "|    reward          | 0.582115 |\n",
            "---------------------------------\n",
            "day: 2183, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 895474.23\n",
            "total_reward: -104525.77\n",
            "total_cost: 998.98\n",
            "total_trades: 4366\n",
            "Sharpe: 0.027\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.3183135455737436\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 918         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 2           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.87727976 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 909           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00068502466 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0188       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.81          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00158      |\n",
            "|    reward               | -0.18356247   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.81          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 911          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010862444 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00958     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.55         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00207     |\n",
            "|    reward               | -0.50755364  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.23         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 901          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009925914 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0463       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.28         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    reward               | -1.2594649   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.81         |\n",
            "------------------------------------------\n",
            "day: 2183, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1326374.50\n",
            "total_reward: 326374.50\n",
            "total_cost: 140567.74\n",
            "total_trades: 16971\n",
            "Sharpe: 0.352\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 891          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003817653 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0227       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.07         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00119     |\n",
            "|    reward               | -0.30941072  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.45         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 810           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020917927 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.3         |\n",
            "|    explained_variance   | 0.00675       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.4           |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00076      |\n",
            "|    reward               | -0.39295593   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.39          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 820         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001034895 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.039       |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 2.55        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00218    |\n",
            "|    reward               | -0.2770544  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.44        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 826         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000733766 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0291      |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.25        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0018     |\n",
            "|    reward               | 0.0973894   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.38        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 833           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023260241 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0076        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.51          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.00105      |\n",
            "|    reward               | 0.60580695    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.99          |\n",
            "-------------------------------------------\n",
            "day: 2183, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1212442.47\n",
            "total_reward: 212442.47\n",
            "total_cost: 133918.63\n",
            "total_trades: 16798\n",
            "Sharpe: 0.229\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 839          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001779265 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.000935    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.82         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000657    |\n",
            "|    reward               | 1.0719472    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 11.5         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 846           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 26            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020727428 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00733       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.2           |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000707     |\n",
            "|    reward               | -0.08154215   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.68          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 847           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 29            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.7769935e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00332      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.28          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000712     |\n",
            "|    reward               | 0.3099693     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.71          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 851          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.244786e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0103       |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.28         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000484    |\n",
            "|    reward               | -0.42661408  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.56         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 856           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00058829936 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00975       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.11          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.00157      |\n",
            "|    reward               | -0.1664971    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.3          |\n",
            "-------------------------------------------\n",
            "day: 2183, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1186590.57\n",
            "total_reward: 186590.57\n",
            "total_cost: 131761.09\n",
            "total_trades: 16764\n",
            "Sharpe: 0.212\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 856           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.7604305e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0179       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.24          |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000661     |\n",
            "|    reward               | 1.7988567     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.8          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.32849472524299034\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 765         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.04       |\n",
            "|    reward             | -0.41109475 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.161       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 771      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 10.3     |\n",
            "|    reward             | -0.11615 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 767      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 45.8     |\n",
            "|    reward             | -1.94742 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 28.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 774      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 22.3     |\n",
            "|    reward             | 2.14514  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 5.42      |\n",
            "|    reward             | 0.8207895 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.16      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 775       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 13.7      |\n",
            "|    reward             | -1.023443 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.55      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 776       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 24.1      |\n",
            "|    reward             | -1.044017 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 4.78      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 778       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -43.7     |\n",
            "|    reward             | -3.016031 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 15.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | -0.768     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -52.9      |\n",
            "|    reward             | 0.18318072 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 24.2       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 779      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0.000578 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -3.05    |\n",
            "|    reward             | 0.936015 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.287    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 779      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 0.745    |\n",
            "|    reward             | 0.983637 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.419    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 780      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 9.4      |\n",
            "|    reward             | -0.44642 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.931    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 781      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 50.9     |\n",
            "|    reward             | 2.54786  |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 26.8     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 782       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 0.797     |\n",
            "|    reward             | -1.123363 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.753     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 782      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -19.5    |\n",
            "|    reward             | 1.520956 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 783       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.000338 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -24.5     |\n",
            "|    reward             | 0.782812  |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.81      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -55       |\n",
            "|    reward             | -1.347602 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 33.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 783        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -2.24      |\n",
            "|    reward             | -0.2608544 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 0.0608     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 784      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 4.17     |\n",
            "|    reward             | 0.328512 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 0.282    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 785        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 15.6       |\n",
            "|    reward             | 0.07858739 |\n",
            "|    std                | 1.12       |\n",
            "|    value_loss         | 2.85       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -40       |\n",
            "|    reward             | -0.113649 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 10.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 783      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 12.5     |\n",
            "|    reward             | -1.81197 |\n",
            "|    std                | 1.12     |\n",
            "|    value_loss         | 2.68     |\n",
            "------------------------------------\n",
            "day: 2246, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1417776.07\n",
            "total_reward: 417776.07\n",
            "total_cost: 3127.41\n",
            "total_trades: 9331\n",
            "Sharpe: 0.300\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 783      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 0.318    |\n",
            "|    reward             | 1.039224 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.867    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 783      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0.00212  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -37.4    |\n",
            "|    reward             | 1.551367 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 18       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 15.1      |\n",
            "|    reward             | 0.7455919 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 2.33      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | -53.8     |\n",
            "|    reward             | -3.189159 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 25.5      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 783          |\n",
            "|    iterations         | 2700         |\n",
            "|    time_elapsed       | 17           |\n",
            "|    total_timesteps    | 13500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.7        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 2699         |\n",
            "|    policy_loss        | 0.126        |\n",
            "|    reward             | -0.052139886 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 0.0193       |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 783      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 9.38     |\n",
            "|    reward             | -0.29669 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 784      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.0482  |\n",
            "|    reward             | 0.128015 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 0.0941   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 38.3      |\n",
            "|    reward             | 0.3845    |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 13.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 28.8      |\n",
            "|    reward             | 1.250868  |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 6.13      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 784      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 5.17     |\n",
            "|    reward             | 2.174996 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.702    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 784      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 35.4     |\n",
            "|    reward             | 1.703575 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 21.1      |\n",
            "|    reward             | 0.334268  |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 2.74      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 783       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | -11.4     |\n",
            "|    reward             | -2.601658 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 4.56      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 771      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 2.62     |\n",
            "|    reward             | 0.413611 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.123    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 764         |\n",
            "|    iterations         | 3700        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 18500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3699        |\n",
            "|    policy_loss        | 1.54        |\n",
            "|    reward             | -0.88683933 |\n",
            "|    std                | 1.21        |\n",
            "|    value_loss         | 0.169       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 9.08      |\n",
            "|    reward             | -0.047712 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 0.907     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 14.4     |\n",
            "|    reward             | 0.188286 |\n",
            "|    std                | 1.27     |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -29      |\n",
            "|    reward             | 0.877    |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 6        |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 19.3      |\n",
            "|    reward             | 0.370623  |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 3.27      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 18.7     |\n",
            "|    reward             | 1.051935 |\n",
            "|    std                | 1.29     |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -15.9    |\n",
            "|    reward             | -0.86374 |\n",
            "|    std                | 1.26     |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -47.5     |\n",
            "|    reward             | -1.485268 |\n",
            "|    std                | 1.26      |\n",
            "|    value_loss         | 18        |\n",
            "-------------------------------------\n",
            "day: 2246, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1659785.09\n",
            "total_reward: 659785.09\n",
            "total_cost: 1897.74\n",
            "total_trades: 10955\n",
            "Sharpe: 0.442\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -0.000636 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -24.7     |\n",
            "|    reward             | -0.088348 |\n",
            "|    std                | 1.25      |\n",
            "|    value_loss         | 5.87      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | -3.95    |\n",
            "|    reward             | 0.38306  |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 0.268    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | -4.89     |\n",
            "|    reward             | -0.804768 |\n",
            "|    std                | 1.3       |\n",
            "|    value_loss         | 0.221     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -1.489865 |\n",
            "|    std                | 1.29      |\n",
            "|    value_loss         | 1.97      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 760       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -11.6     |\n",
            "|    reward             | -1.787683 |\n",
            "|    std                | 1.28      |\n",
            "|    value_loss         | 1.83      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -6.83    |\n",
            "|    reward             | 0.785402 |\n",
            "|    std                | 1.32     |\n",
            "|    value_loss         | 0.468    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 761       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 16.1      |\n",
            "|    reward             | -0.485561 |\n",
            "|    std                | 1.34      |\n",
            "|    value_loss         | 1.98      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 761        |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 34         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -27.9      |\n",
            "|    reward             | -10.482129 |\n",
            "|    std                | 1.34       |\n",
            "|    value_loss         | 23.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 761       |\n",
            "|    iterations         | 5300      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 26500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5299      |\n",
            "|    policy_loss        | -88.4     |\n",
            "|    reward             | 3.487714  |\n",
            "|    std                | 1.35      |\n",
            "|    value_loss         | 59.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 762       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 20.1      |\n",
            "|    reward             | -1.502895 |\n",
            "|    std                | 1.37      |\n",
            "|    value_loss         | 2.68      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 762         |\n",
            "|    iterations         | 5500        |\n",
            "|    time_elapsed       | 36          |\n",
            "|    total_timesteps    | 27500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5499        |\n",
            "|    policy_loss        | -5.03       |\n",
            "|    reward             | -0.18118753 |\n",
            "|    std                | 1.39        |\n",
            "|    value_loss         | 0.474       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -23.3    |\n",
            "|    reward             | 0.166279 |\n",
            "|    std                | 1.42     |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -28.5     |\n",
            "|    reward             | 1.05748   |\n",
            "|    std                | 1.42      |\n",
            "|    value_loss         | 8.4       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 763         |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 37          |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -19.2       |\n",
            "|    reward             | -0.36770582 |\n",
            "|    std                | 1.45        |\n",
            "|    value_loss         | 4.3         |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 9.99     |\n",
            "|    reward             | 0.165798 |\n",
            "|    std                | 1.51     |\n",
            "|    value_loss         | 0.766    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -3.25    |\n",
            "|    reward             | 0.076399 |\n",
            "|    std                | 1.54     |\n",
            "|    value_loss         | 0.748    |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.38669859742161444\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "day: 2246, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1415698.81\n",
            "total_reward: 415698.81\n",
            "total_cost: 6988.69\n",
            "total_trades: 7323\n",
            "Sharpe: 0.328\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.7      |\n",
            "|    critic_loss     | 1.36      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.129183 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1358695.72\n",
            "total_reward: 358695.72\n",
            "total_cost: 1614.89\n",
            "total_trades: 6792\n",
            "Sharpe: 0.299\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 240       |\n",
            "|    time_elapsed    | 74        |\n",
            "|    total_timesteps | 17976     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.35     |\n",
            "|    critic_loss     | 1.99      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 17875     |\n",
            "|    reward          | -0.129183 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1358695.72\n",
            "total_reward: 358695.72\n",
            "total_cost: 1614.89\n",
            "total_trades: 6792\n",
            "Sharpe: 0.299\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 111       |\n",
            "|    total_timesteps | 26964     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.18     |\n",
            "|    critic_loss     | 1.48      |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 26863     |\n",
            "|    reward          | -0.129183 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2369096659790081\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "day: 2246, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1544298.79\n",
            "total_reward: 544298.79\n",
            "total_cost: 1384.41\n",
            "total_trades: 9007\n",
            "Sharpe: 0.384\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 240      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 8988     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 50.9     |\n",
            "|    critic_loss     | 4.33e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8887     |\n",
            "|    reward          | -0.1321  |\n",
            "---------------------------------\n",
            "day: 2246, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1544298.79\n",
            "total_reward: 544298.79\n",
            "total_cost: 1384.41\n",
            "total_trades: 9007\n",
            "Sharpe: 0.384\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 237      |\n",
            "|    time_elapsed    | 75       |\n",
            "|    total_timesteps | 17976    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 174      |\n",
            "|    critic_loss     | 159      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17875    |\n",
            "|    reward          | -0.1321  |\n",
            "---------------------------------\n",
            "day: 2246, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1544298.79\n",
            "total_reward: 544298.79\n",
            "total_cost: 1384.41\n",
            "total_trades: 9007\n",
            "Sharpe: 0.384\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 233      |\n",
            "|    time_elapsed    | 115      |\n",
            "|    total_timesteps | 26964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 124      |\n",
            "|    critic_loss     | 46.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 26863    |\n",
            "|    reward          | -0.1321  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.24270739462699528\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "day: 2246, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1731900.29\n",
            "total_reward: 731900.29\n",
            "total_cost: 998.99\n",
            "total_trades: 11230\n",
            "Sharpe: 0.546\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 48        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 595       |\n",
            "|    critic_loss     | 1.44e+04  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.228495 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1731900.29\n",
            "total_reward: 731900.29\n",
            "total_cost: 998.99\n",
            "total_trades: 11230\n",
            "Sharpe: 0.546\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 188       |\n",
            "|    time_elapsed    | 95        |\n",
            "|    total_timesteps | 17976     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 849       |\n",
            "|    critic_loss     | 97.8      |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 17875     |\n",
            "|    reward          | -0.228495 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 187       |\n",
            "|    time_elapsed    | 143       |\n",
            "|    total_timesteps | 26964     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.02e+03  |\n",
            "|    critic_loss     | 22.7      |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 26863     |\n",
            "|    reward          | -0.228495 |\n",
            "----------------------------------\n",
            "day: 2246, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1731900.29\n",
            "total_reward: 731900.29\n",
            "total_cost: 998.99\n",
            "total_trades: 11230\n",
            "Sharpe: 0.546\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.5091287897701109\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 958         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 2           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.114047654 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 915          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003096829 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00633      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.04         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.000715    |\n",
            "|    reward               | 0.03814688   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.56         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 907           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00075639493 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.0154        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.84          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00196      |\n",
            "|    reward               | 0.67255104    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.15          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 902          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005790141 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00548     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.39         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00141     |\n",
            "|    reward               | 0.93200505   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.06         |\n",
            "------------------------------------------\n",
            "day: 2246, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1301562.32\n",
            "total_reward: 301562.32\n",
            "total_cost: 142730.74\n",
            "total_trades: 17451\n",
            "Sharpe: 0.287\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 899          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002586367 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00292     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.51         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.000665    |\n",
            "|    reward               | 0.062952206  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.48         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 897           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023302171 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00876       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.88          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000842     |\n",
            "|    reward               | -0.5183094    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.78          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 896           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00043423116 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0116       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.17          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    reward               | 0.15808177    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.69          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 897          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.803569e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0105      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.59         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.000462    |\n",
            "|    reward               | 0.117273316  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.87         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 898           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 20            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024078769 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.000767     |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.54          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.00083      |\n",
            "|    reward               | 0.601492      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.38          |\n",
            "-------------------------------------------\n",
            "day: 2246, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1370637.41\n",
            "total_reward: 370637.41\n",
            "total_cost: 135872.99\n",
            "total_trades: 17250\n",
            "Sharpe: 0.325\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 898           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024397596 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00447      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.13          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000932     |\n",
            "|    reward               | -0.27064115   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.1          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 898          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.503396e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00833     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.22         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000581    |\n",
            "|    reward               | -0.13641281  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.71         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 895          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007591782 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00362     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.29         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00133     |\n",
            "|    reward               | -0.080893405 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 894          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003529028 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00615      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 5.98         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    reward               | -0.8744909   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.41         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 893           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00019213962 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0104       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.63          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.00108      |\n",
            "|    reward               | 0.33003125    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11.3          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 885           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.0454385e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00496      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.89          |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000686     |\n",
            "|    reward               | -2.8357813    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.6           |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.5317780934416635\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 756       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -3.46     |\n",
            "|    reward             | -0.175402 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.0919    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 749      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 4.52     |\n",
            "|    reward             | 0.096768 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.208    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 736       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 6.01      |\n",
            "|    reward             | 0.201163  |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.572     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 743      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 11.1     |\n",
            "|    reward             | 1.713746 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 749      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 6.67     |\n",
            "|    reward             | 1.233595 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.441    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 755      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.446    |\n",
            "|    reward             | 0.217563 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.00295  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 15       |\n",
            "|    reward             | 0.341673 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 2.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 753      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -6       |\n",
            "|    reward             | 0.595702 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 757       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 23        |\n",
            "|    reward             | -0.99619  |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 4.13      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 756         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -3.01       |\n",
            "|    reward             | -0.07692526 |\n",
            "|    std                | 1.15        |\n",
            "|    value_loss         | 0.15        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 756      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -1.69    |\n",
            "|    reward             | 0.804719 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.0734   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 735       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 10.8      |\n",
            "|    reward             | 0.80442   |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.11      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 716      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -28.8    |\n",
            "|    reward             | 1.242754 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 7.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 686      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -14.7    |\n",
            "|    reward             | 0.01358  |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 663       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 0.13      |\n",
            "|    reward             | -0.007174 |\n",
            "|    std                | 1.2       |\n",
            "|    value_loss         | 0.0238    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 658      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 4.97     |\n",
            "|    reward             | 0.970778 |\n",
            "|    std                | 1.24     |\n",
            "|    value_loss         | 0.264    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 662       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 2.59      |\n",
            "|    reward             | -0.043943 |\n",
            "|    std                | 1.22      |\n",
            "|    value_loss         | 0.622     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 668      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 9.73     |\n",
            "|    reward             | 0.149188 |\n",
            "|    std                | 1.19     |\n",
            "|    value_loss         | 0.897    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 673       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -15.2     |\n",
            "|    reward             | 0.871994  |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 1.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 677       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.1     |\n",
            "|    reward             | -0.460206 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 0.913     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 681       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | -12.7     |\n",
            "|    reward             | 1.3254738 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 1.67      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 685       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -59.7     |\n",
            "|    reward             | -0.531261 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 25.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 688      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | -1.16    |\n",
            "|    reward             | 0.847113 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.422    |\n",
            "------------------------------------\n",
            "day: 2309, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1849829.60\n",
            "total_reward: 849829.60\n",
            "total_cost: 1375.13\n",
            "total_trades: 10279\n",
            "Sharpe: 0.608\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 691       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | -9.68     |\n",
            "|    reward             | -0.050052 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 0.925     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 694      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 6.87     |\n",
            "|    reward             | 1.39598  |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 0.709    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -19.7    |\n",
            "|    reward             | 1.470684 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 2.76     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 16        |\n",
            "|    reward             | 2.131036  |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 2.99      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 702       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -0.384    |\n",
            "|    reward             | -1.639512 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.0632    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 704      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -4.97    |\n",
            "|    reward             | 0.276001 |\n",
            "|    std                | 1.18     |\n",
            "|    value_loss         | 0.123    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 706      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0.00169  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -22.1    |\n",
            "|    reward             | 0.915924 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 708       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -1.34     |\n",
            "|    reward             | -0.402934 |\n",
            "|    std                | 1.17      |\n",
            "|    value_loss         | 0.104     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 711       |\n",
            "|    iterations         | 3200      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | -10.2     |\n",
            "|    reward             | -0.474714 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.868     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -10.2    |\n",
            "|    reward             | 0.753516 |\n",
            "|    std                | 1.17     |\n",
            "|    value_loss         | 0.868    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 715       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 11.3      |\n",
            "|    reward             | -0.139208 |\n",
            "|    std                | 1.18      |\n",
            "|    value_loss         | 1.13      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 717      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 0.212    |\n",
            "|    reward             | 0.771063 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.845    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 719       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -19.4     |\n",
            "|    reward             | -3.159064 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 4.9       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 719        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | 2.54       |\n",
            "|    reward             | -0.2785562 |\n",
            "|    std                | 1.17       |\n",
            "|    value_loss         | 0.0505     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 4.5      |\n",
            "|    reward             | 0.057904 |\n",
            "|    std                | 1.16     |\n",
            "|    value_loss         | 0.19     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 5.76      |\n",
            "|    reward             | -0.288992 |\n",
            "|    std                | 1.19      |\n",
            "|    value_loss         | 0.361     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 19.7      |\n",
            "|    reward             | -0.156992 |\n",
            "|    std                | 1.21      |\n",
            "|    value_loss         | 3.08      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 725         |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 28          |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | 5.64        |\n",
            "|    reward             | -0.78166115 |\n",
            "|    std                | 1.22        |\n",
            "|    value_loss         | 0.425       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 726      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 1.1      |\n",
            "|    reward             | 0.45576  |\n",
            "|    std                | 1.28     |\n",
            "|    value_loss         | 0.114    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 728         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 29          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.2       |\n",
            "|    explained_variance | 0.463       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | 1.56        |\n",
            "|    reward             | -0.18590459 |\n",
            "|    std                | 1.28        |\n",
            "|    value_loss         | 0.0456      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 4400       |\n",
            "|    time_elapsed       | 30         |\n",
            "|    total_timesteps    | 22000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.3      |\n",
            "|    explained_variance | 0.333      |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4399       |\n",
            "|    policy_loss        | -0.23      |\n",
            "|    reward             | 0.05918128 |\n",
            "|    std                | 1.29       |\n",
            "|    value_loss         | 0.0281     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 729         |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 30          |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | 7.3         |\n",
            "|    reward             | -0.23094653 |\n",
            "|    std                | 1.33        |\n",
            "|    value_loss         | 0.316       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 731        |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | 46.5       |\n",
            "|    reward             | 0.55859923 |\n",
            "|    std                | 1.4        |\n",
            "|    value_loss         | 13         |\n",
            "--------------------------------------\n",
            "day: 2309, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1309406.46\n",
            "total_reward: 309406.46\n",
            "total_cost: 73175.91\n",
            "total_trades: 11926\n",
            "Sharpe: 0.422\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 731       |\n",
            "|    iterations         | 4700      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 23500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4699      |\n",
            "|    policy_loss        | 1.71      |\n",
            "|    reward             | -0.24574  |\n",
            "|    std                | 1.4       |\n",
            "|    value_loss         | 0.0964    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 732        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -14        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | 12.3       |\n",
            "|    reward             | 0.68981916 |\n",
            "|    std                | 1.41       |\n",
            "|    value_loss         | 1.13       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 733      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 8.01     |\n",
            "|    reward             | 0.374629 |\n",
            "|    std                | 1.39     |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 734         |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 34          |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.8       |\n",
            "|    explained_variance | -0.000778   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | 3.5         |\n",
            "|    reward             | -0.45141062 |\n",
            "|    std                | 1.37        |\n",
            "|    value_loss         | 0.335       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 734         |\n",
            "|    iterations         | 5100        |\n",
            "|    time_elapsed       | 34          |\n",
            "|    total_timesteps    | 25500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13.8       |\n",
            "|    explained_variance | -5.31       |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 5099        |\n",
            "|    policy_loss        | -16.2       |\n",
            "|    reward             | -0.34192488 |\n",
            "|    std                | 1.38        |\n",
            "|    value_loss         | 1.4         |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 735      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 5.37     |\n",
            "|    reward             | 1.197928 |\n",
            "|    std                | 1.4      |\n",
            "|    value_loss         | 0.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 8.29     |\n",
            "|    reward             | 1.553406 |\n",
            "|    std                | 1.41     |\n",
            "|    value_loss         | 0.476    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 737       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | 28.3      |\n",
            "|    reward             | -0.270537 |\n",
            "|    std                | 1.41      |\n",
            "|    value_loss         | 5.89      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 737       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | -9.04     |\n",
            "|    reward             | -0.375261 |\n",
            "|    std                | 1.44      |\n",
            "|    value_loss         | 1.01      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 737       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 29.1      |\n",
            "|    reward             | -0.532865 |\n",
            "|    std                | 1.48      |\n",
            "|    value_loss         | 6.19      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 737      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -14.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | -13.7    |\n",
            "|    reward             | 0.06765  |\n",
            "|    std                | 1.51     |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 738       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -29       |\n",
            "|    reward             | 2.0532143 |\n",
            "|    std                | 1.53      |\n",
            "|    value_loss         | 5.77      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 738       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -14.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | -14.4     |\n",
            "|    reward             | 2.9432495 |\n",
            "|    std                | 1.53      |\n",
            "|    value_loss         | 1.68      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 739          |\n",
            "|    iterations         | 6000         |\n",
            "|    time_elapsed       | 40           |\n",
            "|    total_timesteps    | 30000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -14.7        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.005        |\n",
            "|    n_updates          | 5999         |\n",
            "|    policy_loss        | 24           |\n",
            "|    reward             | -0.001626093 |\n",
            "|    std                | 1.56         |\n",
            "|    value_loss         | 3.62         |\n",
            "----------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.07822228028297296\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "day: 2309, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 906980.62\n",
            "total_reward: -93019.38\n",
            "total_cost: 999.00\n",
            "total_trades: 4618\n",
            "Sharpe: -0.118\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 227       |\n",
            "|    time_elapsed    | 40        |\n",
            "|    total_timesteps | 9240      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.14     |\n",
            "|    critic_loss     | 3.6       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 9139      |\n",
            "|    reward          | -0.226414 |\n",
            "----------------------------------\n",
            "day: 2309, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 906980.62\n",
            "total_reward: -93019.38\n",
            "total_cost: 999.00\n",
            "total_trades: 4618\n",
            "Sharpe: -0.118\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 231       |\n",
            "|    time_elapsed    | 79        |\n",
            "|    total_timesteps | 18480     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.53     |\n",
            "|    critic_loss     | 0.234     |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 18379     |\n",
            "|    reward          | -0.226414 |\n",
            "----------------------------------\n",
            "day: 2309, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 906980.62\n",
            "total_reward: -93019.38\n",
            "total_cost: 999.00\n",
            "total_trades: 4618\n",
            "Sharpe: -0.118\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 232       |\n",
            "|    time_elapsed    | 119       |\n",
            "|    total_timesteps | 27720     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.15     |\n",
            "|    critic_loss     | 0.218     |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 27619     |\n",
            "|    reward          | -0.226414 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2453669436670692\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "day: 2309, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1961716.20\n",
            "total_reward: 961716.20\n",
            "total_cost: 999.00\n",
            "total_trades: 9236\n",
            "Sharpe: 0.638\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 235      |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 44.8     |\n",
            "|    critic_loss     | 6.56     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 2.008    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 235      |\n",
            "|    time_elapsed    | 78       |\n",
            "|    total_timesteps | 18480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 74.2     |\n",
            "|    critic_loss     | 3.62     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 18379    |\n",
            "|    reward          | 2.008    |\n",
            "---------------------------------\n",
            "day: 2309, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1961716.20\n",
            "total_reward: 961716.20\n",
            "total_cost: 999.00\n",
            "total_trades: 9236\n",
            "Sharpe: 0.638\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 229      |\n",
            "|    time_elapsed    | 121      |\n",
            "|    total_timesteps | 27720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 64.2     |\n",
            "|    critic_loss     | 4.12     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27619    |\n",
            "|    reward          | 2.008    |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.27664144489752107\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "day: 2309, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1654982.14\n",
            "total_reward: 654982.14\n",
            "total_cost: 6377.35\n",
            "total_trades: 7457\n",
            "Sharpe: 0.391\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 191      |\n",
            "|    time_elapsed    | 48       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 728      |\n",
            "|    critic_loss     | 1.1e+03  |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 0.643213 |\n",
            "---------------------------------\n",
            "day: 2309, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1380143.81\n",
            "total_reward: 380143.81\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 0.343\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 191      |\n",
            "|    time_elapsed    | 96       |\n",
            "|    total_timesteps | 18480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 898      |\n",
            "|    critic_loss     | 4.93e+03 |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 18379    |\n",
            "|    reward          | 0.643213 |\n",
            "---------------------------------\n",
            "day: 2309, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1380143.81\n",
            "total_reward: 380143.81\n",
            "total_cost: 998.99\n",
            "total_trades: 6927\n",
            "Sharpe: 0.343\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 191      |\n",
            "|    time_elapsed    | 144      |\n",
            "|    total_timesteps | 27720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.07e+03 |\n",
            "|    critic_loss     | 65.3     |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 27619    |\n",
            "|    reward          | 0.643213 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.10783531663347884\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 944        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.8151814 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 909           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065903267 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00112      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.3           |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00138      |\n",
            "|    reward               | -2.610139     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.16          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 898          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003326033 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.04         |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.000813    |\n",
            "|    reward               | -0.99073     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.29         |\n",
            "------------------------------------------\n",
            "day: 2309, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1473822.87\n",
            "total_reward: 473822.87\n",
            "total_cost: 146986.16\n",
            "total_trades: 17682\n",
            "Sharpe: 0.366\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 895           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018764896 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0181       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.36          |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00094      |\n",
            "|    reward               | -0.06610394   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.42          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 889           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 11            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018947269 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0195       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.68          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000617     |\n",
            "|    reward               | 0.03622256    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.01          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 889           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025084984 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00454       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 6.32          |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000687     |\n",
            "|    reward               | 0.1638327     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 11.4          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 814           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014498108 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00549      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.55          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000524     |\n",
            "|    reward               | 0.32446888    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.2          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 820         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000637343 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0181     |\n",
            "|    learning_rate        | 1e-05       |\n",
            "|    loss                 | 1.9         |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00157    |\n",
            "|    reward               | -0.36372337 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.69        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 826           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021302834 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0119       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.89          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000936     |\n",
            "|    reward               | 0.99345154    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.28          |\n",
            "-------------------------------------------\n",
            "day: 2309, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1643574.92\n",
            "total_reward: 643574.92\n",
            "total_cost: 154037.35\n",
            "total_trades: 17935\n",
            "Sharpe: 0.504\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 829          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004619004 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0301      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.78         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.000867    |\n",
            "|    reward               | 1.6288446    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.09         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 833           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00072126184 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.000877      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.92          |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00142      |\n",
            "|    reward               | 1.2471055     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.81          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 836          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.543181e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0558      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.97         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00052     |\n",
            "|    reward               | 0.3617051    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.13         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 839           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 31            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013767713 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.00616      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.01          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00058      |\n",
            "|    reward               | 0.36357057    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9             |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 841           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038275748 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0274       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.56          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000786     |\n",
            "|    reward               | -1.0415182    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.87          |\n",
            "-------------------------------------------\n",
            "day: 2309, episode: 65\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1889480.15\n",
            "total_reward: 889480.15\n",
            "total_cost: 143621.27\n",
            "total_trades: 17663\n",
            "Sharpe: 0.561\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 843          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.936222e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.000717    |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.94         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.000394    |\n",
            "|    reward               | 0.07412939   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.67         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.2563298997504203\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 758        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -8.8       |\n",
            "|    reward             | -0.2923182 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.566      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 8.74      |\n",
            "|    reward             | -0.144534 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.697     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 766       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 50.9      |\n",
            "|    reward             | -2.037633 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 31.5      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 759      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 19.9     |\n",
            "|    reward             | 2.151174 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 4.76     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 758       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -11.9     |\n",
            "|    reward             | -0.170926 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.89      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0115    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -0.209    |\n",
            "|    reward             | -0.275315 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.118     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 757       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -3.71     |\n",
            "|    reward             | -0.913347 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 0.262     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0047  |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -16.2    |\n",
            "|    reward             | 3.374767 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 762        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 11.2       |\n",
            "|    reward             | 0.81086683 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.98       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -2.83    |\n",
            "|    reward             | 0.571858 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 0.587    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 18.8      |\n",
            "|    reward             | -1.161438 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.17      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 761       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 13.3      |\n",
            "|    reward             | -1.643846 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.5       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 762       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 1.09      |\n",
            "|    reward             | -2.236818 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.9       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 762       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -0.604    |\n",
            "|    reward             | -1.298901 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.92      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 763      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 0.762    |\n",
            "|    reward             | 0.904133 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.0353   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 763       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 11.8      |\n",
            "|    reward             | -0.277547 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.02      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 18.4     |\n",
            "|    reward             | -1.09743 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 765        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 7.13       |\n",
            "|    reward             | -1.6400921 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 3.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.669    |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -47.2     |\n",
            "|    reward             | 0.4445431 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 52.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.2     |\n",
            "|    reward             | -0.240679 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.12      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 766      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 0.951563 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 766       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 14.4      |\n",
            "|    reward             | 1.0891297 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.94      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 767       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.000163 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 25.5      |\n",
            "|    reward             | -0.382215 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 7.07      |\n",
            "-------------------------------------\n",
            "day: 2372, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1943652.11\n",
            "total_reward: 943652.11\n",
            "total_cost: 2668.53\n",
            "total_trades: 9308\n",
            "Sharpe: 0.476\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 766      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -13.3    |\n",
            "|    reward             | 0.4161   |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 767       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -5.19     |\n",
            "|    reward             | -0.430851 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.253     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 767      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 0.51     |\n",
            "|    reward             | 1.633339 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.173    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.00122  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 26.6      |\n",
            "|    reward             | -0.688466 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 9.24      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | -3.04     |\n",
            "|    reward             | -1.820094 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.599     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -28.7    |\n",
            "|    reward             | 1.460978 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 8.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -39.4    |\n",
            "|    reward             | 1.47063  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 13.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -9.44     |\n",
            "|    reward             | -0.012208 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.22      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -53.5    |\n",
            "|    reward             | 4.215157 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 40.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 3300      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 16500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3299      |\n",
            "|    policy_loss        | 36.1      |\n",
            "|    reward             | 2.124014  |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 11.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 0.748    |\n",
            "|    reward             | 0.077919 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.123    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 9.15      |\n",
            "|    reward             | -0.685791 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.873     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | 4.15      |\n",
            "|    reward             | -0.520134 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.783     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 21.1     |\n",
            "|    reward             | 1.814292 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 4        |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 768         |\n",
            "|    iterations         | 3800        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 19000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.005       |\n",
            "|    n_updates          | 3799        |\n",
            "|    policy_loss        | 0.903       |\n",
            "|    reward             | -0.00923828 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 0.0219      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 0.108325 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 0.897    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -5.07    |\n",
            "|    reward             | 0.033288 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 25       |\n",
            "|    reward             | 0.324694 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 7.23     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | 22.4      |\n",
            "|    reward             | -1.928396 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 4.12      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -11      |\n",
            "|    reward             | 2.925065 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.962    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -1.16    |\n",
            "|    reward             | 0.30324  |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 10.5     |\n",
            "|    reward             | 1.752299 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | -5.74    |\n",
            "|    reward             | 0.71829  |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.399    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -0.0813  |\n",
            "|    reward             | -0.98638 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.579    |\n",
            "------------------------------------\n",
            "day: 2372, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1791961.07\n",
            "total_reward: 791961.07\n",
            "total_cost: 1687.38\n",
            "total_trades: 11915\n",
            "Sharpe: 0.438\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 3.41e-05  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 20.3      |\n",
            "|    reward             | -0.428747 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 5         |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 29.8     |\n",
            "|    reward             | 0.179926 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 7.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 12.3     |\n",
            "|    reward             | 0.313976 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 33        |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 1.68      |\n",
            "|    reward             | -2.632762 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 10.8      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 0.586    |\n",
            "|    reward             | 0.758292 |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.996    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 34       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -2.51    |\n",
            "|    reward             | 0.714676 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.203    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -0.667   |\n",
            "|    reward             | -0.4152  |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 0.489    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 2.18      |\n",
            "|    reward             | -0.955389 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.16      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 16.6      |\n",
            "|    reward             | 0.236862  |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 2.04      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0.529     |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | 4.29      |\n",
            "|    reward             | 0.3620635 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 0.153     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 3.38      |\n",
            "|    reward             | -1.055884 |\n",
            "|    std                | 1.16      |\n",
            "|    value_loss         | 0.22      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 7.46      |\n",
            "|    reward             | -0.224595 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.786     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 22.9      |\n",
            "|    reward             | -2.571146 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 5.14      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.17640807820137663\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "day: 2372, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1819453.56\n",
            "total_reward: 819453.56\n",
            "total_cost: 998.96\n",
            "total_trades: 7116\n",
            "Sharpe: 0.633\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 232      |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -129     |\n",
            "|    critic_loss     | 41.1     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.169378 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1819453.56\n",
            "total_reward: 819453.56\n",
            "total_cost: 998.96\n",
            "total_trades: 7116\n",
            "Sharpe: 0.633\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 235      |\n",
            "|    time_elapsed    | 80       |\n",
            "|    total_timesteps | 18984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -147     |\n",
            "|    critic_loss     | 4.33     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 18883    |\n",
            "|    reward          | 0.169378 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1819453.56\n",
            "total_reward: 819453.56\n",
            "total_cost: 998.96\n",
            "total_trades: 7116\n",
            "Sharpe: 0.633\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 236      |\n",
            "|    time_elapsed    | 120      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -123     |\n",
            "|    critic_loss     | 7.23     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 28375    |\n",
            "|    reward          | 0.169378 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.12781914303975678\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "day: 2372, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1847448.68\n",
            "total_reward: 847448.68\n",
            "total_cost: 998.99\n",
            "total_trades: 14232\n",
            "Sharpe: 0.564\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 245      |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 68.5     |\n",
            "|    critic_loss     | 7.29     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | -0.20113 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 243      |\n",
            "|    time_elapsed    | 77       |\n",
            "|    total_timesteps | 18984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 63.5     |\n",
            "|    critic_loss     | 1.79     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 18883    |\n",
            "|    reward          | -0.20113 |\n",
            "---------------------------------\n",
            "day: 2372, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1847448.68\n",
            "total_reward: 847448.68\n",
            "total_cost: 998.99\n",
            "total_trades: 14232\n",
            "Sharpe: 0.564\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 244      |\n",
            "|    time_elapsed    | 116      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 48.8     |\n",
            "|    critic_loss     | 3.36     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28375    |\n",
            "|    reward          | -0.20113 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.2161366067474184\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "day: 2372, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1834542.46\n",
            "total_reward: 834542.46\n",
            "total_cost: 6293.21\n",
            "total_trades: 9883\n",
            "Sharpe: 0.563\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 189       |\n",
            "|    time_elapsed    | 50        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 589       |\n",
            "|    critic_loss     | 4.96e+03  |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.352056 |\n",
            "----------------------------------\n",
            "day: 2372, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1689225.65\n",
            "total_reward: 689225.65\n",
            "total_cost: 998.95\n",
            "total_trades: 9488\n",
            "Sharpe: 0.522\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 188       |\n",
            "|    time_elapsed    | 100       |\n",
            "|    total_timesteps | 18984     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 696       |\n",
            "|    critic_loss     | 285       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 18883     |\n",
            "|    reward          | -0.352056 |\n",
            "----------------------------------\n",
            "day: 2372, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1689225.65\n",
            "total_reward: 689225.65\n",
            "total_cost: 998.95\n",
            "total_trades: 9488\n",
            "Sharpe: 0.522\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 188       |\n",
            "|    time_elapsed    | 151       |\n",
            "|    total_timesteps | 28476     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 848       |\n",
            "|    critic_loss     | 885       |\n",
            "|    ent_coef        | 0.1       |\n",
            "|    learning_rate   | 0.005     |\n",
            "|    n_updates       | 28375     |\n",
            "|    reward          | -0.352056 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.26474841282517664\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 919        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.5169661 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 888           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00037129142 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0509       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.28          |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00122      |\n",
            "|    reward               | -0.27474257   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.95          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 874           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041893852 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0237       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.63          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00118      |\n",
            "|    reward               | -0.00858414   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.45          |\n",
            "-------------------------------------------\n",
            "day: 2372, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1491989.25\n",
            "total_reward: 491989.25\n",
            "total_cost: 154836.55\n",
            "total_trades: 18353\n",
            "Sharpe: 0.391\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 871           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033540453 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0228       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.9           |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.00105      |\n",
            "|    reward               | -0.20012684   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.97          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 866           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 11            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067887316 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0131       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.87          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00172      |\n",
            "|    reward               | 0.53013176    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.6           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 863          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001457856 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0173      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.5          |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000572    |\n",
            "|    reward               | 0.093508966  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.22         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 863           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 16            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00057120435 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00863       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.08          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.0012       |\n",
            "|    reward               | -0.053089935  |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.04          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 860           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 19            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00064513215 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0137       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.23          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.00148      |\n",
            "|    reward               | -0.22481634   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.39          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 858           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00060740963 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0492       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.48          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.0016       |\n",
            "|    reward               | -2.4853132    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.25          |\n",
            "-------------------------------------------\n",
            "day: 2372, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1402732.32\n",
            "total_reward: 402732.32\n",
            "total_cost: 149756.58\n",
            "total_trades: 18153\n",
            "Sharpe: 0.313\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 858           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00037980752 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0101       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.81          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.00113      |\n",
            "|    reward               | -0.73443747   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.46          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 857          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.471981e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0207      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 5.26         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000506    |\n",
            "|    reward               | -0.27880478  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 857           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033234133 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0175       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.06          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.000851     |\n",
            "|    reward               | 0.37158868    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.94          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 857           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 31            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00031796648 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.021        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.19          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00102      |\n",
            "|    reward               | 0.4193055     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.5           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 857           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013438705 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0295       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.53          |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.000708     |\n",
            "|    reward               | 0.27751198    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.6          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 857           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00025450392 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0073       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.9           |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00084      |\n",
            "|    reward               | -0.10483897   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.99          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.2511327348446871\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.005}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 744        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -36        |\n",
            "|    reward             | -0.3102523 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 8.33       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 745      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 9.2      |\n",
            "|    reward             | 0.096166 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.779    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 742       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0026   |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 33.5      |\n",
            "|    reward             | -1.093496 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 734       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 10.5      |\n",
            "|    reward             | 2.145535  |\n",
            "|    std                | 0.978     |\n",
            "|    value_loss         | 1.92      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | -0.609     |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -4.46      |\n",
            "|    reward             | -1.1352829 |\n",
            "|    std                | 0.984      |\n",
            "|    value_loss         | 0.272      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 741       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 3.17      |\n",
            "|    reward             | -0.308311 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.27      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 744       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -5.96e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -4.06     |\n",
            "|    reward             | -0.288459 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.234     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 747       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 14        |\n",
            "|    reward             | -1.391622 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 2.24      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -1.19     |\n",
            "|    reward             | -0.560148 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.16      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 747       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -0.000341 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 3.82      |\n",
            "|    reward             | 0.305161  |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.944     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 749       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 0.4       |\n",
            "|    reward             | -0.101739 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 0.0841    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -12.4    |\n",
            "|    reward             | 1.004166 |\n",
            "|    std                | 0.992    |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -11.4    |\n",
            "|    reward             | 1.891181 |\n",
            "|    std                | 0.955    |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 15.5     |\n",
            "|    reward             | 1.474427 |\n",
            "|    std                | 0.97     |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 752       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 13.6      |\n",
            "|    reward             | -0.269922 |\n",
            "|    std                | 0.971     |\n",
            "|    value_loss         | 1.86      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 752       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -0.161    |\n",
            "|    reward             | -0.557276 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 0.0724    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -0.801   |\n",
            "|    reward             | 0.527964 |\n",
            "|    std                | 0.963    |\n",
            "|    value_loss         | 0.231    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -3.58    |\n",
            "|    reward             | 1.021955 |\n",
            "|    std                | 0.959    |\n",
            "|    value_loss         | 0.922    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -10.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 37       |\n",
            "|    reward             | 0.427433 |\n",
            "|    std                | 0.965    |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 751       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 14.8      |\n",
            "|    reward             | -0.770553 |\n",
            "|    std                | 0.948     |\n",
            "|    value_loss         | 2.29      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 751       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 5.52      |\n",
            "|    reward             | -0.027693 |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 0.651     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 751       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -10.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 3.31      |\n",
            "|    reward             | -0.010844 |\n",
            "|    std                | 0.956     |\n",
            "|    value_loss         | 0.417     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 752       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -16.8     |\n",
            "|    reward             | -3.176212 |\n",
            "|    std                | 0.968     |\n",
            "|    value_loss         | 2.85      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 8.11     |\n",
            "|    reward             | 2.059705 |\n",
            "|    std                | 0.988    |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "day: 2435, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2480908.20\n",
            "total_reward: 1480908.20\n",
            "total_cost: 1514.22\n",
            "total_trades: 14535\n",
            "Sharpe: 0.720\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 753        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -7.05      |\n",
            "|    reward             | 0.87296647 |\n",
            "|    std                | 0.974      |\n",
            "|    value_loss         | 0.539      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 752       |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | 2.05      |\n",
            "|    reward             | 1.409931  |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 0.448     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 753       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -2.31     |\n",
            "|    reward             | -2.057764 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 3.34      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 753      |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -19.3    |\n",
            "|    reward             | 0.972627 |\n",
            "|    std                | 0.987    |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 754       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -55.9     |\n",
            "|    reward             | 1.747395  |\n",
            "|    std                | 0.974     |\n",
            "|    value_loss         | 35.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 754       |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | -3.55     |\n",
            "|    reward             | -0.122755 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.156     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 754       |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -11.4     |\n",
            "|    reward             | 0.296777  |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.18      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -4.87    |\n",
            "|    reward             | 0.27618  |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.524    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 752      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 47.7     |\n",
            "|    reward             | 2.805086 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 16.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 753       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 10.5      |\n",
            "|    reward             | -2.964319 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.37      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | -12.8    |\n",
            "|    reward             | 0.761153 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 750      |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | -21.7    |\n",
            "|    reward             | 0.394464 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 5.79     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 749       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 7.35      |\n",
            "|    reward             | -1.088845 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.01      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 11        |\n",
            "|    reward             | -0.263851 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 3.27      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 747       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -2.36     |\n",
            "|    reward             | -0.303506 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.0766    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 4.14     |\n",
            "|    reward             | 0.256178 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.169    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 13       |\n",
            "|    reward             | 0.1598   |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -36.9    |\n",
            "|    reward             | 0.051442 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 11.3     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 748        |\n",
            "|    iterations         | 4300       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 21500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.005      |\n",
            "|    n_updates          | 4299       |\n",
            "|    policy_loss        | 14.1       |\n",
            "|    reward             | -1.7256374 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 2.99       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 8.75     |\n",
            "|    reward             | 0.029622 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.784    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -7.14     |\n",
            "|    reward             | -0.050915 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.481     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 749       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 2.45      |\n",
            "|    reward             | -1.692529 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.23      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 749      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 5.86     |\n",
            "|    reward             | 1.070914 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 0.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 749      |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | -27.6    |\n",
            "|    reward             | -0.8279  |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "day: 2435, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1953639.29\n",
            "total_reward: 953639.29\n",
            "total_cost: 998.99\n",
            "total_trades: 16457\n",
            "Sharpe: 0.597\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 749       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -0.154    |\n",
            "|    reward             | -1.508613 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 0.0565    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 749      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -4.4     |\n",
            "|    reward             | 0.317068 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 0.161    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 750      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -19.2    |\n",
            "|    reward             | 0.895538 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 4.49     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 750       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -1.94     |\n",
            "|    reward             | -0.376668 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.144     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 750      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -17.3    |\n",
            "|    reward             | -0.64207 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 750      |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -18.7    |\n",
            "|    reward             | 0.246776 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 2.83     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 751       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 36        |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 4.55      |\n",
            "|    reward             | -0.502785 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.208     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -4.64    |\n",
            "|    reward             | -0.03464 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.302    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 0.435    |\n",
            "|    reward             | 1.078203 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.303    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 38       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -17.6    |\n",
            "|    reward             | 2.534162 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 29500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.005    |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | 18.5     |\n",
            "|    reward             | 0.822652 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 3.05     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 752       |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 39        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.005     |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 0.521     |\n",
            "|    reward             | -0.830325 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.49      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.027701402496733023\n",
            "======ddpg Training========\n",
            "{'buffer_size': 50000, 'learning_rate': 0.005, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "day: 2435, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2019407.08\n",
            "total_reward: 1019407.08\n",
            "total_cost: 998.95\n",
            "total_trades: 4870\n",
            "Sharpe: 0.467\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 234      |\n",
            "|    time_elapsed    | 41       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 22.5     |\n",
            "|    critic_loss     | 152      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 2.079343 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2019407.08\n",
            "total_reward: 1019407.08\n",
            "total_cost: 998.95\n",
            "total_trades: 4870\n",
            "Sharpe: 0.467\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 232      |\n",
            "|    time_elapsed    | 83       |\n",
            "|    total_timesteps | 19488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 5.89     |\n",
            "|    critic_loss     | 2.79     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 19387    |\n",
            "|    reward          | 2.079343 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2019407.08\n",
            "total_reward: 1019407.08\n",
            "total_cost: 998.95\n",
            "total_trades: 4870\n",
            "Sharpe: 0.467\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 229      |\n",
            "|    time_elapsed    | 127      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.372   |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 29131    |\n",
            "|    reward          | 2.079343 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.021284872147851942\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "day: 2435, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1478586.46\n",
            "total_reward: 478586.46\n",
            "total_cost: 999.00\n",
            "total_trades: 7305\n",
            "Sharpe: 0.498\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 232      |\n",
            "|    time_elapsed    | 41       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 101      |\n",
            "|    critic_loss     | 60       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | -0.32114 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 237      |\n",
            "|    time_elapsed    | 82       |\n",
            "|    total_timesteps | 19488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 84.3     |\n",
            "|    critic_loss     | 3.07     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19387    |\n",
            "|    reward          | -0.32114 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 35\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1478586.46\n",
            "total_reward: 478586.46\n",
            "total_cost: 999.00\n",
            "total_trades: 7305\n",
            "Sharpe: 0.498\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 236      |\n",
            "|    time_elapsed    | 123      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 60.4     |\n",
            "|    critic_loss     | 60.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29131    |\n",
            "|    reward          | -0.32114 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.21214629228429277\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.005, 'learning_starts': 100, 'ent_coef': 0.1}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "day: 2435, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2061073.08\n",
            "total_reward: 1061073.08\n",
            "total_cost: 5858.02\n",
            "total_trades: 10115\n",
            "Sharpe: 0.491\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 185      |\n",
            "|    time_elapsed    | 52       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 418      |\n",
            "|    critic_loss     | 60       |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.729408 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 45\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2298851.08\n",
            "total_reward: 1298851.08\n",
            "total_cost: 998.92\n",
            "total_trades: 9740\n",
            "Sharpe: 0.638\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 186      |\n",
            "|    time_elapsed    | 104      |\n",
            "|    total_timesteps | 19488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 627      |\n",
            "|    critic_loss     | 202      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 19387    |\n",
            "|    reward          | 1.729408 |\n",
            "---------------------------------\n",
            "day: 2435, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2298851.08\n",
            "total_reward: 1298851.08\n",
            "total_cost: 998.92\n",
            "total_trades: 9740\n",
            "Sharpe: 0.638\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 186      |\n",
            "|    time_elapsed    | 156      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 724      |\n",
            "|    critic_loss     | 244      |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.005    |\n",
            "|    n_updates       | 29131    |\n",
            "|    reward          | 1.729408 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.07309047588756054\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.005, 'n_steps': 2048, 'learning_rate': 1e-05, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 891        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.8768584 |\n",
            "-----------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 848           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023284619 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0446       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 0.968         |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000832     |\n",
            "|    reward               | 0.10130659    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.82          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 838           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033494618 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0179       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.91          |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000821     |\n",
            "|    reward               | -0.44707873   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.14          |\n",
            "-------------------------------------------\n",
            "day: 2435, episode: 55\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1599153.68\n",
            "total_reward: 599153.68\n",
            "total_cost: 163954.82\n",
            "total_trades: 18899\n",
            "Sharpe: 0.481\n",
            "=================================\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 833            |\n",
            "|    iterations           | 4              |\n",
            "|    time_elapsed         | 9              |\n",
            "|    total_timesteps      | 8192           |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000100136764 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -11.4          |\n",
            "|    explained_variance   | -0.0517        |\n",
            "|    learning_rate        | 1e-05          |\n",
            "|    loss                 | 1.28           |\n",
            "|    n_updates            | 30             |\n",
            "|    policy_gradient_loss | -0.000443      |\n",
            "|    reward               | 0.07583927     |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 3.36           |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 832           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00034070702 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0284       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.94          |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00097      |\n",
            "|    reward               | 0.24694672    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.64          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 833          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002702412 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0144      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.33         |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000953    |\n",
            "|    reward               | -0.98420006  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.98         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 836           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3215136e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0493       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.93          |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000275     |\n",
            "|    reward               | 0.26185963    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.27          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 838           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 19            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030952157 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00767       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.64          |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000843     |\n",
            "|    reward               | 0.7659763     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.61          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 838           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027001888 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0378       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.81          |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000879     |\n",
            "|    reward               | 1.1574285     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.76          |\n",
            "-------------------------------------------\n",
            "day: 2435, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1418677.05\n",
            "total_reward: 418677.05\n",
            "total_cost: 161029.77\n",
            "total_trades: 18854\n",
            "Sharpe: 0.436\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 838           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014714748 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0568       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.58          |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -0.000596     |\n",
            "|    reward               | 0.18964602    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.4           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 840          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007066797 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0186      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 1.32         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00143     |\n",
            "|    reward               | 0.1924049    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.56         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 798           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00011878152 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0139       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 2.41          |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00059      |\n",
            "|    reward               | -0.49878684   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.86          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 801           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00075426337 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | -0.0112       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.76          |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -0.00139      |\n",
            "|    reward               | -1.1971291    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 10.1          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 804          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006497584 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00337      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 3.79         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00216     |\n",
            "|    reward               | 0.9169043    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.62         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 807           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 38            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0758518e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -11.4         |\n",
            "|    explained_variance   | 0.00485       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.91          |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000327     |\n",
            "|    reward               | -0.45512477   |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.18          |\n",
            "-------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.025095014449213422\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  64.73402193387349  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2015-2025_no_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 891\n",
            "[INFO] Moved trained_models to 2015-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_no_crypto/\n",
            "[INFO] Moved results to 2015-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_2 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_2,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_2,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_2,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_no_crypto.csv\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
