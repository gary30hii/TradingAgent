{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
        "\n",
        "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
        "\n",
        "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
        "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGunVt8oLCVS"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOzAKQ-SLGX6"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
        "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
        "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPT0ipYE28wL",
        "outputId": "912ac487-d3c8-467f-ef2a-968fa655b206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-djnd0x5g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-djnd0x5g\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-h_6ks5c3/elegantrl_79d86a90c3114ac58c9b7526e0332a2c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-h_6ks5c3/elegantrl_79d86a90c3114ac58c9b7526e0332a2c\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# ## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install pandas_market_calendars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBHhVysOEzi"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('Agg')  \n",
        "\n",
        "# ===========================\n",
        "# Enable Inline Plotting (Jupyter)\n",
        "# ===========================\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Custom Imports (model.py)\n",
        "# ===========================\n",
        "sys.path.append(os.path.abspath(\".\"))  \n",
        "from models import DRLEnsembleAgent\n",
        "\n",
        "sys.path.append(\"../FinRL-Library\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeQ7iS-LoMm"
      },
      "source": [
        "\n",
        "\n",
        "-----\n",
        "class YahooDownloader:\n",
        "    Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_to_features(csv_path):\n",
        "    # Step 1: Load Data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 3: Apply Technical Indicators\n",
        "    fe_ti = FeatureEngineer(\n",
        "        use_technical_indicator=True,\n",
        "        use_turbulence=False,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    df_5day_full = fe_ti.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe_ti.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 4: Combine and Clean Index\n",
        "    combined_df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    combined_df.index = range(len(combined_df))\n",
        "\n",
        "    # Step 5: Remove dates with only one ticker\n",
        "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "    combined_df = combined_df[combined_df.groupby('date')['date'].transform('count') > 1]\n",
        "    combined_df = combined_df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Apply Turbulence Feature\n",
        "    fe_turb = FeatureEngineer(\n",
        "        use_technical_indicator=False,\n",
        "        use_turbulence=True,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    processed = fe_turb.preprocess_data(combined_df)\n",
        "\n",
        "    # Step 7: Final Cleaning\n",
        "    processed = processed.copy()\n",
        "    processed = processed.fillna(0)\n",
        "    processed = processed.replace(np.inf, 0)\n",
        "\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n",
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
        "\n",
        "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_drl_ensemble_agent(processed_df, \n",
        "                              indicators, \n",
        "                              train_start_date, \n",
        "                              train_end_date, \n",
        "                              trade_start_date, \n",
        "                              trade_end_date, \n",
        "                              rebalance_window=63, \n",
        "                              validation_window=63, \n",
        "                              initial_amount=1_000_000,\n",
        "                              transaction_cost=0.001,\n",
        "                              hmax=100,\n",
        "                              reward_scaling=1e-4,\n",
        "                              print_verbosity=5):\n",
        "    \"\"\"\n",
        "    Setup DRLEnsembleAgent with flexible date and parameter configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate dynamic parameters\n",
        "    stock_dimension = len(processed_df.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(indicators) * stock_dimension\n",
        "\n",
        "    # 2. Environment configuration\n",
        "    env_kwargs = {\n",
        "        \"hmax\": hmax,\n",
        "        \"initial_amount\": initial_amount,\n",
        "        \"buy_cost_pct\": transaction_cost,\n",
        "        \"sell_cost_pct\": transaction_cost,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": indicators,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": reward_scaling,\n",
        "        \"print_verbosity\": print_verbosity\n",
        "    }\n",
        "\n",
        "    # 3. Initialize DRLEnsembleAgent\n",
        "    agent = DRLEnsembleAgent(\n",
        "        df=processed_df,\n",
        "        train_period=(train_start_date, train_end_date),\n",
        "        val_test_period=(trade_start_date, trade_end_date),\n",
        "        rebalance_window=rebalance_window,\n",
        "        validation_window=validation_window,\n",
        "        **env_kwargs\n",
        "    )\n",
        "\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n"
          ]
        }
      ],
      "source": [
        "ensemble_agent_0 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_0,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2007-06-01',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "ensemble_agent_1 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_1,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "ensemble_agent_2 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_2,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms.\n",
        "\n",
        "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
        "                      \"buffer_size\": 10_000,\n",
        "                      \"learning_rate\": 0.0005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "                      \"batch_size\": 128,\n",
        "                      \"buffer_size\": 100000,\n",
        "                      \"learning_rate\": 0.0003,\n",
        "                      \"learning_starts\": 100,\n",
        "                      \"ent_coef\": \"auto_0.1\",\n",
        "                    }\n",
        "\n",
        "TD3_model_kwargs = {\n",
        "                      \"batch_size\": 100,\n",
        "                      \"buffer_size\": 1000000,\n",
        "                      \"learning_rate\": 0.001\n",
        "                   }\n",
        "\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 10_000,\n",
        "                 'ppo' : 10_000,\n",
        "                 'ddpg' : 10_000,\n",
        "                  'sac' : 10_000,\n",
        "                 'td3' : 10_000,\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_ensemble_and_generate_daily_return(ensemble_agent, \n",
        "                                            A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, \n",
        "                                            timesteps_dict, \n",
        "                                            processed_df, \n",
        "                                            trade_start_date, trade_end_date, \n",
        "                                            rebalance_window, validation_window, \n",
        "                                            output_csv_name=\"df_daily_return.csv\",\n",
        "                                            initial_fund=1_000_000,\n",
        "                                            original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Runs DRL Ensemble Strategy, tracks continuous portfolio value, \n",
        "    calculates daily returns, saves outputs, and organizes files into a folder.\n",
        "    \"\"\"\n",
        "    from finrl.main import check_and_make_directories\n",
        "    from finrl.config import (\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR,\n",
        "        INDICATORS,\n",
        "    )\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "\n",
        "    # === Step 1: Create Folder Based on CSV Name ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]\n",
        "    target_folder = f\"{base_name}\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "        print(f\"[INFO] Created folder: {target_folder}\")\n",
        "\n",
        "    # === Step 2: Run Ensemble Strategy ===\n",
        "    print(\"[INFO] Running Ensemble Strategy...\")\n",
        "    df_summary = ensemble_agent.run_ensemble_strategy(\n",
        "        A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, timesteps_dict\n",
        "    )\n",
        "\n",
        "    # === Step 3: Prepare Trade Dates ===\n",
        "    unique_trade_date = processed_df[\n",
        "        (processed_df.date >= trade_start_date) & (processed_df.date <= trade_end_date)\n",
        "    ].date.unique()\n",
        "\n",
        "    current_value = initial_fund\n",
        "    portfolio_tracking = []\n",
        "    is_first_file = True\n",
        "\n",
        "    rebalance_points = list(range(rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window))\n",
        "\n",
        "    # === Step 4: Track Portfolio Value Across Rebalances ===\n",
        "    for i in rebalance_points:\n",
        "        file_path = f'results/account_value_trade_ensemble_{i}.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            temp = pd.read_csv(file_path)\n",
        "\n",
        "            if is_first_file:\n",
        "                first_date = temp.loc[0, 'date']\n",
        "                original_value = temp.loc[0, 'account_value']\n",
        "                portfolio_tracking.append({\n",
        "                    'date': first_date,\n",
        "                    'portfolio_value': current_value,\n",
        "                    'original_account_value': original_value\n",
        "                })\n",
        "                start_idx = 1\n",
        "                is_first_file = False\n",
        "            else:\n",
        "                start_idx = 1\n",
        "\n",
        "            for idx in range(start_idx, len(temp)):\n",
        "                daily_return = temp.loc[idx, 'daily_return']\n",
        "                date = temp.loc[idx, 'date']\n",
        "                original_value = temp.loc[idx, 'account_value']\n",
        "                if pd.notna(daily_return):\n",
        "                    current_value *= (1 + daily_return)\n",
        "                    portfolio_tracking.append({\n",
        "                        'date': date,\n",
        "                        'portfolio_value': current_value,\n",
        "                        'original_account_value': original_value\n",
        "                    })\n",
        "        else:\n",
        "            print(f\"[Warning] File does not exist: {file_path}\")\n",
        "\n",
        "    df_portfolio = pd.DataFrame(portfolio_tracking)\n",
        "\n",
        "    # === Step 5: Plot Portfolio Value ===\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['portfolio_value'], label='Continuous Portfolio Value')\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['original_account_value'], label='Original (Resetting) Account Value', linestyle='--')\n",
        "    plt.title('Portfolio Value: Continuous vs Original')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Portfolio Value')\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(target_folder, \"portfolio_value_plot.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"[INFO] Portfolio value plot saved to: {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # === Step 6: Calculate Daily Returns ===\n",
        "    df_daily_return = df_portfolio.copy()\n",
        "    df_daily_return[\"daily_return\"] = df_daily_return[\"portfolio_value\"].pct_change()\n",
        "    df_daily_return = df_daily_return.infer_objects(copy=False)\n",
        "    df_daily_return.loc[0, \"daily_return\"] = 0.0\n",
        "    df_daily_return = df_daily_return[[\"date\", \"daily_return\"]]\n",
        "\n",
        "    # === Step 7: Save Daily Return CSV into Folder ===\n",
        "    csv_full_path = os.path.join(target_folder, output_csv_name)\n",
        "    df_daily_return.to_csv(csv_full_path, index=False)\n",
        "    print(f\"[INFO] Daily return saved to: {csv_full_path}\")\n",
        "\n",
        "    # === Step 8: Move Directories into the Folder ===\n",
        "    dirs_to_move = [TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR]\n",
        "\n",
        "    for dir_path in dirs_to_move:\n",
        "        if os.path.exists(dir_path):\n",
        "            dest_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(dest_path):\n",
        "                shutil.rmtree(dest_path)  # Clean if already exists\n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[Warning] Directory not found: {dir_path}\")\n",
        "\n",
        "    return df_daily_return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 546      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.131    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 16.4     |\n",
            "|    reward             | -0.8996  |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 559         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0.233       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 1.82        |\n",
            "|    reward             | -0.09260846 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 0.661       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 565        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.263      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 1.41       |\n",
            "|    reward             | -0.4666614 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.781      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 567         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.503      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -15         |\n",
            "|    reward             | -0.30650017 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 3.6         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 569       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 4.69      |\n",
            "|    reward             | 0.6790567 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 0.736     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 571        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -18        |\n",
            "|    reward             | 0.17706585 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.24       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 571       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -5.61     |\n",
            "|    reward             | 0.2269444 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.25      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 571       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.218     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -11.9     |\n",
            "|    reward             | -0.536374 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.54      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 572       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.752    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -29.1     |\n",
            "|    reward             | 0.9314057 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 8.13      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 572        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.00544   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 22.7       |\n",
            "|    reward             | -1.6702392 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.56       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 573        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.444     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 12.4       |\n",
            "|    reward             | -0.9318917 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.6        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 573       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0282    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -14.6     |\n",
            "|    reward             | 4.3752804 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 1.78      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 573         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -4.24       |\n",
            "|    reward             | -0.02550442 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 3.99        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 574      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 10.8     |\n",
            "|    reward             | 3.725414 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 6.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 574      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -26.5    |\n",
            "|    reward             | 1.99738  |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 6.15     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 573        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 25.1       |\n",
            "|    reward             | -0.9105748 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 5.83       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 574         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -9.78       |\n",
            "|    reward             | -0.45906943 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.08        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 574        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0278     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -22.5      |\n",
            "|    reward             | -1.7749954 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.96       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 574       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.136    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 10.7      |\n",
            "|    reward             | 0.8790828 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.26      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 574       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -29.1     |\n",
            "|    reward             | -0.018126 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 6.58      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.09703275514780658\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 3925, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2069032.03\n",
            "total_reward: 1069032.03\n",
            "total_cost: 998.98\n",
            "total_trades: 11775\n",
            "Sharpe: 0.479\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.34083553571908376\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.18288951347654236\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 3925, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2585006.45\n",
            "total_reward: 1585006.45\n",
            "total_cost: 8750.69\n",
            "total_trades: 12383\n",
            "Sharpe: 0.441\n",
            "=================================\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.17786289778917627\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 642        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.3513402 |\n",
            "-----------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 616        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0115092  |\n",
            "|    clip_fraction        | 0.0943     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.3      |\n",
            "|    explained_variance   | -0.00952   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.56       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.00504   |\n",
            "|    reward               | -0.5331463 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 5.32       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 617          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075408234 |\n",
            "|    clip_fraction        | 0.06         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00124      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 12.5         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00743     |\n",
            "|    reward               | -0.8075693   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 19.9         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 609         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007100794 |\n",
            "|    clip_fraction        | 0.0593      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0267      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.6         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0099     |\n",
            "|    reward               | -1.9098927  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.51        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 608         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010893717 |\n",
            "|    clip_fraction        | 0.0989      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0141      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.1         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00767    |\n",
            "|    reward               | 1.2508116   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 16.2        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.062032961404262156\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 543         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -2.25       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 14.2        |\n",
            "|    reward             | -0.65742606 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 1.95        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 546          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | -0.00941     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 0.259        |\n",
            "|    reward             | -0.008501386 |\n",
            "|    std                | 0.993        |\n",
            "|    value_loss         | 1.43         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 546         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.153      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 9.29        |\n",
            "|    reward             | -0.68862396 |\n",
            "|    std                | 0.991       |\n",
            "|    value_loss         | 2.67        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 545         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -36.9       |\n",
            "|    reward             | 0.102034375 |\n",
            "|    std                | 0.994       |\n",
            "|    value_loss         | 15.4        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 531      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -33.4    |\n",
            "|    reward             | 1.560022 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 6.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 521      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 17       |\n",
            "|    reward             | 0.201656 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 41.1      |\n",
            "|    reward             | -0.021809 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 13.9      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 526         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -1.52       |\n",
            "|    reward             | -0.12751924 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 0.0414      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 527        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 28.5       |\n",
            "|    reward             | -0.7438204 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 7.45       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 20.5      |\n",
            "|    reward             | -1.721972 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 4.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -54.5     |\n",
            "|    reward             | 0.7718826 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 25.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 2.7       |\n",
            "|    reward             | -0.309456 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 2.49      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 526      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -4.95    |\n",
            "|    reward             | 0.622816 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 0.461    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 3.88      |\n",
            "|    reward             | -0.329137 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 3.19      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -41.5     |\n",
            "|    reward             | -4.762806 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 15.7      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 528          |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | -10.3        |\n",
            "|    reward             | -0.016845815 |\n",
            "|    std                | 0.991        |\n",
            "|    value_loss         | 0.669        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 529        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | -0.279     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -6.26      |\n",
            "|    reward             | 0.23865302 |\n",
            "|    std                | 0.988      |\n",
            "|    value_loss         | 2.69       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 7.38      |\n",
            "|    reward             | 1.9951576 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 1.75      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -8.95     |\n",
            "|    reward             | 1.2307025 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.68      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 530       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.7     |\n",
            "|    reward             | -0.640685 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 1.03      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.015366889844768325\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "day: 3988, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2027760.41\n",
            "total_reward: 1027760.41\n",
            "total_cost: 1227.13\n",
            "total_trades: 15902\n",
            "Sharpe: 0.307\n",
            "=================================\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.16280730949440644\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.05765951633305641\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "day: 3988, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1602726.86\n",
            "total_reward: 602726.86\n",
            "total_cost: 5362.89\n",
            "total_trades: 12461\n",
            "Sharpe: 0.272\n",
            "=================================\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.1901084340817186\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 616         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.22305275 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 599         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007344513 |\n",
            "|    clip_fraction        | 0.08        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.016      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.02        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00656    |\n",
            "|    reward               | -1.2532485  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.26        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010315418 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0045     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.87        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00747    |\n",
            "|    reward               | 0.23599254  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 8           |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 589         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009310203 |\n",
            "|    clip_fraction        | 0.0972      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0494     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.884       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    reward               | -0.11348463 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 2.7         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 586         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008514726 |\n",
            "|    clip_fraction        | 0.0594      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00316    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.37        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00784    |\n",
            "|    reward               | 0.21981403  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 6.15        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.1165817077628184\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 488        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0436    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 19.6       |\n",
            "|    reward             | -1.0242496 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.49       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 504         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 1.08        |\n",
            "|    reward             | -0.11028306 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.853       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 501         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | -0.19       |\n",
            "|    reward             | -0.37921488 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.157       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 508        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -5.2       |\n",
            "|    reward             | 0.16498572 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.834      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 504      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.146    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 1.62     |\n",
            "|    reward             | 0.407824 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.0873   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 509        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0117    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -12.7      |\n",
            "|    reward             | 0.11158043 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.09       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 512        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.134     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -2.96      |\n",
            "|    reward             | 0.10313675 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.574      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 511        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 4.17e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -2.93      |\n",
            "|    reward             | -0.6125517 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.16       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 513       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.00153   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -8.57     |\n",
            "|    reward             | 2.4517782 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.3       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 513         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.302      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 26.7        |\n",
            "|    reward             | -0.45867273 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 6.6         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 510       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.257    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -35.9     |\n",
            "|    reward             | 1.1663587 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 9.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 509        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -40.9      |\n",
            "|    reward             | -2.2211878 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 12.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 509       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 18.8      |\n",
            "|    reward             | 2.5583832 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.32      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 511       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0871    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 11.1      |\n",
            "|    reward             | 1.4126133 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.84      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 510       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.00072   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 28.8      |\n",
            "|    reward             | 1.0492748 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 509        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 4.21       |\n",
            "|    reward             | -1.6434317 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 3.98       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 510       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0849   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 44.8      |\n",
            "|    reward             | -0.800358 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 18.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 511        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -1.62      |\n",
            "|    reward             | 0.65923744 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.199      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 513          |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.6        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | 6.74         |\n",
            "|    reward             | -0.117084846 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 0.532        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 514        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 5.02       |\n",
            "|    reward             | -1.5024729 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.354      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3741411451901609\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "day: 4051, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1344451.63\n",
            "total_reward: 344451.63\n",
            "total_cost: 999.00\n",
            "total_trades: 20205\n",
            "Sharpe: 0.282\n",
            "=================================\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.5071587983599345\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.140076203662405\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "day: 4051, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1284145.64\n",
            "total_reward: 284145.64\n",
            "total_cost: 5824.13\n",
            "total_trades: 23761\n",
            "Sharpe: 0.179\n",
            "=================================\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.48354200473265413\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 601         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.16684674 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 583         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006014194 |\n",
            "|    clip_fraction        | 0.0559      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00442    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.13        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00691    |\n",
            "|    reward               | -0.3914169  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.37        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 585        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00776191 |\n",
            "|    clip_fraction        | 0.0661     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.0144    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 5.53       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.00602   |\n",
            "|    reward               | -1.6588538 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 11.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 547         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009539857 |\n",
            "|    clip_fraction        | 0.0586      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0296      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.49        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00536    |\n",
            "|    reward               | 0.17727026  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.92        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 555         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006891502 |\n",
            "|    clip_fraction        | 0.0647      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00491    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.57        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00763    |\n",
            "|    reward               | 0.01849503  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.4578474708161286\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 532        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 17.4       |\n",
            "|    reward             | -1.4399533 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 5.43       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 9.15       |\n",
            "|    reward             | 0.23550482 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 2.35       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 525        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 14.3       |\n",
            "|    reward             | -0.6573591 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 4.59       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 524         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -32.5       |\n",
            "|    reward             | -0.28636935 |\n",
            "|    std                | 0.99        |\n",
            "|    value_loss         | 12          |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -6.15     |\n",
            "|    reward             | 1.928358  |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 3.55      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 2.37      |\n",
            "|    reward             | 0.6815147 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 3.23      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -74.7     |\n",
            "|    reward             | 0.2685055 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 49.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 528         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | -0.303      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 5.44        |\n",
            "|    reward             | -0.06404068 |\n",
            "|    std                | 0.985       |\n",
            "|    value_loss         | 6.78        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 526        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | -0.00421   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -10.9      |\n",
            "|    reward             | -1.6375833 |\n",
            "|    std                | 0.986      |\n",
            "|    value_loss         | 7.42       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | -0.186     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -10.2      |\n",
            "|    reward             | 0.45499244 |\n",
            "|    std                | 0.986      |\n",
            "|    value_loss         | 1.69       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 520        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | -0.336     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 14.5       |\n",
            "|    reward             | 0.54630196 |\n",
            "|    std                | 0.985      |\n",
            "|    value_loss         | 3.28       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -4.23     |\n",
            "|    reward             | -0.206242 |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 0.186     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 521      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -5.94    |\n",
            "|    reward             | 0.333622 |\n",
            "|    std                | 0.983    |\n",
            "|    value_loss         | 2.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 520      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 25.6     |\n",
            "|    reward             | 0.713071 |\n",
            "|    std                | 0.979    |\n",
            "|    value_loss         | 7.12     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -1.72     |\n",
            "|    reward             | 1.3134371 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 3.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 0.000743   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -4.2       |\n",
            "|    reward             | 0.50251186 |\n",
            "|    std                | 0.977      |\n",
            "|    value_loss         | 1.57       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 515       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 9.3e-06   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -2.72     |\n",
            "|    reward             | -2.486577 |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 0.335     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 513        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 0.358      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 26.9       |\n",
            "|    reward             | -1.2689897 |\n",
            "|    std                | 0.98       |\n",
            "|    value_loss         | 6.96       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 514       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | -0.0478   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -7.27     |\n",
            "|    reward             | 0.9387228 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 1.74      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 514         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 19          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | -9.2        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 12.4        |\n",
            "|    reward             | -0.90909386 |\n",
            "|    std                | 0.977       |\n",
            "|    value_loss         | 1.39        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.32915153875470493\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 4114, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1617689.17\n",
            "total_reward: 617689.17\n",
            "total_cost: 999.00\n",
            "total_trades: 12283\n",
            "Sharpe: 0.314\n",
            "=================================\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3947335357265396\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3522043272770913\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "day: 4114, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1491375.81\n",
            "total_reward: 491375.81\n",
            "total_cost: 5713.46\n",
            "total_trades: 13312\n",
            "Sharpe: 0.337\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.46932966911558316\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 589         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.26320586 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 589         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007604941 |\n",
            "|    clip_fraction        | 0.0738      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00273    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.926       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00767    |\n",
            "|    reward               | -0.1270486  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.19        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 588          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070006913 |\n",
            "|    clip_fraction        | 0.0405       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0134      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 7.91         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00683     |\n",
            "|    reward               | -0.12003653  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 14           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 585          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067963973 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0067       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.56         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00694     |\n",
            "|    reward               | 0.112615086  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 6.02         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 586         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008519396 |\n",
            "|    clip_fraction        | 0.0577      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00133    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00843    |\n",
            "|    reward               | 0.53515404  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 21.3        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.2823063321682283\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 1.56       |\n",
            "|    reward             | -1.1351397 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.507      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 502       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 2.29      |\n",
            "|    reward             | 0.2932047 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.262     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 462        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 2.91       |\n",
            "|    reward             | -0.6893961 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.853      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 432         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -24.4       |\n",
            "|    reward             | -0.14994833 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 7.46        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 451      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0537  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -10.5    |\n",
            "|    reward             | 1.564638 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 457       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.142    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -1.36     |\n",
            "|    reward             | 0.5531936 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.41      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 468        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0314    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -64.5      |\n",
            "|    reward             | 0.19510643 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 29.3       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 477        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.218      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -0.898     |\n",
            "|    reward             | 0.15514421 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.8        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 482        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.066     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 4.73       |\n",
            "|    reward             | 0.87594724 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.15       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 488      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.355    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 18.1     |\n",
            "|    reward             | 1.341467 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 493       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.11      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 7.15      |\n",
            "|    reward             | 0.4652926 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.09      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 497        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 26.3       |\n",
            "|    reward             | 0.90525043 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 7          |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 501         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -3.43       |\n",
            "|    reward             | -0.25962716 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.245       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 503        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.00911   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 5.76       |\n",
            "|    reward             | -1.5990493 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.04       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 506       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 27.8      |\n",
            "|    reward             | -1.134013 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.17      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 509      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 104      |\n",
            "|    reward             | 1.69425  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 71.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 510        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.187     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -24.3      |\n",
            "|    reward             | -0.5006164 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 8.85       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 512       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0854   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 2.27      |\n",
            "|    reward             | 0.5643158 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.286     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 514        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0569     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 19.8       |\n",
            "|    reward             | -1.6636592 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.88       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 516       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.817    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -12.1     |\n",
            "|    reward             | -0.523211 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.77      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.11347702284244594\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "day: 4177, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2330402.63\n",
            "total_reward: 1330402.63\n",
            "total_cost: 999.00\n",
            "total_trades: 20885\n",
            "Sharpe: 0.451\n",
            "=================================\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.489315457529445\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.4726266111421579\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "day: 4177, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1624248.41\n",
            "total_reward: 624248.41\n",
            "total_cost: 5558.72\n",
            "total_trades: 17249\n",
            "Sharpe: 0.243\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.225734368544709\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 632         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.14165683 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 617         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007889681 |\n",
            "|    clip_fraction        | 0.0669      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.014      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.988       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0077     |\n",
            "|    reward               | 0.004090973 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.2         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 612         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006102161 |\n",
            "|    clip_fraction        | 0.0571      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.012       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.63        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0065     |\n",
            "|    reward               | -0.45697898 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 5.6         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 610          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053136926 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.028        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.946        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00429     |\n",
            "|    reward               | -0.73208916  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 2.4          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 608         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006801123 |\n",
            "|    clip_fraction        | 0.0479      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0189      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.72        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00662    |\n",
            "|    reward               | 0.85469395  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 8.89        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.2549006631514094\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.446     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 6.85       |\n",
            "|    reward             | -1.1887915 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.78       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.212      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 4.29       |\n",
            "|    reward             | 0.17989948 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.578      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 540         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.117       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 1.16        |\n",
            "|    reward             | -0.48929924 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.187       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0305     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -13        |\n",
            "|    reward             | 0.37803563 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.46       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 541        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0413     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -9.56      |\n",
            "|    reward             | 0.64841866 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.01       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 541         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.185       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 3.7         |\n",
            "|    reward             | -0.11653122 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.244       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 541         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.17        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 6.06        |\n",
            "|    reward             | -0.13750137 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.2         |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 541         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.00566    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -10.7       |\n",
            "|    reward             | -0.03624401 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.12        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0771    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -6.2       |\n",
            "|    reward             | -1.0959487 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.469      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 541        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0322     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 14.9       |\n",
            "|    reward             | -0.5676083 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.71       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 541        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 9.54e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -1.21      |\n",
            "|    reward             | -1.2090751 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.839      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.77      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -7.96      |\n",
            "|    reward             | -0.3656122 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.848      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.00436    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -14.3      |\n",
            "|    reward             | 0.92653465 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.95       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 20.6       |\n",
            "|    reward             | 0.26222995 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 3.27       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 540      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | -0.0547  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -21.7    |\n",
            "|    reward             | -4.31418 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.152      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -14.8      |\n",
            "|    reward             | -3.1417215 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.65       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -4.24      |\n",
            "|    reward             | 0.43372634 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.173      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -26.8      |\n",
            "|    reward             | -2.0188048 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.31       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0644     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -4.13      |\n",
            "|    reward             | -0.4680922 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.218      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 539          |\n",
            "|    iterations         | 2000         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 10000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0.00782      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1999         |\n",
            "|    policy_loss        | -12          |\n",
            "|    reward             | 0.0064445105 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 1.2          |\n",
            "----------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.053637283365196686\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "day: 4240, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1924595.83\n",
            "total_reward: 924595.83\n",
            "total_cost: 999.00\n",
            "total_trades: 8480\n",
            "Sharpe: 0.411\n",
            "=================================\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.04448473959080572\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.3541335216957744\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "day: 4240, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2331948.82\n",
            "total_reward: 1331948.82\n",
            "total_cost: 4557.11\n",
            "total_trades: 13227\n",
            "Sharpe: 0.423\n",
            "=================================\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.5443529024738012\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 601        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.3485609 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 596          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077667567 |\n",
            "|    clip_fraction        | 0.0955       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0875      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.19         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0118      |\n",
            "|    reward               | -0.12991616  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.18         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 595        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00911145 |\n",
            "|    clip_fraction        | 0.0918     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.00537   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.85       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0111    |\n",
            "|    reward               | 0.558337   |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 7.33       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 591         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008804485 |\n",
            "|    clip_fraction        | 0.0613      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0254     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.36        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    reward               | -2.2551832  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.27        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 591          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061958376 |\n",
            "|    clip_fraction        | 0.0214       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00649      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 19.6         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00471     |\n",
            "|    reward               | 0.68966097   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 28.2         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.4185930776040992\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0541    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 8.3        |\n",
            "|    reward             | -0.6584598 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.766      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 5.45       |\n",
            "|    reward             | 0.12080806 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.81       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.5       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 0.775      |\n",
            "|    reward             | -0.7433666 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.1        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 531         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0064      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -14.3       |\n",
            "|    reward             | -0.20413984 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.82        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 532      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -1.09    |\n",
            "|    reward             | 1.385228 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -4.1       |\n",
            "|    reward             | 0.53488666 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.58       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 534       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -44.8     |\n",
            "|    reward             | -0.041145 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 12.3      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 533          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | 4.84         |\n",
            "|    reward             | -0.109725036 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 3.45         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 1.47e-05   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -13.2      |\n",
            "|    reward             | -0.8560831 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.19       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 14.4      |\n",
            "|    reward             | 0.0979944 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.19      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 533         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -5.48       |\n",
            "|    reward             | -0.61876124 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.497       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 14.5       |\n",
            "|    reward             | 0.40661368 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.67       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 9.65     |\n",
            "|    reward             | 2.436588 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 3.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -49.5    |\n",
            "|    reward             | 3.261676 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 45.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 24.4      |\n",
            "|    reward             | 1.67596   |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.16      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 534      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -99.4    |\n",
            "|    reward             | -5.93032 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 71.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 534      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 18.6     |\n",
            "|    reward             | 3.777356 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.59     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0357    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -6.23      |\n",
            "|    reward             | -1.1678683 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.16       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 534          |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 17           |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | 10.5         |\n",
            "|    reward             | -0.026017003 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 2.72         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 534       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -2.52     |\n",
            "|    reward             | 0.403062  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.213     |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.19453690871634838\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "day: 4303, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2288112.77\n",
            "total_reward: 1288112.77\n",
            "total_cost: 1407.27\n",
            "total_trades: 17258\n",
            "Sharpe: 0.327\n",
            "=================================\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.16073219746549752\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.3370970201683676\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "day: 4303, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1845154.66\n",
            "total_reward: 845154.66\n",
            "total_cost: 4396.24\n",
            "total_trades: 13822\n",
            "Sharpe: 0.403\n",
            "=================================\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.465710413108216\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 595         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.31826234 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 588         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008805653 |\n",
            "|    clip_fraction        | 0.0851      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0234     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.46        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -0.13892472 |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 4.71        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 586         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006668494 |\n",
            "|    clip_fraction        | 0.0585      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.00962     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.5         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00555    |\n",
            "|    reward               | -1.2866633  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 12.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 582         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008374657 |\n",
            "|    clip_fraction        | 0.0756      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0924     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.51        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00862    |\n",
            "|    reward               | 0.8031778   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.31        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 581         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008366593 |\n",
            "|    clip_fraction        | 0.0637      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0128     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00819    |\n",
            "|    reward               | 1.4644717   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.24208573591689125\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.282     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 12.6      |\n",
            "|    reward             | -1.093175 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 1.85      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 517        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 7.24       |\n",
            "|    reward             | 0.20520699 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.54       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 12         |\n",
            "|    reward             | -1.0015098 |\n",
            "|    std                | 0.99       |\n",
            "|    value_loss         | 2.47       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -19.3     |\n",
            "|    reward             | -0.468935 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 4.7       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 521      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -8.16    |\n",
            "|    reward             | 1.632301 |\n",
            "|    std                | 0.993    |\n",
            "|    value_loss         | 1.57     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -0.0331    |\n",
            "|    reward             | 0.55910265 |\n",
            "|    std                | 0.99       |\n",
            "|    value_loss         | 1.82       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -28.6     |\n",
            "|    reward             | -0.043768 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 9.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -0.479    |\n",
            "|    reward             | -0.099463 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 2.77      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0.000752  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 19.4      |\n",
            "|    reward             | 1.4550606 |\n",
            "|    std                | 0.982     |\n",
            "|    value_loss         | 7.11      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -2.17      |\n",
            "|    reward             | -0.7437893 |\n",
            "|    std                | 0.983      |\n",
            "|    value_loss         | 0.534      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -18       |\n",
            "|    reward             | 2.0384808 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 4.23      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 12.6      |\n",
            "|    reward             | -0.323088 |\n",
            "|    std                | 0.984     |\n",
            "|    value_loss         | 1.24      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -15      |\n",
            "|    reward             | 1.091648 |\n",
            "|    std                | 0.985    |\n",
            "|    value_loss         | 8.6      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 12.3      |\n",
            "|    reward             | 0.5469955 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 1.18      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.0753  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 2.34     |\n",
            "|    reward             | -0.14916 |\n",
            "|    std                | 0.989    |\n",
            "|    value_loss         | 0.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -11.7    |\n",
            "|    reward             | 0.699643 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 524      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -19.7    |\n",
            "|    reward             | 2.01061  |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 3.61       |\n",
            "|    reward             | -1.9191186 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 1.56       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 524         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -11.1       |\n",
            "|    reward             | -0.49656025 |\n",
            "|    std                | 0.991       |\n",
            "|    value_loss         | 6.01        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.0282   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -35.4     |\n",
            "|    reward             | 1.2064158 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 10.9      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.2294398132668453\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "day: 4366, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3281667.04\n",
            "total_reward: 2281667.04\n",
            "total_cost: 999.00\n",
            "total_trades: 26138\n",
            "Sharpe: 0.505\n",
            "=================================\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.05582287026151492\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.06171818580335023\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "day: 4366, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1392081.40\n",
            "total_reward: 392081.40\n",
            "total_cost: 5216.26\n",
            "total_trades: 13636\n",
            "Sharpe: 0.248\n",
            "=================================\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.032992002251174705\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 603         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.10804996 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004280756 |\n",
            "|    clip_fraction        | 0.0516      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0495      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.33        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00588    |\n",
            "|    reward               | -0.09990869 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 582         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006905999 |\n",
            "|    clip_fraction        | 0.0678      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.111      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.23        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00944    |\n",
            "|    reward               | 0.03308014  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.54        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 581         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009061714 |\n",
            "|    clip_fraction        | 0.0849      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.000924   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.32        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00979    |\n",
            "|    reward               | 1.1732646   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 6.07        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 577         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007773604 |\n",
            "|    clip_fraction        | 0.0821      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.000944   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.76        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00926    |\n",
            "|    reward               | -0.5915507  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.66        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.07226634322327394\n",
            "======Best Model Retraining from:  2007-06-01 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  25.916514817873637  minutes\n",
            "[INFO] Portfolio value plot saved to: 2007-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2007-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2007-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2007-2025_no_crypto/\n",
            "[INFO] Moved results to 2007-2025_no_crypto/\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 762         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -11.1       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -13.6       |\n",
            "|    reward             | -0.29320446 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 1.11        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 765        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.00214   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -2.09      |\n",
            "|    reward             | 0.07016074 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 0.188      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.016     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 9.34      |\n",
            "|    reward             | 1.1284808 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 2.49      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 770          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.6        |\n",
            "|    explained_variance | 0.0117       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -56.7        |\n",
            "|    reward             | -0.047660425 |\n",
            "|    std                | 0.987        |\n",
            "|    value_loss         | 22.6         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 772        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -1.65      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -0.0107    |\n",
            "|    reward             | 0.07055729 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 0.166      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 774      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0276  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 1.68     |\n",
            "|    reward             | 1.014668 |\n",
            "|    std                | 0.988    |\n",
            "|    value_loss         | 0.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 776      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.12     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 16.4     |\n",
            "|    reward             | 0.861145 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 775          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.7        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -2.26        |\n",
            "|    reward             | -0.002978453 |\n",
            "|    std                | 0.99         |\n",
            "|    value_loss         | 0.042        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 776        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.811      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -0.962     |\n",
            "|    reward             | 0.15849401 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 0.0156     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 777        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 24         |\n",
            "|    reward             | -0.4733041 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.44       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 778       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.164     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 18.1      |\n",
            "|    reward             | 2.9993348 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 3         |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 777         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 1.62        |\n",
            "|    reward             | 0.106679566 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 0.0291      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 778      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 1.74     |\n",
            "|    reward             | 0.170932 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 0.0713   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 778       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 5.14      |\n",
            "|    reward             | 0.4349453 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 0.398     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 1.02       |\n",
            "|    reward             | 0.82375854 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 2.53       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 779         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 1.09        |\n",
            "|    reward             | -0.13959177 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 0.0111      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 779       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.589     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 0.847     |\n",
            "|    reward             | 0.3529669 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.0206    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 779         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 6.55        |\n",
            "|    reward             | -0.10434334 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 0.226       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 779         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.592      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 14.8        |\n",
            "|    reward             | -0.86338663 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.8         |\n",
            "---------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1031648.17\n",
            "total_reward: 31648.17\n",
            "total_cost: 32945.62\n",
            "total_trades: 11126\n",
            "Sharpe: 0.116\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -2.16      |\n",
            "|    reward             | -0.6513948 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0336     |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.22844382177065484\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1155597.88\n",
            "total_reward: 155597.88\n",
            "total_cost: 998.99\n",
            "total_trades: 7976\n",
            "Sharpe: 0.246\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 28       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 233      |\n",
            "|    critic_loss     | 320      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 0.581226 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2324987661533825\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 22506270.21\n",
            "total_reward: 21506270.21\n",
            "total_cost: 998.97\n",
            "total_trades: 11964\n",
            "Sharpe: 0.941\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 262      |\n",
            "|    time_elapsed    | 30       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 525      |\n",
            "|    critic_loss     | 3.5e+04  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 9.145572 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.4437344408344165\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 27641782.15\n",
            "total_reward: 26641782.15\n",
            "total_cost: 43555.77\n",
            "total_trades: 14017\n",
            "Sharpe: 0.990\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 40        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.63e+03  |\n",
            "|    critic_loss     | 9.3e+03   |\n",
            "|    ent_coef        | 0.0571    |\n",
            "|    ent_coef_loss   | -13.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | 13.102817 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.43572982897293455\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 16531367.80\n",
            "total_reward: 15531367.80\n",
            "total_cost: 1600073.09\n",
            "total_trades: 17410\n",
            "Sharpe: 1.098\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 934       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.1643554 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 927         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011426499 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000309    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.19e+03    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    reward               | 0.8043883   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.48e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 922         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009186547 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00701     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 114         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | -0.4909058  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 206         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0087838415 |\n",
            "|    clip_fraction        | 0.0727       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00104     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.16e+03     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00659     |\n",
            "|    reward               | -1.0110922   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.87e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 920         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013455127 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00566     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 88          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00902    |\n",
            "|    reward               | 0.88598335  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 144         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.378337656550761\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.186     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 46.7       |\n",
            "|    reward             | 0.88355064 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 27.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 778        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -39.1      |\n",
            "|    reward             | -0.9634505 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 26.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 780        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.0654     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 76.9       |\n",
            "|    reward             | -3.2295303 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 62         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 779       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0413    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 122       |\n",
            "|    reward             | 4.0440025 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 108       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.343      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -12        |\n",
            "|    reward             | -1.4067669 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 0.922      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.0435     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -13.7      |\n",
            "|    reward             | -1.5470284 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 1.86       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 782      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.213   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -4.04    |\n",
            "|    reward             | 0.564313 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 783       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -59.7     |\n",
            "|    reward             | 1.3842535 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 24.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 783        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.842     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -20        |\n",
            "|    reward             | -0.2680113 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 3.72       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 783       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.773    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -25.4     |\n",
            "|    reward             | 0.4629826 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 4.09      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.0517   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -16.2     |\n",
            "|    reward             | 0.9521965 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 2.39      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 785      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0151  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 57.9     |\n",
            "|    reward             | 3.904731 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 23.6     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 784         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.853       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -3.66       |\n",
            "|    reward             | -0.37899527 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.0982      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 785         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -0.00909    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 24.4        |\n",
            "|    reward             | -0.32277194 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 4.32        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.464    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -17.2     |\n",
            "|    reward             | 2.6909893 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 12.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 785        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -118       |\n",
            "|    reward             | 0.63193405 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 94.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.955    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 26.7      |\n",
            "|    reward             | 0.9749289 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 9.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 786       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0317    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 9.38      |\n",
            "|    reward             | 1.2358057 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 2.39      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 786       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.867    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 28.7      |\n",
            "|    reward             | 0.5404906 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 11.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 786       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.00203  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 71.8      |\n",
            "|    reward             | 1.5872576 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 47.6      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.028755809800748183\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 294       |\n",
            "|    time_elapsed    | 27        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.55e+03 |\n",
            "|    critic_loss     | 4.19e+04  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | 1.136553  |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.19578369960800038\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 261      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 350      |\n",
            "|    critic_loss     | 837      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.150397 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.06384622418913473\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 202       |\n",
            "|    time_elapsed    | 40        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.46e+03  |\n",
            "|    critic_loss     | 1.07e+05  |\n",
            "|    ent_coef        | 0.0654    |\n",
            "|    ent_coef_loss   | -22.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | 125.51847 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.09503333804804244\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    fps             | 667      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "| train/             |          |\n",
            "|    reward          | -4.73824 |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 766         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008499883 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00266    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.8        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    reward               | -0.7527909  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 104         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 807         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009492405 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00509     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 80.8        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | -1.1435249  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 123         |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 829       |\n",
            "|    iterations           | 4         |\n",
            "|    time_elapsed         | 9         |\n",
            "|    total_timesteps      | 8192      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0081924 |\n",
            "|    clip_fraction        | 0.075     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -12.8     |\n",
            "|    explained_variance   | 0.00531   |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 104       |\n",
            "|    n_updates            | 30        |\n",
            "|    policy_gradient_loss | -0.00684  |\n",
            "|    reward               | 65.01446  |\n",
            "|    std                  | 1.01      |\n",
            "|    value_loss           | 184       |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 843          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021687918 |\n",
            "|    clip_fraction        | 0.000781     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.9        |\n",
            "|    explained_variance   | 6.94e-05     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.44e+04     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00382     |\n",
            "|    reward               | -44.27122    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 9.24e+04     |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.22058612104283007\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 759        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 3.58       |\n",
            "|    reward             | 0.45932084 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.3        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 767        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 15.6       |\n",
            "|    reward             | 0.67055136 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.12       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 766       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.136    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 107       |\n",
            "|    reward             | -3.600007 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 106       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 769      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.0614  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 63.2     |\n",
            "|    reward             | 3.738893 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 26.9     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 768        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 0.367      |\n",
            "|    reward             | 0.62580967 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0464     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 769        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.301      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 0.147      |\n",
            "|    reward             | -0.8968652 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.135      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 26.6       |\n",
            "|    reward             | -1.4711998 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 8.78       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 772       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -22.5     |\n",
            "|    reward             | 1.7499254 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.52      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 772        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.139      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 9.23       |\n",
            "|    reward             | 0.08559752 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.87       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 773        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.407     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -6.18      |\n",
            "|    reward             | -4.2673993 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.459      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 773       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -31.7     |\n",
            "|    reward             | -2.254844 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.55      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 774         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -30.6       |\n",
            "|    reward             | -0.42439997 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 7.97        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 774        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -1.03e-05  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -8.03      |\n",
            "|    reward             | -0.6066425 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.623      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.268    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -0.62     |\n",
            "|    reward             | 0.3016772 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.0811    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0346    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -40.1     |\n",
            "|    reward             | 1.5940233 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 11.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 775      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.00255  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -84.7    |\n",
            "|    reward             | 0.347776 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 40.3     |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 775          |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | 0.115        |\n",
            "|    reward             | -0.020499302 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.00553      |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 775       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 10.4      |\n",
            "|    reward             | -0.190147 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.708     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 775        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -19.5      |\n",
            "|    reward             | 0.18969661 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.89       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 770      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 31.3     |\n",
            "|    reward             | 1.494378 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 8.7      |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.4899252496018759\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 293      |\n",
            "|    time_elapsed    | 28       |\n",
            "|    total_timesteps | 8484     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -405     |\n",
            "|    critic_loss     | 9.95e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8383     |\n",
            "|    reward          | -74.1055 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.1409099611305788\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 250       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 198       |\n",
            "|    critic_loss     | 2.92e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -84.84333 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.1394609148312305\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 63         |\n",
            "|    time_elapsed    | 133        |\n",
            "|    total_timesteps | 8484       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 789        |\n",
            "|    critic_loss     | 1.8e+04    |\n",
            "|    ent_coef        | 0.0764     |\n",
            "|    ent_coef_loss   | -33.5      |\n",
            "|    learning_rate   | 0.0003     |\n",
            "|    n_updates       | 8383       |\n",
            "|    reward          | -57.923084 |\n",
            "-----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.1354774894118911\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 862        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -3.2321491 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 853          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075933198 |\n",
            "|    clip_fraction        | 0.0801       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00637      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 49.6         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00826     |\n",
            "|    reward               | -6.5986176   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 107          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 848         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009749532 |\n",
            "|    clip_fraction        | 0.0873      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00279    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.85e+03    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00787    |\n",
            "|    reward               | -0.85846    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.19e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 848         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006153981 |\n",
            "|    clip_fraction        | 0.0356      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000779    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.06e+03    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00428    |\n",
            "|    reward               | -1.1046414  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.55e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 839        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 12         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00652091 |\n",
            "|    clip_fraction        | 0.0353     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.8      |\n",
            "|    explained_variance   | 0.00249    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 429        |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.00484   |\n",
            "|    reward               | -1.2518269 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 1.26e+03   |\n",
            "----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.36227016311407373\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -3.14      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -5.4       |\n",
            "|    reward             | -0.3001608 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.258      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 733         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 15.2        |\n",
            "|    reward             | -0.08966013 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 1.77        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.14      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 28.3       |\n",
            "|    reward             | -0.3288712 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 8.2        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.0383   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 29.1     |\n",
            "|    reward             | 3.974926 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 4.63     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.26      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -3         |\n",
            "|    reward             | 0.58169425 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 0.641      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 736        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.253      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -2.09      |\n",
            "|    reward             | 0.43781945 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 0.256      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.0136    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -3.27      |\n",
            "|    reward             | 0.43981302 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 5.19       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 735       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.279     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -30.2     |\n",
            "|    reward             | 2.2826462 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 5.77      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0445    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -5.55     |\n",
            "|    reward             | 1.9084759 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 2.07      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0164   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 475       |\n",
            "|    reward             | 19.481667 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 2.15e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 732      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.00656 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -976     |\n",
            "|    reward             | 83.75473 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 1.73e+04 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -6.34e+03 |\n",
            "|    reward             | 472.88406 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.52e+05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 733       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 198       |\n",
            "|    reward             | -2.232983 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 379       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 732      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.564   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -30.2    |\n",
            "|    reward             | 0.73022  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 5.62     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 732        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0307     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -36.8      |\n",
            "|    reward             | 0.64895105 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 11.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 733        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.0602     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 9.28       |\n",
            "|    reward             | -1.5735278 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 1.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 734       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0256    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 22.4      |\n",
            "|    reward             | -1.083798 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 6.94      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 734         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.179       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -23.1       |\n",
            "|    reward             | -0.05159889 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 3.77        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 734      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0619  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -50.6    |\n",
            "|    reward             | 0.314373 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 28.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 734      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.00673 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -8.55    |\n",
            "|    reward             | 0.871307 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.847    |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.48081263860802465\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -782     |\n",
            "|    critic_loss     | 2.22e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.42434  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.43043642136085747\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 246      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.7      |\n",
            "|    critic_loss     | 11.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.524969 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.4421061880654582\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 189      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.35e+03 |\n",
            "|    critic_loss     | 1.92e+03 |\n",
            "|    ent_coef        | 1.27     |\n",
            "|    ent_coef_loss   | -25.2    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.42434  |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.43482913654019784\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    fps             | 884      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "| train/             |          |\n",
            "|    reward          | -4.51842 |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 857        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 4          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00924096 |\n",
            "|    clip_fraction        | 0.0938     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.8      |\n",
            "|    explained_variance   | -0.000282  |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 686        |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.00719   |\n",
            "|    reward               | -13.181597 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 1.38e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 835         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008270556 |\n",
            "|    clip_fraction        | 0.0878      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00232    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 236         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00762    |\n",
            "|    reward               | -4.53954    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 436         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 834         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007849754 |\n",
            "|    clip_fraction        | 0.0727      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00033     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 587         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0092     |\n",
            "|    reward               | 34.52677    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.02e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008064812 |\n",
            "|    clip_fraction        | 0.0312      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000256    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.3e+03     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00591    |\n",
            "|    reward               | 329.53308   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.62e+03    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.31977134121570905\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 716       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0231   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 203       |\n",
            "|    reward             | 5.8966017 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 389       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -643       |\n",
            "|    reward             | -7.1920166 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.64e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.00255  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 2.04e+04  |\n",
            "|    reward             | -632.7801 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.79e+06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.183    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 852       |\n",
            "|    reward             | 139.75127 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.11e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 719        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.54      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -8.24      |\n",
            "|    reward             | 0.40621245 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.05       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.407     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 11.9      |\n",
            "|    reward             | -1.417712 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.666     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 32.9      |\n",
            "|    reward             | -0.521829 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 8.88      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -49.4      |\n",
            "|    reward             | -1.8249142 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 23.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 720        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.0207     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -34.9      |\n",
            "|    reward             | 0.03624278 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 10.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.255     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -9.63     |\n",
            "|    reward             | 0.5957075 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.58      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.594    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 2.29      |\n",
            "|    reward             | 1.0789241 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.189     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 24        |\n",
            "|    reward             | 0.8052709 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.24      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 62        |\n",
            "|    reward             | 1.5203512 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 30.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 3.71      |\n",
            "|    reward             | -1.128618 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.885     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.255     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -24.1     |\n",
            "|    reward             | 0.4899755 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.15      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 720      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -10.2    |\n",
            "|    reward             | 0.515635 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.811    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -53.6     |\n",
            "|    reward             | -1.944893 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 22.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 720        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -2.53      |\n",
            "|    reward             | 0.14190397 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0333     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.251     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 7.61      |\n",
            "|    reward             | 0.1567713 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.615     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 23.2      |\n",
            "|    reward             | 0.5199837 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.4       |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.3889921784016777\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 261       |\n",
            "|    time_elapsed    | 34        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 126       |\n",
            "|    critic_loss     | 31.6      |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.250924 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.42580677275937184\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 245       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 469       |\n",
            "|    critic_loss     | 1.97e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.245131 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.3814024319630318\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 191       |\n",
            "|    time_elapsed    | 47        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -200      |\n",
            "|    critic_loss     | 4.39e+05  |\n",
            "|    ent_coef        | 0.0654    |\n",
            "|    ent_coef_loss   | -37.9     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 259.11386 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.23865465898066826\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 910         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 2           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.055538934 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 880         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009210166 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000239    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 60.7        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | 18.302952   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 121         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 867          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077271312 |\n",
            "|    clip_fraction        | 0.0652       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000499    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 272          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00491     |\n",
            "|    reward               | 1.3290982    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 762          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010335308 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.0024      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    reward               | 1.8385755   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 83.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 862         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011866614 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.0226      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31.7        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0119     |\n",
            "|    reward               | 0.54147655  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 58.9        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.21545220048710753\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -5.59      |\n",
            "|    reward             | -0.2173932 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.195      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 731        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 6.28       |\n",
            "|    reward             | 0.63071674 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.589      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 730       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.102     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 46.7      |\n",
            "|    reward             | -1.684271 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 732      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 39.4     |\n",
            "|    reward             | 0.470711 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 732       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 1.01      |\n",
            "|    reward             | 1.1561015 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.277     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 733       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -9.73     |\n",
            "|    reward             | -0.352485 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.677     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 17.8       |\n",
            "|    reward             | 0.08625779 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 2.97       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 735         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -0.248      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 18          |\n",
            "|    reward             | -0.18377179 |\n",
            "|    std                | 0.992       |\n",
            "|    value_loss         | 4.43        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 736         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 10.3        |\n",
            "|    reward             | -0.46454662 |\n",
            "|    std                | 0.994       |\n",
            "|    value_loss         | 1.38        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 735       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 6.34      |\n",
            "|    reward             | 0.1993567 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.449     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 736       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -2.2      |\n",
            "|    reward             | 0.8348019 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.0462    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 736       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 14.1      |\n",
            "|    reward             | 2.2133098 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.63      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 737      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -30.7    |\n",
            "|    reward             | 1.735257 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 7.26     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 736        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -1.6       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -7.04      |\n",
            "|    reward             | 0.50823855 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.34       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -7.25    |\n",
            "|    reward             | -0.36113 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.415    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0425   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 10.6     |\n",
            "|    reward             | 1.155333 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.691    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 736        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.00391   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 19.8       |\n",
            "|    reward             | 0.19697165 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 737       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0106    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -2.14     |\n",
            "|    reward             | -0.103992 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.699     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 736         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 24.2        |\n",
            "|    reward             | -0.05670632 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.67        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -6.1       |\n",
            "|    reward             | -1.0926468 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.837      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  -0.19517629586106494\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 283      |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -191     |\n",
            "|    critic_loss     | 48.4     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 0.738171 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.06581803437522336\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 252        |\n",
            "|    time_elapsed    | 36         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -53.6      |\n",
            "|    critic_loss     | 3.61e+04   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -59.419964 |\n",
            "-----------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  -0.2238729303810734\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 195      |\n",
            "|    time_elapsed    | 47       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -81.9    |\n",
            "|    critic_loss     | 1.83e+04 |\n",
            "|    ent_coef        | 0.0596   |\n",
            "|    ent_coef_loss   | -0.523   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.20414176953486343\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 897        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.1323547 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 861         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006854051 |\n",
            "|    clip_fraction        | 0.0817      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000813   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 250         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00768    |\n",
            "|    reward               | -5.6313057  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 538         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 853         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010732565 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0168     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 45.9        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00927    |\n",
            "|    reward               | -7.631157   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 93.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 846         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008870937 |\n",
            "|    clip_fraction        | 0.0647      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00627    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 72.2        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00874    |\n",
            "|    reward               | 24.312145   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 117         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004102311 |\n",
            "|    clip_fraction        | 0.0137      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000117    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.16e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00319    |\n",
            "|    reward               | -1.7247633  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.33e+04    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  -0.14550087266565742\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 707         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.00012     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.06       |\n",
            "|    reward             | -0.13895606 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.177       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 634         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.166      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 11.3        |\n",
            "|    reward             | -0.14249718 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.35        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 654       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.104    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 50.5      |\n",
            "|    reward             | -1.137824 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 20.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 665       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 20        |\n",
            "|    reward             | 2.5081718 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.25      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 668         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0.149       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -5.45       |\n",
            "|    reward             | -0.21787581 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.669       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 675         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.58       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -15.5       |\n",
            "|    reward             | -0.25785145 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.74        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 679        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.211      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -6.55      |\n",
            "|    reward             | -0.9277689 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.397      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 682       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.0472    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -6.37     |\n",
            "|    reward             | 1.4186193 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.499     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 685       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 11.3      |\n",
            "|    reward             | 0.8871435 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.09      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 685        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.029     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -15.2      |\n",
            "|    reward             | 0.44847578 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.58       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 688       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.00751  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 24.3      |\n",
            "|    reward             | -1.285184 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.57      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 692        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 18.3       |\n",
            "|    reward             | -1.7374316 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.75       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 694       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0678   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -9.56     |\n",
            "|    reward             | -2.229728 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 0.738     |\n",
            "|    reward             | -1.213086 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.66      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 699        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.991     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 9.73       |\n",
            "|    reward             | 0.38127783 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.703      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.00255   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -2.07     |\n",
            "|    reward             | -0.347868 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.193     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 702       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 13.3      |\n",
            "|    reward             | -0.657334 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.59      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 704       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 5.07      |\n",
            "|    reward             | -0.734131 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.29      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 704        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -2.94      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -36.3      |\n",
            "|    reward             | 0.22871706 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 17.3       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 706         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.183      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -5.75       |\n",
            "|    reward             | -0.06391591 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.34        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.24013523977113932\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 283       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 409       |\n",
            "|    critic_loss     | 1.89e+04  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.172193 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2931043098457235\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 251      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 162      |\n",
            "|    critic_loss     | 125      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.22709  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.1766261486234415\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 119       |\n",
            "|    time_elapsed    | 79        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 889       |\n",
            "|    critic_loss     | 4.07e+04  |\n",
            "|    ent_coef        | 0.0334    |\n",
            "|    ent_coef_loss   | -11.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | 12.567791 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.14499130826540485\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 877        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.0117112 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 843         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005816684 |\n",
            "|    clip_fraction        | 0.0717      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000395   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.51e+03    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0055     |\n",
            "|    reward               | -0.12298374 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.98e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 837         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008790742 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00267    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 144         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00886    |\n",
            "|    reward               | 0.4347762   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 277         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008797821 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0276     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0091     |\n",
            "|    reward               | 0.9827081   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 38.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008930921 |\n",
            "|    clip_fraction        | 0.0997      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.9       |\n",
            "|    explained_variance   | -0.00794    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.32        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00978    |\n",
            "|    reward               | 0.6106333   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.21884701310072954\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 679       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.082    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -1.01     |\n",
            "|    reward             | -0.569146 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.385     |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 698          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 8.32         |\n",
            "|    reward             | -0.055588175 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.466        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 704        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0108    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -69.9      |\n",
            "|    reward             | -1.5083992 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 25.5       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 708         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.232      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | 29.2        |\n",
            "|    reward             | -0.08175445 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 6.76        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 709         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.873      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 16          |\n",
            "|    reward             | -0.66853154 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.7         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 708        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -0.0033    |\n",
            "|    reward             | 0.32617122 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.135      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 709        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0556     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -13.5      |\n",
            "|    reward             | 0.05398309 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.09       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 711        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.243      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 7.6        |\n",
            "|    reward             | 0.23561127 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.497      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 10        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 412       |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0435    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 2.54      |\n",
            "|    reward             | -1.793435 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.698     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 12          |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 413         |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 3.19        |\n",
            "|    reward             | -0.38262695 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.142       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 13       |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 413      |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0438   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 10.5     |\n",
            "|    reward             | 0.051852 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 14         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 414        |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0532    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 3.26       |\n",
            "|    reward             | 0.19457036 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.43       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 15         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 415        |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.149      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 9.45       |\n",
            "|    reward             | 0.45911634 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.868      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 16        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 416       |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.00285   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 15.8      |\n",
            "|    reward             | -0.846346 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 17         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 416        |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -8.6       |\n",
            "|    reward             | -0.8665836 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.557      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 19       |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 417      |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.248   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -3.88    |\n",
            "|    reward             | 0.197087 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.388    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 20        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 418       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.31      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -4.49     |\n",
            "|    reward             | 0.2803177 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.204     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 21          |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 418         |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 20.5        |\n",
            "|    reward             | 0.058994778 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.76        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 22        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 419       |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 22.4      |\n",
            "|    reward             | -0.421328 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.85      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 23        |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 420       |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 4.17e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 11.9      |\n",
            "|    reward             | 1.2877182 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.13      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.07759254743594436\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 275       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 643       |\n",
            "|    critic_loss     | 6.49e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 164.05103 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.48086574508797625\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 22        |\n",
            "|    time_elapsed    | 434       |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 638       |\n",
            "|    critic_loss     | 3.07e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 170.46278 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.4824466803023115\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 183      |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.38e+03 |\n",
            "|    critic_loss     | 1.8e+03  |\n",
            "|    ent_coef        | 1.59     |\n",
            "|    ent_coef_loss   | -49.7    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.049774 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.016011641056379262\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 823        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.2144302 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 794         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006534317 |\n",
            "|    clip_fraction        | 0.0796      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0111     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 93          |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0067     |\n",
            "|    reward               | 4.597636    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 176         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 786          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071768872 |\n",
            "|    clip_fraction        | 0.0905       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.00105     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 88.7         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00919     |\n",
            "|    reward               | -1.410839    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 181          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 782         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007462776 |\n",
            "|    clip_fraction        | 0.0979      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00646    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 88.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00699    |\n",
            "|    reward               | -0.8207816  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 134         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 780         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011345446 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00592    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.5        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | 0.33526474  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 43.5        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.27419941652020247\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  35.87108846505483  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2015-2025_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_crypto/\n",
            "[INFO] Moved results to 2015-2025_crypto/\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 813         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.81       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -3.52       |\n",
            "|    reward             | -0.23200266 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.191       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 822        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 4.3        |\n",
            "|    reward             | 0.03640779 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.173      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 820        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.364     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 18.3       |\n",
            "|    reward             | -0.5018182 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.39       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 820          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0.194        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -28.4        |\n",
            "|    reward             | -0.039650235 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 6.35         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 824       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.51     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 8.17      |\n",
            "|    reward             | 0.1417553 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.49      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 826        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -5.14      |\n",
            "|    reward             | 0.92955476 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.931      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 828         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.056      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -4.02       |\n",
            "|    reward             | -0.36553803 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.13        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 829          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -2.9         |\n",
            "|    reward             | -0.014351382 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.0745       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 830        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.125     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -3.95      |\n",
            "|    reward             | 0.88391876 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.332      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 831         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.102      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 19.3        |\n",
            "|    reward             | -0.24975741 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 3.13        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 832       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.607    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.646    |\n",
            "|    reward             | 2.1812334 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.59      |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 831           |\n",
            "|    iterations         | 1200          |\n",
            "|    time_elapsed       | 7             |\n",
            "|    total_timesteps    | 6000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -11.4         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1199          |\n",
            "|    policy_loss        | 0.608         |\n",
            "|    reward             | -0.0001309198 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 0.00599       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 832         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.108      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 9.36        |\n",
            "|    reward             | -0.28922695 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.683       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 833      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 0.423513 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 834       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -0.786    |\n",
            "|    reward             | 1.3960855 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.96      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 834         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | -0.863      |\n",
            "|    reward             | -0.20168124 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.00649     |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 832      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 2.28     |\n",
            "|    reward             | 0.28951  |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.218    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 833         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.307      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 17.1        |\n",
            "|    reward             | -0.47505543 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.72        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 833        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 47.5       |\n",
            "|    reward             | -0.3034096 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 22.1       |\n",
            "--------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1524831.04\n",
            "total_reward: 524831.04\n",
            "total_cost: 12839.00\n",
            "total_trades: 9156\n",
            "Sharpe: 0.369\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 834         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -2.65       |\n",
            "|    reward             | -0.74405986 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0681      |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.06039892983103166\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1416045.28\n",
            "total_reward: 416045.28\n",
            "total_cost: 999.00\n",
            "total_trades: 5982\n",
            "Sharpe: 0.321\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 291       |\n",
            "|    time_elapsed    | 27        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 7.88      |\n",
            "|    critic_loss     | 12.9      |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.575632 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.05723056180195707\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1289567.70\n",
            "total_reward: 289567.70\n",
            "total_cost: 999.00\n",
            "total_trades: 9970\n",
            "Sharpe: 0.280\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 252       |\n",
            "|    time_elapsed    | 31        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 407       |\n",
            "|    critic_loss     | 5.81e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.313064 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.04709174327952072\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1507212.66\n",
            "total_reward: 507212.66\n",
            "total_cost: 998.99\n",
            "total_trades: 7946\n",
            "Sharpe: 0.376\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 193       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.43e+03  |\n",
            "|    critic_loss     | 1.16e+03  |\n",
            "|    ent_coef        | 0.982     |\n",
            "|    ent_coef_loss   | 1.34      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.332517 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.11276190186810073\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1107798.19\n",
            "total_reward: 107798.19\n",
            "total_cost: 129986.19\n",
            "total_trades: 15531\n",
            "Sharpe: 0.181\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 1019      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.1448144 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 983         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011696417 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00924    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.09        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00808    |\n",
            "|    reward               | -0.11788982 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.61        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 966         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010547401 |\n",
            "|    clip_fraction        | 0.0901      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00482    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.21        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00988    |\n",
            "|    reward               | -0.10060742 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 961          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0109418975 |\n",
            "|    clip_fraction        | 0.0871       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00235      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.27         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0102      |\n",
            "|    reward               | -0.5548608   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 5.57         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 959         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006925586 |\n",
            "|    clip_fraction        | 0.0558      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.000731    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.26        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00901    |\n",
            "|    reward               | 0.45920166  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.88        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.018273758863122738\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 790         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0.134       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -10.2       |\n",
            "|    reward             | -0.10839158 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.845       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 805        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.169      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 10.5       |\n",
            "|    reward             | 0.26749247 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.977      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 807        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0485     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 40.1       |\n",
            "|    reward             | -1.3060035 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 14.3       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 783      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0419  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 15.5     |\n",
            "|    reward             | 1.707912 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 789        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.52      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -1.71      |\n",
            "|    reward             | -0.7557109 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0889     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 795        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0947     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -10.9      |\n",
            "|    reward             | -1.9689803 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.89       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 800        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -7.63      |\n",
            "|    reward             | 0.76904714 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.26       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.711    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -86.1     |\n",
            "|    reward             | 1.5327336 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 59.4      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 803         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -0.0714     |\n",
            "|    reward             | -0.10949476 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.097       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 806         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.288      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 1.22        |\n",
            "|    reward             | -0.22747333 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.255       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 808         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.382      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -2.05       |\n",
            "|    reward             | -0.16865432 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.149       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 810      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -4.8e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 0.89     |\n",
            "|    reward             | 0.442736 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.0344   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 810        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.69      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -0.158     |\n",
            "|    reward             | -0.4030108 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0524     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 811        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -4.29      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 2.77       |\n",
            "|    reward             | 0.05669934 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.706      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 812       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0781    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -15.4     |\n",
            "|    reward             | 1.0673382 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 813       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.171    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -43.4     |\n",
            "|    reward             | 0.4391266 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 813       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.145    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 17.7      |\n",
            "|    reward             | 0.8532557 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.02      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 814        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.131      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 4.39       |\n",
            "|    reward             | -0.6756428 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.31       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 814        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.141      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 13.2       |\n",
            "|    reward             | 0.18610185 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 815       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0993    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 49        |\n",
            "|    reward             | 1.1726766 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 23.3      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.33207304196477155\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 272       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -177      |\n",
            "|    critic_loss     | 817       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -1.876871 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.16280730949440644\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 233      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 69.9     |\n",
            "|    critic_loss     | 1.43     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.924048 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.22350301918295257\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 203      |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.91e+03 |\n",
            "|    critic_loss     | 315      |\n",
            "|    ent_coef        | 1.05     |\n",
            "|    ent_coef_loss   | -3.91    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.126081 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.1441712928399153\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 1052       |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 1          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.9430153 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1012        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008162204 |\n",
            "|    clip_fraction        | 0.0937      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00291     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.05        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    reward               | 0.16076407  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 996          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070318743 |\n",
            "|    clip_fraction        | 0.0933       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0356      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.82         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00612     |\n",
            "|    reward               | -0.11956893  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.63         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 990         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007810783 |\n",
            "|    clip_fraction        | 0.0617      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0133     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.53        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00901    |\n",
            "|    reward               | 1.2599881   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.3         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 987         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008530028 |\n",
            "|    clip_fraction        | 0.0787      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00996     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.14        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00894    |\n",
            "|    reward               | -0.16039571 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 13.3        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.170382961391612\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 818         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -6.41       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -6.96       |\n",
            "|    reward             | -0.03599492 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 0.435       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 828        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.956     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 4.11       |\n",
            "|    reward             | 0.10215138 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.148      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 831        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 25.9       |\n",
            "|    reward             | -0.3877966 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.61       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 832       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0827    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 18.8      |\n",
            "|    reward             | 2.7824376 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.17      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 832       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -7.01     |\n",
            "|    reward             | -0.192453 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.476     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 830         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.195      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -1.05       |\n",
            "|    reward             | -0.10594409 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0283      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 830        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.56      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 9.75       |\n",
            "|    reward             | -0.5138578 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.2        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 831      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.471   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -8.58    |\n",
            "|    reward             | -0.13576 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.641    |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 830           |\n",
            "|    iterations         | 900           |\n",
            "|    time_elapsed       | 5             |\n",
            "|    total_timesteps    | 4500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -11.4         |\n",
            "|    explained_variance | -0.0324       |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 899           |\n",
            "|    policy_loss        | 0.319         |\n",
            "|    reward             | -0.0002838579 |\n",
            "|    std                | 1.01          |\n",
            "|    value_loss         | 0.746         |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 831        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0958     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -1.36      |\n",
            "|    reward             | -4.3285065 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.178      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 832       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.98     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -2.180639 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.76      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 833        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.131     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -57.8      |\n",
            "|    reward             | -2.5984724 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 22.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 832        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.108      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -2.27      |\n",
            "|    reward             | -0.2606478 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.299      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 833        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -5.23      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -2.11      |\n",
            "|    reward             | 0.15030997 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.107      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 834       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.47      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -19.7     |\n",
            "|    reward             | 2.1311903 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.46      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 835      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0267  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 8.42     |\n",
            "|    reward             | 1.318268 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 835         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 0.901       |\n",
            "|    reward             | 0.012303708 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0155      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 835      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 7.25     |\n",
            "|    reward             | 0.17649  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.618    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 836         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -10.1       |\n",
            "|    reward             | -0.29002097 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.08        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 837         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 27.1        |\n",
            "|    reward             | -0.12512818 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 9.43        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3384541451029498\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 305       |\n",
            "|    time_elapsed    | 27        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 21.3      |\n",
            "|    critic_loss     | 1.78e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -0.397883 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.5792121244159574\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 269       |\n",
            "|    time_elapsed    | 31        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.69      |\n",
            "|    critic_loss     | 1.01e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.359738 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.44815405516795653\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 203       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.27e+03  |\n",
            "|    critic_loss     | 849       |\n",
            "|    ent_coef        | 1.09      |\n",
            "|    ent_coef_loss   | -8.15     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -0.439206 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.5831068319786893\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 1037      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 1         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -1.824561 |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 997          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063932855 |\n",
            "|    clip_fraction        | 0.075        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0244      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.92         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00634     |\n",
            "|    reward               | -1.9971148   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.86         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 985          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044513396 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00121     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.56         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00498     |\n",
            "|    reward               | -1.5589792   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.02         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 977         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008966746 |\n",
            "|    clip_fraction        | 0.0754      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.011       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.42        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0074     |\n",
            "|    reward               | -0.983772   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.62        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 975         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009148006 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00762     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.33        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0104     |\n",
            "|    reward               | -2.070759   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.82        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.43022350987839353\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 810         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -5.15       |\n",
            "|    reward             | -0.29707056 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.296       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 816         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.00032    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 0.583       |\n",
            "|    reward             | 0.077474475 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0563      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 818        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0896     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -67.8      |\n",
            "|    reward             | -1.6873881 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 25.4       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 813         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.00335     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | 24.6        |\n",
            "|    reward             | 0.121558845 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 6.54        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 812         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.134      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 4.56        |\n",
            "|    reward             | -0.36333773 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.179       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 814         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.69       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -1.53       |\n",
            "|    reward             | -0.78859216 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.24        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 815       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.21     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 4.76      |\n",
            "|    reward             | 0.6011532 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.15      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 817         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.214       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -19.5       |\n",
            "|    reward             | 0.095121846 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.09        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 816         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0719     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -11.1       |\n",
            "|    reward             | -0.31396666 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.719       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 814         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -4.54       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 9.34        |\n",
            "|    reward             | -0.76506686 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.901       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 816      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | -0.00494 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -13.4    |\n",
            "|    reward             | 2.643195 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.12     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 817        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0249    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -7.18      |\n",
            "|    reward             | 0.27916858 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.913      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 818       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.000196  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 6.81      |\n",
            "|    reward             | -0.639357 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 818        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 3.87e-06   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 12         |\n",
            "|    reward             | 0.13866597 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.747      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 819       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0927    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -5.15     |\n",
            "|    reward             | -0.017138 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.211     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 819      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -4.4     |\n",
            "|    reward             | 0.267984 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.326    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 819        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.000156  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -26.9      |\n",
            "|    reward             | 0.68992645 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.08       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 819         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -4.26       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 23.4        |\n",
            "|    reward             | -0.31690618 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.79        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 819       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0579    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -26.1     |\n",
            "|    reward             | 0.4589056 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.6       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 820        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.666     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -3.7       |\n",
            "|    reward             | 0.45358983 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.152      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.3801850416931415\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 287      |\n",
            "|    time_elapsed    | 30       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 212      |\n",
            "|    critic_loss     | 159      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.64252  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.38626029238474835\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 252      |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 46       |\n",
            "|    critic_loss     | 2.1e+04  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.93831  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.36823786021133087\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 8        |\n",
            "|    time_elapsed    | 1020     |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.72e+03 |\n",
            "|    critic_loss     | 1.36e+03 |\n",
            "|    ent_coef        | 1.33     |\n",
            "|    ent_coef_loss   | -34.5    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 1.080082 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.39609282934273093\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 985        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.7258227 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 951          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067069894 |\n",
            "|    clip_fraction        | 0.0734       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0207      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.58         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00811     |\n",
            "|    reward               | -0.58106893  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.33         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 939         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008780348 |\n",
            "|    clip_fraction        | 0.0977      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0191     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.38        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | -0.5908549  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.46        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 823         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006973393 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00784    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.47        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00532    |\n",
            "|    reward               | 0.23531301  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.4         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 843          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074837836 |\n",
            "|    clip_fraction        | 0.0686       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00139      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.83         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0107      |\n",
            "|    reward               | -0.36582488  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 9.69         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.35844776911230314\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 664        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.379     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -3.53      |\n",
            "|    reward             | -0.4757444 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.108      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 650        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.449      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 2.24       |\n",
            "|    reward             | 0.20019177 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 0.223      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 682        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.1       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 23.4       |\n",
            "|    reward             | -0.8347806 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 5.7        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 703        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 24.5       |\n",
            "|    reward             | 0.70596987 |\n",
            "|    std                | 0.985      |\n",
            "|    value_loss         | 4.95       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0.161     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -0.0801   |\n",
            "|    reward             | 0.5015621 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 0.174     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 731         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.361      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 8.82        |\n",
            "|    reward             | -0.96685535 |\n",
            "|    std                | 0.99        |\n",
            "|    value_loss         | 0.811       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 741       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 3.11      |\n",
            "|    reward             | -0.238701 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 0.133     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 749       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.484     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -0.819    |\n",
            "|    reward             | -1.229887 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 0.153     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 755         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.834      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -28.2       |\n",
            "|    reward             | 0.057633728 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 8.01        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 761      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.104    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -0.277   |\n",
            "|    reward             | 0.026235 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.0203   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 765      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 2.76     |\n",
            "|    reward             | 0.27918  |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.0517   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 770       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 2.07      |\n",
            "|    reward             | -0.035033 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0458    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 773         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.112      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 10.2        |\n",
            "|    reward             | -0.22341321 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.19        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 775         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.777      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 1.74        |\n",
            "|    reward             | -0.15000467 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.132       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 778         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0779      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -5.32       |\n",
            "|    reward             | -0.20318206 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.189       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 778       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.535    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -8.97     |\n",
            "|    reward             | 0.2442683 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.723     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 780        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.183      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -28.1      |\n",
            "|    reward             | -0.7013553 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.81       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 781          |\n",
            "|    iterations         | 1800         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 9000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1799         |\n",
            "|    policy_loss        | -0.555       |\n",
            "|    reward             | -0.013867573 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.0032       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 783        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.13      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 4.64       |\n",
            "|    reward             | 0.07957325 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.256      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 784        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0586     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 4.8        |\n",
            "|    reward             | 0.08322433 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.309      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.09527887687189164\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 299       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -65.7     |\n",
            "|    critic_loss     | 6.72e+04  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.223926 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.39571260697991195\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 8         |\n",
            "|    time_elapsed    | 1082      |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 37.9      |\n",
            "|    critic_loss     | 4.89      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.418207 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.37579604706623654\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 9         |\n",
            "|    time_elapsed    | 963       |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.32e+03  |\n",
            "|    critic_loss     | 66.5      |\n",
            "|    ent_coef        | 1.41      |\n",
            "|    ent_coef_loss   | -33.3     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.276216 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.4418692024647543\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 1004       |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.4995055 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 965         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007062851 |\n",
            "|    clip_fraction        | 0.0829      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.000963    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.23        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | -0.26087937 |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 5.73        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 955         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007903403 |\n",
            "|    clip_fraction        | 0.0826      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.00809    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.37        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00956    |\n",
            "|    reward               | 0.51484084  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 3.37        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 950         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008122424 |\n",
            "|    clip_fraction        | 0.0752      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0334     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.92        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00491    |\n",
            "|    reward               | 0.63539964  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 6.07        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 946         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009428045 |\n",
            "|    clip_fraction        | 0.0895      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.00688     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.4         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | 0.76789576  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.3         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.2779071295360645\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 785         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -9.98       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -6.42       |\n",
            "|    reward             | -0.13931067 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.352       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 793         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -3.04       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -0.499      |\n",
            "|    reward             | 0.098471224 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0354      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 794         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | -5.71       |\n",
            "|    reward             | 0.056999642 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.364       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 797        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.251      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 11.8       |\n",
            "|    reward             | 0.53283185 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.12       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 796      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.0914   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -0.682   |\n",
            "|    reward             | 0.507245 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.102    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 798        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -8.17      |\n",
            "|    reward             | -0.4028549 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.484      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 799      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 15.5     |\n",
            "|    reward             | 0.056027 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 800         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 18.2        |\n",
            "|    reward             | -0.17076047 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.54        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 801         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 8.39        |\n",
            "|    reward             | -0.36545464 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.12        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 801        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 0.848      |\n",
            "|    reward             | 0.31217512 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0305     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 801      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 7.22     |\n",
            "|    reward             | 0.307765 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 6.18     |\n",
            "|    reward             | 0.850356 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.449    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -16.5     |\n",
            "|    reward             | 1.05675   |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.76      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.211    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -11      |\n",
            "|    reward             | 0.037863 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.995    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -0.0175   |\n",
            "|    reward             | -0.374102 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.00811   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -1.96     |\n",
            "|    reward             | 0.9559212 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.269     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 803        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 2.6        |\n",
            "|    reward             | 0.89428926 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.323      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 803         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 4.17        |\n",
            "|    reward             | -0.05657773 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.264       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.513   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 2.31     |\n",
            "|    reward             | 0.192774 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.219    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 802       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -8.36     |\n",
            "|    reward             | -1.428758 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.701     |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.11631156068641049\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 295      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52.5     |\n",
            "|    critic_loss     | 749      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.195236 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.045051849743101446\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 9        |\n",
            "|    time_elapsed    | 963      |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 128      |\n",
            "|    critic_loss     | 7.55e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.335476 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.3821649413745514\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 195      |\n",
            "|    time_elapsed    | 47       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.18e+03 |\n",
            "|    critic_loss     | 3.75e+03 |\n",
            "|    ent_coef        | 1.47     |\n",
            "|    ent_coef_loss   | -29.3    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.587312 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.25889379083717345\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 961        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.1622725 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 940         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006392054 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0145     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.1         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00912    |\n",
            "|    reward               | -3.944924   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.15        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 929          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.010419555  |\n",
            "|    clip_fraction        | 0.1          |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0135      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.16         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0121      |\n",
            "|    reward               | -0.045415115 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 9.59         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 922         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007059076 |\n",
            "|    clip_fraction        | 0.0758      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0201     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.23        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00895    |\n",
            "|    reward               | 0.2684449   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.29        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.005924084  |\n",
            "|    clip_fraction        | 0.0432       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00303      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.79         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00644     |\n",
            "|    reward               | -0.068337224 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 5.43         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.09608979773397928\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 773         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.2        |\n",
            "|    reward             | -0.16272685 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.301       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 777       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 12.5      |\n",
            "|    reward             | 0.0114865 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.71      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 780      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.256    |\n",
            "|    reward             | 1.141728 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 776      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 29.5     |\n",
            "|    reward             | 3.602367 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 5.34     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 775        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.11      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -19.4      |\n",
            "|    reward             | -0.3678552 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 4.02       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 777         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.493      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 1.28        |\n",
            "|    reward             | -0.30902618 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 0.25        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -6.91      |\n",
            "|    reward             | -0.7546254 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.502      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 780      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 2.28     |\n",
            "|    reward             | 3.171024 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 781         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -26.1       |\n",
            "|    reward             | -0.70192933 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.8         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -2.01      |\n",
            "|    reward             | 0.14289851 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.134      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 781         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.000284   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 9.41        |\n",
            "|    reward             | -0.15619078 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.621       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 782       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 0.0674    |\n",
            "|    reward             | -0.024396 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.000281  |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 782          |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | -0.31        |\n",
            "|    reward             | -0.024397355 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.00194      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 782          |\n",
            "|    iterations         | 1400         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 7000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1399         |\n",
            "|    policy_loss        | 0.526        |\n",
            "|    reward             | -0.033810195 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 0.00306      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 782        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -3.87      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 0.522      |\n",
            "|    reward             | 0.28230825 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.0179     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 760         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -1.14       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 3.21        |\n",
            "|    reward             | -0.27297476 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.1         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.171     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 5.75       |\n",
            "|    reward             | -0.4444616 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.426      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 681         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -0.0835     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 2.84        |\n",
            "|    reward             | -0.78759015 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.988       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 67         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 141        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -0.585     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -18.6      |\n",
            "|    reward             | 0.08217031 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 7.46       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 70          |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 141         |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | 0.281       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -6.79       |\n",
            "|    reward             | -0.05410775 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.334       |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.15250711823130106\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 289      |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -140     |\n",
            "|    critic_loss     | 108      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.300066 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.19061917157634525\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 564       |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 318       |\n",
            "|    critic_loss     | 276       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.083926 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.17904328472139364\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 178       |\n",
            "|    time_elapsed    | 53        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.16e+03  |\n",
            "|    critic_loss     | 937       |\n",
            "|    ent_coef        | 1.49      |\n",
            "|    ent_coef_loss   | -30.3     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.237801 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.29302503951454567\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 900        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.7340034 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 876         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006588918 |\n",
            "|    clip_fraction        | 0.0651      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00914    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.23        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0058     |\n",
            "|    reward               | -0.26446977 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.33        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 870         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006646486 |\n",
            "|    clip_fraction        | 0.0653      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0258     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.41        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00584    |\n",
            "|    reward               | 0.2313412   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.93        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 867         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008356684 |\n",
            "|    clip_fraction        | 0.066       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00208     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.18        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00767    |\n",
            "|    reward               | -0.24480939 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.4         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 862          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074022296 |\n",
            "|    clip_fraction        | 0.0817       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00689     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00975     |\n",
            "|    reward               | 0.41401994   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.13         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.20567718559101503\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 718        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.197     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -4.98      |\n",
            "|    reward             | -0.5030374 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.228      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 725          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | -2.07        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 3.2          |\n",
            "|    reward             | -0.110712714 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.235        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 691        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.106      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -50.9      |\n",
            "|    reward             | -1.4603891 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 16.7       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 705        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.48       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 27.1       |\n",
            "|    reward             | 0.52983123 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 6.06       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 712         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 1.12        |\n",
            "|    reward             | -0.15191686 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0178      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.166      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 0.973      |\n",
            "|    reward             | 0.25220412 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0327     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 721         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.132       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -11.3       |\n",
            "|    reward             | -0.03844063 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.886       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.216     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 4.55       |\n",
            "|    reward             | 0.18918695 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.225      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.259      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 5.54       |\n",
            "|    reward             | -1.7171175 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.948      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 729         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.272       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 2.26        |\n",
            "|    reward             | -0.10621271 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0919      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 730         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.149       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 9.25        |\n",
            "|    reward             | 0.048768133 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.584       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 730        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.227     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 1.16       |\n",
            "|    reward             | 0.16096877 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.508      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 732        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.00835    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -3.85      |\n",
            "|    reward             | 0.69867074 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.517      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 734        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.296      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 9.06       |\n",
            "|    reward             | 0.64001924 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.831      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 733         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.119       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -0.998      |\n",
            "|    reward             | -0.21055037 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.18        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -10.3      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -3.41      |\n",
            "|    reward             | -0.1293714 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.12       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.172      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -1.45      |\n",
            "|    reward             | 0.31713483 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0928     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.0653   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 10.2     |\n",
            "|    reward             | 1.032493 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.696     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 34.9       |\n",
            "|    reward             | 0.57217973 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 12.3       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.173     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 18.4       |\n",
            "|    reward             | -1.3319477 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.43       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.24440856067935615\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.22e+03 |\n",
            "|    critic_loss     | 8.92e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.219157 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.042166237860999406\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 242      |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 110      |\n",
            "|    critic_loss     | 19.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.427204 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.045788501216716286\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 188      |\n",
            "|    time_elapsed    | 51       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 83.9     |\n",
            "|    critic_loss     | 10       |\n",
            "|    ent_coef        | 0.0279   |\n",
            "|    ent_coef_loss   | -30.8    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.872992 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.05586519603054317\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 912       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -1.777918 |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 878          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066713965 |\n",
            "|    clip_fraction        | 0.097        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0132      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.28         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00633     |\n",
            "|    reward               | -0.07293187  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.54         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 866        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 7          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00854846 |\n",
            "|    clip_fraction        | 0.0893     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | 0.00558    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.92       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0111    |\n",
            "|    reward               | -1.3952495 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 4.33       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 861          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.007915443  |\n",
            "|    clip_fraction        | 0.0796       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0244       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.1          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0104      |\n",
            "|    reward               | -0.030610327 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 7.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 850          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083266795 |\n",
            "|    clip_fraction        | 0.1          |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00664     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.33         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0099      |\n",
            "|    reward               | 0.09723018   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 10.1         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.044585539234815834\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  111.63297735055288  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2015-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_no_crypto/\n",
            "[INFO] Moved results to 2015-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "# df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "#     ensemble_agent = ensemble_agent,\n",
        "#     A2C_kwargs = A2C_model_kwargs,\n",
        "#     PPO_kwargs = PPO_model_kwargs,\n",
        "#     DDPG_kwargs = DDPG_model_kwargs,\n",
        "#     SAC_kwargs = SAC_model_kwargs,\n",
        "#     TD3_kwargs = TD3_model_kwargs,\n",
        "#     timesteps_dict = timesteps_dict,\n",
        "#     processed_df = processed,\n",
        "#     trade_start_date = '2023-01-04',\n",
        "#     trade_end_date = '2025-04-11',\n",
        "#     rebalance_window = 63,\n",
        "#     validation_window = 63,\n",
        "#     output_csv_name = \"df_daily_return_ensemble.csv\"\n",
        "# )\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_0,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_0,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2007-2025_no_crypto.csv\"\n",
        ")\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_1,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_1,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_crypto.csv\"\n",
        ")\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_2,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_2,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_no_crypto.csv\"\n",
        ")\n",
        "\n",
        "# processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "# processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "# processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
