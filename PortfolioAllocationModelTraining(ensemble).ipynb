{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "## Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPT0ipYE28wL",
        "outputId": "912ac487-d3c8-467f-ef2a-968fa655b206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-tt9pa65s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-tt9pa65s\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-cm1at__x/elegantrl_dabc31873bfe492496036e85fdfbbfae\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-cm1at__x/elegantrl_dabc31873bfe492496036e85fdfbbfae\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# ## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install pandas_market_calendars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')  \n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Enable Inline Plotting (Jupyter)\n",
        "# ===========================\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Custom Imports (model.py)\n",
        "# ===========================\n",
        "sys.path.append(os.path.abspath(\".\"))  \n",
        "from models import DRLEnsembleAgent\n",
        "\n",
        "sys.path.append(\"../FinRL-Library\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "## `process_csv_to_features(csv_path)`\n",
        "\n",
        "Processes financial data from a CSV by adding technical indicators and turbulence features.\n",
        "\n",
        "### **Parameters**\n",
        "- `csv_path` *(str)*: Path to the raw financial data CSV.\n",
        "\n",
        "### **Workflow**\n",
        "1. Load data.\n",
        "2. Identify 5-day and 7-day tickers.\n",
        "3. Apply technical indicators.\n",
        "4. Combine datasets.\n",
        "5. Add turbulence feature.\n",
        "6. Clean `NaN` and infinite values.\n",
        "\n",
        "### **Returns**\n",
        "- `processed` *(DataFrame)*: Feature-enhanced, cleaned DataFrame for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_to_features(csv_path):\n",
        "    # Step 1: Load Data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 3: Apply Technical Indicators\n",
        "    fe_ti = FeatureEngineer(\n",
        "        use_technical_indicator=True,\n",
        "        use_turbulence=False,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    df_5day_full = fe_ti.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe_ti.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 4: Combine and Clean Index\n",
        "    combined_df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    combined_df.index = range(len(combined_df))\n",
        "\n",
        "    # Step 5: Remove dates with only one ticker\n",
        "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "    combined_df = combined_df[combined_df.groupby('date')['date'].transform('count') > 1]\n",
        "    combined_df = combined_df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Apply Turbulence Feature\n",
        "    fe_turb = FeatureEngineer(\n",
        "        use_technical_indicator=False,\n",
        "        use_turbulence=True,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    processed = fe_turb.preprocess_data(combined_df)\n",
        "\n",
        "    # Step 7: Final Cleaning\n",
        "    processed = processed.copy()\n",
        "    processed = processed.fillna(0)\n",
        "    processed = processed.replace(np.inf, 0)\n",
        "\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "## `setup_drl_ensemble_agent(...)`\n",
        "\n",
        "Initializes a `DRLEnsembleAgent` with dynamic environment and parameter settings.\n",
        "\n",
        "### **Key Parameters**\n",
        "- `processed_df` *(DataFrame)*: Data with features for training and trading.\n",
        "- `indicators` *(list)*: Technical indicators used.\n",
        "- `train_start_date`, `train_end_date`: Training period.\n",
        "- `trade_start_date`, `trade_end_date`: Trading period.\n",
        "- `rebalance_window`, `validation_window`: Rebalancing and validation frequency.\n",
        "- `initial_amount`, `transaction_cost`, `hmax`, `reward_scaling`: Trading environment settings.\n",
        "\n",
        "### **Returns**\n",
        "- `agent`: Configured `DRLEnsembleAgent` ready for training and trading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_drl_ensemble_agent(processed_df,  \n",
        "                              indicators, \n",
        "                              train_start_date, \n",
        "                              train_end_date,\n",
        "                              trade_start_date, \n",
        "                              trade_end_date, \n",
        "                              rebalance_window=63, \n",
        "                              validation_window=63, \n",
        "                              initial_amount=1_000_000,\n",
        "                              transaction_cost=0.001,\n",
        "                              hmax=100,\n",
        "                              reward_scaling=1e-4,\n",
        "                              print_verbosity=5):\n",
        "    \"\"\"\n",
        "    Setup DRLEnsembleAgent with flexible date and parameter configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate dynamic parameters\n",
        "    stock_dimension = len(processed_df.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(indicators) * stock_dimension\n",
        "\n",
        "    # 2. Environment configuration\n",
        "    env_kwargs = {\n",
        "        \"hmax\": hmax,\n",
        "        \"initial_amount\": initial_amount,\n",
        "        \"buy_cost_pct\": transaction_cost,\n",
        "        \"sell_cost_pct\": transaction_cost,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": indicators,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": reward_scaling,\n",
        "        \"print_verbosity\": print_verbosity\n",
        "    }\n",
        "\n",
        "    # 3. Initialize DRLEnsembleAgent\n",
        "    agent = DRLEnsembleAgent(\n",
        "        df=processed_df,\n",
        "        train_period=(train_start_date, train_end_date),\n",
        "        val_test_period=(trade_start_date, trade_end_date),\n",
        "        rebalance_window=rebalance_window,\n",
        "        validation_window=validation_window,\n",
        "        **env_kwargs\n",
        "    )\n",
        "\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "## DRL Model Hyperparameters & Training Timesteps\n",
        "\n",
        "Defines hyperparameters for five DRL algorithms and their training timesteps.\n",
        "\n",
        "### **Model Hyperparameters**\n",
        "- **A2C**:  \n",
        "  `n_steps`, `ent_coef`, `learning_rate`\n",
        "\n",
        "- **PPO**:  \n",
        "  `n_steps`, `ent_coef`, `learning_rate`, `batch_size`\n",
        "\n",
        "- **DDPG**:  \n",
        "  `buffer_size`, `learning_rate`, `batch_size`\n",
        "\n",
        "- **SAC**:  \n",
        "  `batch_size`, `buffer_size`, `learning_rate`, `learning_starts`, `ent_coef`\n",
        "\n",
        "- **TD3**:  \n",
        "  `batch_size`, `buffer_size`, `learning_rate`\n",
        "\n",
        "### **Training Timesteps**\n",
        "- Each model: `10,000` timesteps  \n",
        "  *(Defined in `timesteps_dict`)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      \"buffer_size\": 10_000,\n",
        "                      \"learning_rate\": 0.0005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "                      \"batch_size\": 128,\n",
        "                      \"buffer_size\": 100000,\n",
        "                      \"learning_rate\": 0.0003,\n",
        "                      \"learning_starts\": 100,\n",
        "                      \"ent_coef\": \"auto_0.1\",\n",
        "                    }\n",
        "\n",
        "TD3_model_kwargs = {\n",
        "                      \"batch_size\": 100,\n",
        "                      \"buffer_size\": 1000000,\n",
        "                      \"learning_rate\": 0.001\n",
        "                   }\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 10_000,\n",
        "                 'ppo' : 10_000,\n",
        "                 'ddpg' : 10_000,\n",
        "                  'sac' : 10_000,\n",
        "                 'td3' : 10_000,\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "## `run_ensemble_and_generate_daily_return(...)`\n",
        "\n",
        "Executes a DRL Ensemble Strategy, tracks portfolio performance, calculates daily returns, and organizes output files.\n",
        "\n",
        "### **Key Features**\n",
        "- Runs `ensemble_agent` with specified model hyperparameters and timesteps.\n",
        "- Tracks continuous portfolio value across rebalancing periods.\n",
        "- Generates and saves a portfolio value plot.\n",
        "- Calculates daily returns and exports to CSV.\n",
        "- Organizes trained models, logs, and results into a structured folder.\n",
        "\n",
        "### **Parameters**\n",
        "- `ensemble_agent`: Initialized DRLEnsembleAgent.\n",
        "- Model kwargs: `A2C_kwargs`, `PPO_kwargs`, `DDPG_kwargs`, `SAC_kwargs`, `TD3_kwargs`.\n",
        "- `timesteps_dict`: Training timesteps per model.\n",
        "- `processed_df`: Feature-enhanced DataFrame.\n",
        "- Date ranges, rebalancing configs, and file management options.\n",
        "\n",
        "### **Returns**\n",
        "- `df_daily_return` *(DataFrame)*: Daily return series for the portfolio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ensemble_and_generate_daily_return(ensemble_agent, \n",
        "                                            A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, \n",
        "                                            timesteps_dict, \n",
        "                                            processed_df, \n",
        "                                            trade_start_date, trade_end_date, \n",
        "                                            rebalance_window, validation_window, \n",
        "                                            output_csv_name=\"df_daily_return.csv\",\n",
        "                                            initial_fund=1_000_000,\n",
        "                                            original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Runs DRL Ensemble Strategy, tracks continuous portfolio value, \n",
        "    calculates daily returns, saves outputs, and organizes files into a folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "\n",
        "    # === Step 1: Create Folder Based on CSV Name ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]\n",
        "    target_folder = f\"{base_name}\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "        print(f\"[INFO] Created folder: {target_folder}\")\n",
        "\n",
        "    # === Step 2: Run Ensemble Strategy ===\n",
        "    print(\"[INFO] Running Ensemble Strategy...\")\n",
        "    df_summary = ensemble_agent.run_ensemble_strategy(\n",
        "        A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, timesteps_dict\n",
        "    )\n",
        "\n",
        "    # === Step 3: Prepare Trade Dates ===\n",
        "    unique_trade_date = processed_df[\n",
        "        (processed_df.date >= trade_start_date) & (processed_df.date <= trade_end_date)\n",
        "    ].date.unique()\n",
        "\n",
        "    current_value = initial_fund\n",
        "    portfolio_tracking = []\n",
        "    is_first_file = True\n",
        "\n",
        "    rebalance_points = list(range(rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window))\n",
        "\n",
        "    # === Step 4: Track Portfolio Value Across Rebalances ===\n",
        "    for i in rebalance_points:\n",
        "        file_path = f'results/account_value_trade_ensemble_{i}.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            temp = pd.read_csv(file_path)\n",
        "\n",
        "            if is_first_file:\n",
        "                first_date = temp.loc[0, 'date']\n",
        "                original_value = temp.loc[0, 'account_value']\n",
        "                portfolio_tracking.append({\n",
        "                    'date': first_date,\n",
        "                    'portfolio_value': current_value,\n",
        "                    'original_account_value': original_value\n",
        "                })\n",
        "                start_idx = 1\n",
        "                is_first_file = False\n",
        "            else:\n",
        "                start_idx = 1\n",
        "\n",
        "            for idx in range(start_idx, len(temp)):\n",
        "                daily_return = temp.loc[idx, 'daily_return']\n",
        "                date = temp.loc[idx, 'date']\n",
        "                original_value = temp.loc[idx, 'account_value']\n",
        "                if pd.notna(daily_return):\n",
        "                    current_value *= (1 + daily_return)\n",
        "                    portfolio_tracking.append({\n",
        "                        'date': date,\n",
        "                        'portfolio_value': current_value,\n",
        "                        'original_account_value': original_value\n",
        "                    })\n",
        "        else:\n",
        "            print(f\"[Warning] File does not exist: {file_path}\")\n",
        "\n",
        "    df_portfolio = pd.DataFrame(portfolio_tracking)\n",
        "\n",
        "    # === Step 5: Plot Portfolio Value ===\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['portfolio_value'], label='Continuous Portfolio Value')\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['original_account_value'], label='Original (Resetting) Account Value', linestyle='--')\n",
        "    plt.title('Portfolio Value: Continuous vs Original')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Portfolio Value')\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(target_folder, \"portfolio_value_plot.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"[INFO] Portfolio value plot saved to: {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # === Step 6: Calculate Daily Returns ===\n",
        "    df_daily_return = df_portfolio.copy()\n",
        "    df_daily_return[\"daily_return\"] = df_daily_return[\"portfolio_value\"].pct_change()\n",
        "    df_daily_return = df_daily_return.infer_objects(copy=False)\n",
        "    df_daily_return.loc[0, \"daily_return\"] = 0.0\n",
        "    df_daily_return = df_daily_return[[\"date\", \"daily_return\"]]\n",
        "\n",
        "    # === Step 7: Save Daily Return CSV into Folder ===\n",
        "    csv_full_path = os.path.join(target_folder, output_csv_name)\n",
        "    df_daily_return.to_csv(csv_full_path, index=False)\n",
        "    print(f\"[INFO] Daily return saved to: {csv_full_path}\")\n",
        "    \n",
        "    # === Step 8: Merge Trade Action Files ===\n",
        "    print(\"[INFO] Merging trade action files...\")\n",
        "    action_files = sorted(glob.glob(os.path.join(RESULTS_DIR, \"actions_trade_ensemble_*.csv\")))\n",
        "\n",
        "    if action_files:\n",
        "        df_actions_list = [pd.read_csv(f) for f in action_files]\n",
        "        df_actions_merged = pd.concat(df_actions_list, ignore_index=True)\n",
        "\n",
        "        # Save merged actions\n",
        "        actions_output_path = os.path.join(target_folder, \"merged_trade_actions.csv\")\n",
        "        df_actions_merged.to_csv(actions_output_path, index=False)\n",
        "        print(f\"[INFO] Merged trade actions saved to: {actions_output_path}\")\n",
        "\n",
        "        # Optional: Basic Trade Stats\n",
        "        total_trades = (df_actions_merged.drop(columns=['date'], errors='ignore') != 0).sum().sum()\n",
        "        print(f\"[INFO] Total trades executed: {total_trades}\")\n",
        "\n",
        "    else:\n",
        "        print(\"[Warning] No trade action files found to merge.\")\n",
        "\n",
        "    # === Step 9: Move Directories into the Folder ===\n",
        "    dirs_to_move = [TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR]\n",
        "\n",
        "    for dir_path in dirs_to_move:\n",
        "        if os.path.exists(dir_path):\n",
        "            dest_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(dest_path):\n",
        "                shutil.rmtree(dest_path)  \n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[Warning] Directory not found: {dir_path}\")\n",
        "\n",
        "    return df_daily_return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DRL Ensemble Strategy Workflow\n",
        "\n",
        "This part processes datasets, initializes DRL ensemble agents, runs the ensemble trading strategy, and generates daily returns for three different datasets.\n",
        "\n",
        "### **Workflow Overview**\n",
        "For each dataset:\n",
        "1. **Process Data**  \n",
        "   Apply feature engineering using `process_csv_to_features()`.\n",
        "\n",
        "2. **Setup DRL Ensemble Agent**  \n",
        "   Configure the agent with `setup_drl_ensemble_agent()`.\n",
        "\n",
        "3. **Run Ensemble Strategy & Generate Daily Returns**  \n",
        "   Execute `run_ensemble_and_generate_daily_return()` to:\n",
        "   - Train models (A2C, PPO, DDPG, SAC, TD3)\n",
        "   - Track portfolio value\n",
        "   - Calculate and save daily returns\n",
        "   - Organize outputs\n",
        "\n",
        "---\n",
        "\n",
        "### **Datasets Processed**\n",
        "1. `2007-2025_no_crypto.csv`  \n",
        "   - **Train**: 2007-06-01 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "2. `2015-2025_crypto.csv`  \n",
        "   - **Train**: 2015-02-02 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "3. `2015-2025_no_crypto.csv`  \n",
        "   - **Train**: 2015-02-02 to 2023-01-03  \n",
        "   - **Trade**: 2023-01-04 to 2025-04-11  \n",
        "\n",
        "---\n",
        "\n",
        "### **Outputs**\n",
        "- Daily return CSV: `df_daily_return_ensemble.csv` (saved in dataset-specific folders)\n",
        "- Portfolio value plots\n",
        "- Organized directories for models, logs, and results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 490        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 5.76       |\n",
            "|    reward             | -0.9636925 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.449      |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 511           |\n",
            "|    iterations         | 200           |\n",
            "|    time_elapsed       | 1             |\n",
            "|    total_timesteps    | 1000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -11.4         |\n",
            "|    explained_variance | -1.24         |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 199           |\n",
            "|    policy_loss        | -0.00893      |\n",
            "|    reward             | -0.0036511072 |\n",
            "|    std                | 1             |\n",
            "|    value_loss         | 0.00792       |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 472        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -1.87      |\n",
            "|    reward             | 0.22248401 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0652     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 472          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | -0.385       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -7.98        |\n",
            "|    reward             | -0.004219932 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.636        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 480       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 2.06      |\n",
            "|    reward             | 0.020234  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0346    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 485       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 2.36      |\n",
            "|    reward             | -0.174621 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0554    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 489       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.168     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 5.97      |\n",
            "|    reward             | -0.141318 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.306     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 491         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.268      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -19.5       |\n",
            "|    reward             | -0.40614972 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.61        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 492        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.00442   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -37.7      |\n",
            "|    reward             | 0.88688177 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 10.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 494       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.042     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 12.7      |\n",
            "|    reward             | -2.214606 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.29      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 494        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -8.34e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 22.4       |\n",
            "|    reward             | -0.9712234 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.87       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 495      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.0153   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 0.517    |\n",
            "|    reward             | 2.790371 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.626    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 495       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.454     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 7.77      |\n",
            "|    reward             | 0.9872257 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.627     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 495       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 23.9      |\n",
            "|    reward             | 1.202465  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 6.71      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 497       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -5.01     |\n",
            "|    reward             | 0.801109  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.207     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 496         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 16          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.472      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 14.2        |\n",
            "|    reward             | 0.012079294 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 2.02        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 497         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 1.2         |\n",
            "|    reward             | 0.044519085 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.106       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 494         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -5.64       |\n",
            "|    reward             | -0.32743073 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.29        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 494         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 19          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -0.0109     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 4.08        |\n",
            "|    reward             | 0.099239945 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.144       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 493      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | -70.6    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 2.66     |\n",
            "|    reward             | 0.020129 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.0612   |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.014783180818240786\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_2\n",
            "day: 3925, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1809088.31\n",
            "total_reward: 809088.31\n",
            "total_cost: 999.00\n",
            "total_trades: 27457\n",
            "Sharpe: 0.383\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.15480063690708284\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_2\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.23071966032398503\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_2\n",
            "day: 3925, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2002487.94\n",
            "total_reward: 1002487.94\n",
            "total_cost: 4665.36\n",
            "total_trades: 19903\n",
            "Sharpe: 0.352\n",
            "=================================\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.19055468043652446\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_2\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 544         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.27934384 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 521          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060746074 |\n",
            "|    clip_fraction        | 0.0779       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00844     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.41         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0054      |\n",
            "|    reward               | -0.42923823  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.21         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 527         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005280062 |\n",
            "|    clip_fraction        | 0.0489      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00357     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.91        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00692    |\n",
            "|    reward               | -1.1030195  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.67        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 538        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 15         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00921092 |\n",
            "|    clip_fraction        | 0.0602     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | 0.0215     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 2.08       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.00775   |\n",
            "|    reward               | -1.316173  |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 5.65       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 543         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008057329 |\n",
            "|    clip_fraction        | 0.0537      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0495      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.2         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    reward               | 1.0004021   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 19.4        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.007495181770570206\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 506        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.156      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 6.35       |\n",
            "|    reward             | -1.1310922 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.843      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 505        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 0.655      |\n",
            "|    reward             | 0.07842208 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0262     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 497        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 6.16       |\n",
            "|    reward             | -0.4436643 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.655      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 500         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.384       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -13.1       |\n",
            "|    reward             | -0.37017965 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.65        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 502        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0981     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 1.03       |\n",
            "|    reward             | 0.54034907 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.663      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 501        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.356      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 3.17       |\n",
            "|    reward             | -0.3625222 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.422      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 493       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0347    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -15.9     |\n",
            "|    reward             | -0.052195 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.69      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 493         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -1.97       |\n",
            "|    reward             | -0.02869938 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0439      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 494         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 20.4        |\n",
            "|    reward             | -0.54017025 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.9         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 496        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.118     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 10.7       |\n",
            "|    reward             | -1.0635319 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.06       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 497       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.68     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -13       |\n",
            "|    reward             | 0.5308984 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.23      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 498         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.0247     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 1.85        |\n",
            "|    reward             | -0.13966373 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.375       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 497        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.525     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -0.8       |\n",
            "|    reward             | 0.36943546 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0411     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 497        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.297     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -6.19      |\n",
            "|    reward             | -0.2812784 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.73       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 497        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -12.9      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -1.23      |\n",
            "|    reward             | -0.3703538 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.088      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 497         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 16          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | -6.11       |\n",
            "|    reward             | 0.104283996 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.312       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 498         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.125      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -8.35       |\n",
            "|    reward             | -0.16381596 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.46        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 499       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0612    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 5.36      |\n",
            "|    reward             | 1.1918912 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.453     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 500        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.00167   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -4.45      |\n",
            "|    reward             | 0.87336844 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.92       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 500        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0492     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -14.1      |\n",
            "|    reward             | -1.0520611 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.67       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.3258671044145653\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_2\n",
            "day: 3988, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2388227.94\n",
            "total_reward: 1388227.94\n",
            "total_cost: 1988.74\n",
            "total_trades: 19783\n",
            "Sharpe: 0.425\n",
            "=================================\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.1688751171959003\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_2\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.18165517210207846\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_2\n",
            "day: 3988, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1254125.90\n",
            "total_reward: 254125.90\n",
            "total_cost: 7015.76\n",
            "total_trades: 5018\n",
            "Sharpe: 0.202\n",
            "=================================\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.12186310540422965\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_2\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 594         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.48467174 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 563         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008020759 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0459     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.02        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00907    |\n",
            "|    reward               | -0.45985073 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.12        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 573         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007534716 |\n",
            "|    clip_fraction        | 0.0501      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0342      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.24        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00615    |\n",
            "|    reward               | 0.28850487  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 5.61        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 569          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.00696706   |\n",
            "|    clip_fraction        | 0.0836       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0591       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.452        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00967     |\n",
            "|    reward               | -0.022013303 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.19         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 574         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006432831 |\n",
            "|    clip_fraction        | 0.0646      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00148     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.367       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00697    |\n",
            "|    reward               | 0.21604963  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.1         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.005858169136567545\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.8       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 2.98       |\n",
            "|    reward             | -0.6198616 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0981     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 538          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | -6.31        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | -0.0462      |\n",
            "|    reward             | -0.071266755 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.0273       |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.159    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 0.369     |\n",
            "|    reward             | -0.017591 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0282    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 531        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.92      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -22.3      |\n",
            "|    reward             | -0.3342119 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.39       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.0888   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -1.95    |\n",
            "|    reward             | 1.036512 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.781    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.176      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -1.98      |\n",
            "|    reward             | 0.22484379 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.466      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 536          |\n",
            "|    iterations         | 700          |\n",
            "|    time_elapsed       | 6            |\n",
            "|    total_timesteps    | 3500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.6        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 699          |\n",
            "|    policy_loss        | -22.7        |\n",
            "|    reward             | -0.072656125 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 4.63         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -8.23       |\n",
            "|    reward             | -0.26129737 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.569       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0.0158    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -17.2     |\n",
            "|    reward             | 1.4823515 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 3.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 536        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.0715    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 8.17       |\n",
            "|    reward             | -0.8610401 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.62       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 537        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.121     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -30.1      |\n",
            "|    reward             | 0.19787945 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 6.5        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 537         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0.118       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -1.64       |\n",
            "|    reward             | -0.76265496 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.322       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0.00306   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 20        |\n",
            "|    reward             | 1.2866904 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.55      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 537      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -2.6     |\n",
            "|    reward             | 1.236268 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.434    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.222    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 32.4      |\n",
            "|    reward             | 0.9462698 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 11.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 536        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.0632     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -5.87      |\n",
            "|    reward             | -1.5003288 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.08       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 535         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 29.7        |\n",
            "|    reward             | -0.38047838 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 5.83        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -1.64     |\n",
            "|    reward             | 0.3320262 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.0865    |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 534          |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 17           |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.7        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | 0.0129       |\n",
            "|    reward             | -0.000581917 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 1.55e-06     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 534         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -0.0314     |\n",
            "|    reward             | -0.00214919 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 1.66e-05    |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.6922612673398318\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_2\n",
            "day: 4051, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1948378.56\n",
            "total_reward: 948378.56\n",
            "total_cost: 1157.26\n",
            "total_trades: 28357\n",
            "Sharpe: 0.328\n",
            "=================================\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.38026545277320206\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_2\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.5112686906557153\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_2\n",
            "day: 4051, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1484907.82\n",
            "total_reward: 484907.82\n",
            "total_cost: 5131.19\n",
            "total_trades: 20596\n",
            "Sharpe: 0.236\n",
            "=================================\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.49403992899104776\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_2\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 618         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.12915318 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 601         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006659752 |\n",
            "|    clip_fraction        | 0.0635      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0199      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.89        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00488    |\n",
            "|    reward               | -0.5266121  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.35        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 598          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055353018 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00967      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.25         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    reward               | -1.3982608   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.49         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 596          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063233366 |\n",
            "|    clip_fraction        | 0.073        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00237      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.47         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00695     |\n",
            "|    reward               | 0.60744303   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.44         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 596         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008348066 |\n",
            "|    clip_fraction        | 0.0778      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0468     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.37        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00862    |\n",
            "|    reward               | 0.17454489  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.508934525108003\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.37      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 4.11       |\n",
            "|    reward             | -0.9372921 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.382      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.0517    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 4.06       |\n",
            "|    reward             | 0.19418353 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 0.826      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 526        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.391     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 2.61       |\n",
            "|    reward             | -0.5510086 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.441      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 526         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -9.83       |\n",
            "|    reward             | -0.17206775 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.15        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 524        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -2.63      |\n",
            "|    reward             | 0.19967933 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.488      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 525         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 2.18        |\n",
            "|    reward             | -0.45563966 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.055       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 525        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0305    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 21         |\n",
            "|    reward             | -0.4389606 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.58       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 526        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.228     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -3.06      |\n",
            "|    reward             | -1.7674577 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.98       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 526         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.00269    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -27.4       |\n",
            "|    reward             | -0.24729298 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 11.2        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.295    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 5.4       |\n",
            "|    reward             | 2.8700767 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.32      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.351     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -1.08     |\n",
            "|    reward             | 0.6395822 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.394     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 527         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0196      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 18.9        |\n",
            "|    reward             | -0.23896827 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.22        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 527        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -32.5      |\n",
            "|    reward             | -0.5647519 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.68       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.273     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -3.29      |\n",
            "|    reward             | 0.03154824 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.587      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0171   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 6.93      |\n",
            "|    reward             | -7.928643 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.71      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 528         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0457     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 11.1        |\n",
            "|    reward             | -0.56425416 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.53        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.106   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 24.8     |\n",
            "|    reward             | 3.496685 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 5.88     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 527       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.019    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 14.2      |\n",
            "|    reward             | 1.3007088 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.94      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0307     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -20.3      |\n",
            "|    reward             | 0.87864906 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.18       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.414     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 11.7      |\n",
            "|    reward             | 1.4411377 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.09      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.21452973940725056\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_2\n",
            "day: 4114, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 957444.59\n",
            "total_reward: -42555.41\n",
            "total_cost: 998.99\n",
            "total_trades: 16456\n",
            "Sharpe: 0.013\n",
            "=================================\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.43043642136085747\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_2\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3228232902469571\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_2\n",
            "day: 4114, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1571400.44\n",
            "total_reward: 571400.44\n",
            "total_cost: 4454.21\n",
            "total_trades: 12841\n",
            "Sharpe: 0.236\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.3719586878752957\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_2\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 594         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.32709515 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 587          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060970285 |\n",
            "|    clip_fraction        | 0.0681       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00441     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.31         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00662     |\n",
            "|    reward               | -0.29750726  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.68         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072641466 |\n",
            "|    clip_fraction        | 0.0557       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00515     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 6.17         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00716     |\n",
            "|    reward               | -0.025380675 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.64         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068503898 |\n",
            "|    clip_fraction        | 0.0635       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0898      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.469        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00646     |\n",
            "|    reward               | -0.08524354  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.12         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 575         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008511625 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0144     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.12        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    reward               | 0.1209438   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 5.31        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.2786528529892725\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_2\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 379         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 9.97        |\n",
            "|    reward             | -0.82946587 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 1.05        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 430         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0367      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 2.85        |\n",
            "|    reward             | 0.017853662 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.157       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 418         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.105       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | -1.16       |\n",
            "|    reward             | 0.028859625 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0169      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 425          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0.275        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -5.49        |\n",
            "|    reward             | -0.008044439 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.305        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 432       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 1.64      |\n",
            "|    reward             | 0.01629   |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0435    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 442         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0366      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 2.14        |\n",
            "|    reward             | -0.19550832 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0554      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 453         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 0.639       |\n",
            "|    reward             | -0.22694783 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0186      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 462       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -5.1      |\n",
            "|    reward             | -0.138414 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.202     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 469         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -0.13       |\n",
            "|    reward             | -0.06612445 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.00534     |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 475      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -5.82    |\n",
            "|    reward             | -0.23615 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.254    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 481      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    reward             | 0.085014 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.0328   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 486      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -1.5     |\n",
            "|    reward             | 0.009446 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.0407   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 489       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 1.84      |\n",
            "|    reward             | -0.236125 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.0555    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 493      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.39    |\n",
            "|    reward             | 0.144996 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.0158   |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 496         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -0.62       |\n",
            "|    reward             | -0.31781754 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.0103      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 497       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 9.05      |\n",
            "|    reward             | -1.098404 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.562     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 498        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | -0.725     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -9.55      |\n",
            "|    reward             | -0.2372012 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 2.13       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 499       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -2.67     |\n",
            "|    reward             | -0.000103 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.0558    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 501      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -1.43    |\n",
            "|    reward             | 0.172854 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 0.0618   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 503        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -2.61      |\n",
            "|    reward             | 0.13339439 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.0896     |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.10870423477402676\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_2\n",
            "day: 4177, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1727778.24\n",
            "total_reward: 727778.24\n",
            "total_cost: 998.99\n",
            "total_trades: 16658\n",
            "Sharpe: 0.258\n",
            "=================================\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.18647765654249757\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_2\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.29274271321857653\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_2\n",
            "day: 4177, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2042645.93\n",
            "total_reward: 1042645.93\n",
            "total_cost: 4919.47\n",
            "total_trades: 13284\n",
            "Sharpe: 0.368\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.36226274009861426\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_2\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 625        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.3776154 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 605          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076220296 |\n",
            "|    clip_fraction        | 0.0621       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0672      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.44         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00673     |\n",
            "|    reward               | -0.35095188  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.7          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 602        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00955875 |\n",
            "|    clip_fraction        | 0.0675     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.00159   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 11.4       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.00768   |\n",
            "|    reward               | -0.8087649 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 24.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 602         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010855388 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.000927    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.09        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | -0.6215579  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 4.86        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 600         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006482026 |\n",
            "|    clip_fraction        | 0.036       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.025      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.19        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    reward               | 0.8137646   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.3942723421716754\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 528        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.334     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 10.5       |\n",
            "|    reward             | -0.7634234 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 530         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.106      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -2.85       |\n",
            "|    reward             | -0.24659073 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 0.873       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 532         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 7.36        |\n",
            "|    reward             | -0.43953305 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 1.36        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 533         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -19.1       |\n",
            "|    reward             | -0.23685312 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 3.51        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 0.901     |\n",
            "|    reward             | 0.8202697 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 0.589     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.041      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -17.3      |\n",
            "|    reward             | 0.19897287 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 2.22       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -7.9       |\n",
            "|    reward             | 0.24557263 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.41       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 2.18       |\n",
            "|    reward             | -1.1567739 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.339      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -22.2      |\n",
            "|    reward             | -0.6047212 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.22       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 534       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 18.3      |\n",
            "|    reward             | -1.112228 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.91      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.000911  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -5.36      |\n",
            "|    reward             | -1.5288888 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 1.4        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 535        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -5.88      |\n",
            "|    reward             | 0.22256233 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.397      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 535        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.307     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -14        |\n",
            "|    reward             | 0.77515084 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.73       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 7.81     |\n",
            "|    reward             | 1.310842 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 0.527    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 535         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -11.2       |\n",
            "|    reward             | -0.27450863 |\n",
            "|    std                | 0.994       |\n",
            "|    value_loss         | 3.27        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 7.26e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 3.55      |\n",
            "|    reward             | -3.972578 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 2.52      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -0.983    |\n",
            "|    reward             | 0.1455244 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 0.00792   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 535        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -25.5      |\n",
            "|    reward             | -2.4749646 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 5.18       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 535       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -13       |\n",
            "|    reward             | -1.946082 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 1.47      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 536        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -29.6      |\n",
            "|    reward             | -0.5589063 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 7.01       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.29501893041824484\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_2\n",
            "day: 4240, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1940037.02\n",
            "total_reward: 940037.02\n",
            "total_cost: 998.99\n",
            "total_trades: 16960\n",
            "Sharpe: 0.414\n",
            "=================================\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.5271425383352856\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_2\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.051689226153857966\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_2\n",
            "day: 4240, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3061017.15\n",
            "total_reward: 2061017.15\n",
            "total_cost: 5353.46\n",
            "total_trades: 13324\n",
            "Sharpe: 0.490\n",
            "=================================\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.05807434518959782\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_2\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 602         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.68127537 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 589          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073303115 |\n",
            "|    clip_fraction        | 0.0817       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00889      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.36         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00749     |\n",
            "|    reward               | -0.068751246 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.03         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 586        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00949616 |\n",
            "|    clip_fraction        | 0.087      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | 0.00927    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 10.3       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0057    |\n",
            "|    reward               | 0.10076772 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 20.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010150102 |\n",
            "|    clip_fraction        | 0.0996      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0108      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.757       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    reward               | -0.34562302 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.47        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 578         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006243363 |\n",
            "|    clip_fraction        | 0.0587      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | -0.0383     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.78        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00671    |\n",
            "|    reward               | -0.35777897 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 3.64        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  -0.06000761313352827\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_2\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 506        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 8.2        |\n",
            "|    reward             | -1.0845155 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1.06       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 511        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.44      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 9.42       |\n",
            "|    reward             | 0.23406565 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.71       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 509       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.152     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 7.23      |\n",
            "|    reward             | -0.758351 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 1.18      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 511         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0.0944      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -25.6       |\n",
            "|    reward             | -0.22467898 |\n",
            "|    std                | 0.989       |\n",
            "|    value_loss         | 5.82        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 513       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -7.39     |\n",
            "|    reward             | 1.1473079 |\n",
            "|    std                | 0.987     |\n",
            "|    value_loss         | 0.88      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 513       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0755    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -9.76     |\n",
            "|    reward             | 0.3557273 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 1.16      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 514      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -24.2    |\n",
            "|    reward             | 0.075391 |\n",
            "|    std                | 0.989    |\n",
            "|    value_loss         | 5.41     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 513         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 7.27        |\n",
            "|    reward             | -0.20521937 |\n",
            "|    std                | 0.987       |\n",
            "|    value_loss         | 1.86        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 512        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 2.38e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -12.7      |\n",
            "|    reward             | -0.7562894 |\n",
            "|    std                | 0.984      |\n",
            "|    value_loss         | 2.86       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 513         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 21          |\n",
            "|    reward             | 0.102230094 |\n",
            "|    std                | 0.983       |\n",
            "|    value_loss         | 2.66        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 514        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -5.44      |\n",
            "|    reward             | -0.6434414 |\n",
            "|    std                | 0.983      |\n",
            "|    value_loss         | 0.484      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 514        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 9.12       |\n",
            "|    reward             | 0.41025496 |\n",
            "|    std                | 0.988      |\n",
            "|    value_loss         | 1.88       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 514      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 8.22     |\n",
            "|    reward             | 2.520621 |\n",
            "|    std                | 0.985    |\n",
            "|    value_loss         | 3.94     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 515       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0.0125    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -56.9     |\n",
            "|    reward             | 3.3503454 |\n",
            "|    std                | 0.985     |\n",
            "|    value_loss         | 46.7      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 509      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 22       |\n",
            "|    reward             | 1.723528 |\n",
            "|    std                | 0.98     |\n",
            "|    value_loss         | 7.71     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 509       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 0.000286  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -84.6     |\n",
            "|    reward             | -6.094564 |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 75.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 510      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 21.4     |\n",
            "|    reward             | 3.883516 |\n",
            "|    std                | 0.981    |\n",
            "|    value_loss         | 6.1      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 510        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.2      |\n",
            "|    explained_variance | 0.263      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -4.62      |\n",
            "|    reward             | -1.2237226 |\n",
            "|    std                | 0.981      |\n",
            "|    value_loss         | 1.92       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 511          |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | 10.7         |\n",
            "|    reward             | -0.016905906 |\n",
            "|    std                | 0.976        |\n",
            "|    value_loss         | 2.87         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 511       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -2.58     |\n",
            "|    reward             | 0.3986215 |\n",
            "|    std                | 0.981     |\n",
            "|    value_loss         | 0.203     |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.1735969004341485\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_2\n",
            "day: 4303, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2301548.89\n",
            "total_reward: 1301548.89\n",
            "total_cost: 999.00\n",
            "total_trades: 12909\n",
            "Sharpe: 0.399\n",
            "=================================\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.14970880279830698\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_2\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.12797720290379674\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_2\n",
            "day: 4303, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1349302.02\n",
            "total_reward: 349302.02\n",
            "total_cost: 5729.45\n",
            "total_trades: 20651\n",
            "Sharpe: 0.189\n",
            "=================================\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.4495506855112066\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_2\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 600        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.2940361 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 582          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.007471478  |\n",
            "|    clip_fraction        | 0.0901       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.131       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.48         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0121      |\n",
            "|    reward               | -0.062779784 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.83         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 560         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009976933 |\n",
            "|    clip_fraction        | 0.0934      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00625    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.35        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00763    |\n",
            "|    reward               | -1.9437306  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 15.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 560         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006694254 |\n",
            "|    clip_fraction        | 0.054       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0167     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.72        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0066     |\n",
            "|    reward               | 1.1499777   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 6.92        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 561         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008037495 |\n",
            "|    clip_fraction        | 0.0579      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0131     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.56        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00919    |\n",
            "|    reward               | 0.65431005  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 23.1        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.11184971262406518\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_2\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 516         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0325     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 19.2        |\n",
            "|    reward             | -0.92581564 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.36        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 520          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | -0.602       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 4.33         |\n",
            "|    reward             | 0.0020138188 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.47         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 520         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 7.59        |\n",
            "|    reward             | -0.69986105 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.16        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.083     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -8.84     |\n",
            "|    reward             | 0.1620725 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.732     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.166      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -1.39      |\n",
            "|    reward             | 0.64730436 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.146      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0246     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -1.67      |\n",
            "|    reward             | 0.10628007 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.287      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 521         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0314      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -2.52       |\n",
            "|    reward             | -0.10610905 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.996       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 521          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -0.0743      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | 5.01         |\n",
            "|    reward             | -0.100066744 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 1.69         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.253    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 18.3      |\n",
            "|    reward             | 0.6703917 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 6.68      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 521        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.633     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 14.3       |\n",
            "|    reward             | -0.7066421 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.99       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 521       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0543    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -29.5     |\n",
            "|    reward             | 2.6247914 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.98      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.38      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 18.7       |\n",
            "|    reward             | 0.20588769 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.44       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 518      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.108    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -16.8    |\n",
            "|    reward             | 1.669039 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 518       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.726     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 6.58      |\n",
            "|    reward             | 0.3000829 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.315     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 519         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.227       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | 7.22        |\n",
            "|    reward             | -0.28044942 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.431       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 519         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0724      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | -10.8       |\n",
            "|    reward             | -0.17892677 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.46        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.000421 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -22.2     |\n",
            "|    reward             | 1.4483105 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 8.01      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 519         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0065      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -3.39       |\n",
            "|    reward             | -0.88652486 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.01        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 519         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.0718     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -2.8        |\n",
            "|    reward             | -0.30985373 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.06        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 519        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0357     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -4.37      |\n",
            "|    reward             | 0.16199529 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.128      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.03859147359271851\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_2\n",
            "day: 4366, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1952624.25\n",
            "total_reward: 952624.25\n",
            "total_cost: 1303.13\n",
            "total_trades: 26034\n",
            "Sharpe: 0.305\n",
            "=================================\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.07863305197308505\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_2\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.04404815009840411\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_2\n",
            "day: 4366, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2063271.17\n",
            "total_reward: 1063271.17\n",
            "total_cost: 5658.34\n",
            "total_trades: 13705\n",
            "Sharpe: 0.386\n",
            "=================================\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.015635071812607662\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_2\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 590        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.3175003 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007989259 |\n",
            "|    clip_fraction        | 0.0723      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00279    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.65        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0082     |\n",
            "|    reward               | 0.009772368 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.9         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 568          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008135971  |\n",
            "|    clip_fraction        | 0.0576       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00548     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.77         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00771     |\n",
            "|    reward               | -0.055796813 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 12.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 567         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009564178 |\n",
            "|    clip_fraction        | 0.0797      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0307      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.43        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | 3.2371922   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.7         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 564          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046689957 |\n",
            "|    clip_fraction        | 0.0236       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00857     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 10.8         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00434     |\n",
            "|    reward               | -0.3079088   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 19.8         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.18671319092408337\n",
            "======Best Model Retraining from:  2007-06-01 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  26.43622929652532  minutes\n",
            "[INFO] Portfolio value plot saved to: 2007-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2007-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2007-2025_no_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 1315\n",
            "[INFO] Moved trained_models to 2007-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2007-2025_no_crypto/\n",
            "[INFO] Moved results to 2007-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_0 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_0,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2007-06-01',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_0,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_0,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2007-2025_no_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 762         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.507      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -3.01       |\n",
            "|    reward             | -0.27371314 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.238       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 764        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0978    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 15.4       |\n",
            "|    reward             | 0.12076885 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.23       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0228   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 64.2      |\n",
            "|    reward             | -2.052734 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 35.2      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 768          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -52.5        |\n",
            "|    reward             | -0.034977082 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 20.5         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 770         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.0137      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 14.1        |\n",
            "|    reward             | 0.085579224 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.92        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 770       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -4.79     |\n",
            "|    reward             | 1.2114694 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.29      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0358     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -1.79      |\n",
            "|    reward             | -0.5426695 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.76       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -2.45      |\n",
            "|    reward             | 0.25193712 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0407     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 770       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0481    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -6.14     |\n",
            "|    reward             | 1.4135938 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.744     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 768        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0284     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 30.3       |\n",
            "|    reward             | -0.4591123 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 6.51       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 768      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.193   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 7.55     |\n",
            "|    reward             | 4.114836 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 767         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 12.1        |\n",
            "|    reward             | -0.10552003 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1           |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 766          |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0.424        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | 12.3         |\n",
            "|    reward             | -0.071214944 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.19         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 767        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.141      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 7.31       |\n",
            "|    reward             | 0.42410398 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.816      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 767      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0179   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 8.44     |\n",
            "|    reward             | 7.674653 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 767         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | -1.6        |\n",
            "|    reward             | -0.11541499 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0185      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 767        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.96       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 5.12       |\n",
            "|    reward             | 0.20941404 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.204      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 767         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.0559      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 11.7        |\n",
            "|    reward             | -0.29519683 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.25        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 768         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 55          |\n",
            "|    reward             | -0.75926626 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 20.6        |\n",
            "---------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1477242.17\n",
            "total_reward: 477242.17\n",
            "total_cost: 37828.08\n",
            "total_trades: 11350\n",
            "Sharpe: 0.349\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 768        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 8.86       |\n",
            "|    reward             | -1.0344019 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.661      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.4367357360849435\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1505364.62\n",
            "total_reward: 505364.62\n",
            "total_cost: 2521.85\n",
            "total_trades: 4049\n",
            "Sharpe: 0.348\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 263       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -13.2     |\n",
            "|    critic_loss     | 884       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.852622 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.0521897963361444\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 17740292.81\n",
            "total_reward: 16740292.81\n",
            "total_cost: 999.00\n",
            "total_trades: 13958\n",
            "Sharpe: 0.904\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 219      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 678      |\n",
            "|    critic_loss     | 1.87e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 6.85445  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.4427132531839712\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 971190.86\n",
            "total_reward: -28809.14\n",
            "total_cost: 105016.24\n",
            "total_trades: 14020\n",
            "Sharpe: 0.071\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 177      |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 962      |\n",
            "|    critic_loss     | 8.97e+03 |\n",
            "|    ent_coef        | 0.0737   |\n",
            "|    ent_coef_loss   | -33.4    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 0.346977 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.4476010027067173\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3336163.99\n",
            "total_reward: 2336163.99\n",
            "total_cost: 1014559.01\n",
            "total_trades: 17309\n",
            "Sharpe: 0.667\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 900       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.3162977 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 886         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007427941 |\n",
            "|    clip_fraction        | 0.0904      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 4.02e-05    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 229         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00834    |\n",
            "|    reward               | 0.1316847   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 479         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 876         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009307576 |\n",
            "|    clip_fraction        | 0.0937      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0134     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37.9        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00805    |\n",
            "|    reward               | -0.70801705 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 80.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 871          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077292263 |\n",
            "|    clip_fraction        | 0.0873       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.0115       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 24.1         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00668     |\n",
            "|    reward               | -0.5400236   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 49.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 859         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009657795 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00131     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 34          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0097     |\n",
            "|    reward               | 0.87045604  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 58          |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.06639594768969467\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 706      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0156  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 92.4     |\n",
            "|    reward             | 2.030192 |\n",
            "|    std                | 0.993    |\n",
            "|    value_loss         | 66.2     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 712        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.0277     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -212       |\n",
            "|    reward             | -1.5321157 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 423        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 689        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.000394  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 4.56e+03   |\n",
            "|    reward             | -130.23944 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 1.43e+05   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 697      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.00981  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 200      |\n",
            "|    reward             | 32.00031 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 277      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 702      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.129   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 85.6     |\n",
            "|    reward             | 7.94279  |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 33.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 705      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.00224  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -24.4    |\n",
            "|    reward             | 1.363111 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 623      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 707      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.00521  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 645      |\n",
            "|    reward             | 43.90123 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 6.16e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 708       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -2.19e+03 |\n",
            "|    reward             | -46.61835 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.51e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 707      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0218   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -202     |\n",
            "|    reward             | 2.321024 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 348      |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 710         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.0127      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 484         |\n",
            "|    reward             | -111.173454 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 3.62e+03    |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 710      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.0128  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 460      |\n",
            "|    reward             | 54.4387  |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 3.18e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 710       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -2e-05    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 3.84e+03  |\n",
            "|    reward             | 366.00433 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 1.97e+05  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 711         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.0402     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 5.65        |\n",
            "|    reward             | -0.20932269 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.476       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 712        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.416     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 29.8       |\n",
            "|    reward             | -0.3218557 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 6.37       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 713       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0263   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -41.1     |\n",
            "|    reward             | 4.2109976 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 27.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 713        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0455     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -91.2      |\n",
            "|    reward             | 0.95654476 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 60.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 714        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.00603    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 66.8       |\n",
            "|    reward             | 0.42774987 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 41.4       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 713        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.121     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 0.705      |\n",
            "|    reward             | -0.7070121 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 14.1       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 714      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0204   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 29.5     |\n",
            "|    reward             | 0.42388  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 17       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 714      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 173      |\n",
            "|    reward             | 3.232077 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 188      |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.08053015263899625\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 252      |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 43.2     |\n",
            "|    critic_loss     | 7.19e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 42.2013  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.0965709076110018\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 230       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 733       |\n",
            "|    critic_loss     | 2.57e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -0.402935 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.24642497163497148\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 181      |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 606      |\n",
            "|    critic_loss     | 1.37e+03 |\n",
            "|    ent_coef        | 0.052    |\n",
            "|    ent_coef_loss   | -22.6    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.254556 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.040386990343625445\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 929        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -10.887084 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 883         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008416254 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000286    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 271         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00736    |\n",
            "|    reward               | -2.0179193  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 607         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 880        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00841116 |\n",
            "|    clip_fraction        | 0.0658     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.8      |\n",
            "|    explained_variance   | 0.00113    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 328        |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.00763   |\n",
            "|    reward               | -0.6831829 |\n",
            "|    std                  | 0.999      |\n",
            "|    value_loss           | 527        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 878         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009185825 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.0113      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.5        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00786    |\n",
            "|    reward               | 3.1023984   |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 33.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 877         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008155515 |\n",
            "|    clip_fraction        | 0.0817      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.7       |\n",
            "|    explained_variance   | 0.000255    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 47.9        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00796    |\n",
            "|    reward               | -0.16265433 |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.10018357374084287\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -3.97      |\n",
            "|    reward             | -0.3896831 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.149      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.203   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 12       |\n",
            "|    reward             | -0.00755 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 746        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.216     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 33.1       |\n",
            "|    reward             | -0.9904576 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 10.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 28.9      |\n",
            "|    reward             | 0.6053992 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.4       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 748         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 8.4         |\n",
            "|    reward             | 0.030392472 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.451       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 710       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.576    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -9.38     |\n",
            "|    reward             | -0.212092 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.911     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 5.71      |\n",
            "|    reward             | 1.2230502 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.381     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0439   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -3.54     |\n",
            "|    reward             | -1.110413 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.314     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.41       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -2.75      |\n",
            "|    reward             | 0.49971104 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.217      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 728      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.0879  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -8.25    |\n",
            "|    reward             | 0.210754 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.747    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 731         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 2.12        |\n",
            "|    reward             | -0.08672687 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0354      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 733       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -36.8     |\n",
            "|    reward             | -1.269203 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 10.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 734        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.4       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 5          |\n",
            "|    reward             | -0.3664287 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.458      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 735      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | -0.0876  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -6.37    |\n",
            "|    reward             | 0.180169 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.373    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 737      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.0378  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -36      |\n",
            "|    reward             | 2.333    |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 738      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.0879   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -12.2    |\n",
            "|    reward             | 0.959793 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 738         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 2.09        |\n",
            "|    reward             | 0.013368567 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0565      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 739         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 1.01e-06    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 9.01        |\n",
            "|    reward             | -0.02848208 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.453       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 739      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 5.36e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -5.84    |\n",
            "|    reward             | 0.277434 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.355    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.00108   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 27.1       |\n",
            "|    reward             | 0.66074455 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 9.52       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3219924286055681\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 271       |\n",
            "|    time_elapsed    | 31        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 181       |\n",
            "|    critic_loss     | 5.33e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -66.83184 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.14543681653935606\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 225       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 790       |\n",
            "|    critic_loss     | 5.61e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -90.65846 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.1388613018261818\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 172       |\n",
            "|    time_elapsed    | 49        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.55e+03  |\n",
            "|    critic_loss     | 6.97e+03  |\n",
            "|    ent_coef        | 1.1       |\n",
            "|    ent_coef_loss   | -8.38     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.383524 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.510346444118816\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 869        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -23.577173 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 827         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006104958 |\n",
            "|    clip_fraction        | 0.0685      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00143     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 873         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00441    |\n",
            "|    reward               | -1.5058111  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.98e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 821         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009604083 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0311     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.5        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | -0.6229069  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 81.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 777        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00832185 |\n",
            "|    clip_fraction        | 0.117      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12.8      |\n",
            "|    explained_variance   | -0.00782   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 35.9       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.0104    |\n",
            "|    reward               | -3.3433366 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 80.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 784         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008636662 |\n",
            "|    clip_fraction        | 0.0944      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000265    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 44          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00635    |\n",
            "|    reward               | -7.845096   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 89.8        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.31967568795040124\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 651         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.143      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 7.01        |\n",
            "|    reward             | -0.08704545 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.895       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 684          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0.00166      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 11.2         |\n",
            "|    reward             | -0.013309736 |\n",
            "|    std                | 0.999        |\n",
            "|    value_loss         | 1.02         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 701        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.172      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 28         |\n",
            "|    reward             | -0.2561651 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 6.62       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 707       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 25.1      |\n",
            "|    reward             | 2.9090192 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 5.17      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 710        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.13       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 1.2        |\n",
            "|    reward             | 0.66090226 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.237      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 712        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.271      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -16.8      |\n",
            "|    reward             | -0.4280824 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 2.05       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 712       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0607   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 0.00808   |\n",
            "|    reward             | 0.3667483 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 5.17      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 714       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.68     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -14.3     |\n",
            "|    reward             | 0.8432178 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 1.5       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 715        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -1.51      |\n",
            "|    reward             | 0.13976008 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 0.0536     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 717         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -1.2        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 0.00198     |\n",
            "|    reward             | 0.033417713 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.0167      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 716       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.422    |\n",
            "|    reward             | -0.169668 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.0136    |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 716          |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | -1.42        |\n",
            "|    reward             | -0.087529115 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.0144       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 716        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 2.87       |\n",
            "|    reward             | 0.13975155 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0979     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 715        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.0559     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -5.76      |\n",
            "|    reward             | -0.7926308 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.256      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -13.6      |\n",
            "|    reward             | 0.21736123 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.64       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 718         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 2.27        |\n",
            "|    reward             | -0.22861595 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0426      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 0.75       |\n",
            "|    reward             | 0.03231186 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.00681    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.0799     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -25.4      |\n",
            "|    reward             | 0.27342984 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.25       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.416      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -9.79      |\n",
            "|    reward             | 0.14400665 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.693      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 718        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -4.38      |\n",
            "|    reward             | 0.15015712 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0916     |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.4137698942012713\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 248      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 276      |\n",
            "|    critic_loss     | 2.92e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.749162 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.40981804933305394\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 232      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -669     |\n",
            "|    critic_loss     | 2.52e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.204232 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3249881999568395\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 177      |\n",
            "|    time_elapsed    | 49       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.15e+03 |\n",
            "|    critic_loss     | 227      |\n",
            "|    ent_coef        | 1.3      |\n",
            "|    ent_coef_loss   | -27.9    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.51261  |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.41792590838930177\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 899        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -4.4456286 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 784         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010001372 |\n",
            "|    clip_fraction        | 0.0796      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00236     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 193         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00782    |\n",
            "|    reward               | -1.8785266  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 469         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 788          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067658024 |\n",
            "|    clip_fraction        | 0.0771       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00203      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 393          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00756     |\n",
            "|    reward               | -0.5291503   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 964          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 801         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009840208 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0357     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 40.4        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00796    |\n",
            "|    reward               | -1.169517   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 71.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 804         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007103945 |\n",
            "|    clip_fraction        | 0.0784      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00672     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 93.1        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00835    |\n",
            "|    reward               | 16.850786   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 143         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.3144604091490075\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 722         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.28       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -8          |\n",
            "|    reward             | -0.31713688 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.498       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 720         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 1.25        |\n",
            "|    reward             | -0.15246513 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.118       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 11.1      |\n",
            "|    reward             | -1.687816 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.47      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.179     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | 34         |\n",
            "|    reward             | 0.08249402 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 10         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.336      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 7.17       |\n",
            "|    reward             | 0.63740814 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.804      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 723        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0985    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 9.69       |\n",
            "|    reward             | -0.8380947 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.525      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.245     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 19.3       |\n",
            "|    reward             | -0.9905199 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.35       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 725        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0164     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -40.2      |\n",
            "|    reward             | -4.3073654 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 19.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 725       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.775    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -73.5     |\n",
            "|    reward             | 0.1494546 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 50.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0158    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 2.14      |\n",
            "|    reward             | 1.2962356 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.733     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 727      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0221   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 2.07     |\n",
            "|    reward             | 0.826376 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.819    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 727        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.00453    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 18.5       |\n",
            "|    reward             | -1.5873746 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.89       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 71.9      |\n",
            "|    reward             | 2.9707265 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 37.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 2.17      |\n",
            "|    reward             | -1.096721 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.947     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 728      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.0613   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -22.9    |\n",
            "|    reward             | 1.295702 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 729      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0.00847  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -43.8    |\n",
            "|    reward             | 1.207212 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 2.38e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -92.2     |\n",
            "|    reward             | -1.958041 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 57.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 728         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -3.19       |\n",
            "|    reward             | -0.04569535 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0775      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.205     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 4.65      |\n",
            "|    reward             | 0.4595896 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.274     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.121     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 16.6       |\n",
            "|    reward             | 0.22370063 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.55       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.27763585015442044\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 251       |\n",
            "|    time_elapsed    | 35        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 448       |\n",
            "|    critic_loss     | 1.38e+04  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 186.03722 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.38862821785909685\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 233      |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 8988     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 338      |\n",
            "|    critic_loss     | 700      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8887     |\n",
            "|    reward          | -0.29022 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.47912957922979843\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 185      |\n",
            "|    time_elapsed    | 48       |\n",
            "|    total_timesteps | 8988     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.87e+03 |\n",
            "|    critic_loss     | 4.24e+04 |\n",
            "|    ent_coef        | 1.34     |\n",
            "|    ent_coef_loss   | -24.5    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8887     |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.0\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 882        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -11.572349 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 857         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011655709 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00238    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 130         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00974    |\n",
            "|    reward               | 3.6965597   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 264         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 837         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011535037 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0032     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 62.2        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00834    |\n",
            "|    reward               | 0.33204865  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 117         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 834         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011858275 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00682     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | 0.038714923 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 38.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 833         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011603883 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.0195      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.1        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    reward               | 0.33846834  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 15.8        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.38054948355085516\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 680         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.29       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -1.26       |\n",
            "|    reward             | 0.023080185 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0925      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 686         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.405      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 1.91        |\n",
            "|    reward             | 0.104798906 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0391      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 696       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.1      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 9.3       |\n",
            "|    reward             | -0.210228 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.782     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.975    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 13.2      |\n",
            "|    reward             | 0.7586293 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 700       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.407    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 18.6      |\n",
            "|    reward             | 0.8050127 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.44      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 698        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.19       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -0.115     |\n",
            "|    reward             | 0.08658697 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.00858    |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 696      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 14.8     |\n",
            "|    reward             | 0.341271 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 698      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0.0175   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -10.8    |\n",
            "|    reward             | 0.568104 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 699        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 24.7       |\n",
            "|    reward             | -0.9293906 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 3.93       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 698        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -1.89      |\n",
            "|    reward             | 0.10984254 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0489     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 699        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.739     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 6.91       |\n",
            "|    reward             | 0.65963084 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.414      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 698       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 27.9      |\n",
            "|    reward             | 1.118512  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 5.21      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -47.1     |\n",
            "|    reward             | 1.92156   |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 698        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.651     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -18.6      |\n",
            "|    reward             | 0.01650098 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.38       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 698       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 0.222     |\n",
            "|    reward             | -0.019148 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0483    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 698      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 8.16     |\n",
            "|    reward             | 1.36458  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.571    |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 698          |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.9        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | 5.2          |\n",
            "|    reward             | -0.057473347 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.36         |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 700      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 17.8     |\n",
            "|    reward             | 0.205927 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 701       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -26.7     |\n",
            "|    reward             | 1.8041788 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.59      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 701      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.011    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -14.6    |\n",
            "|    reward             | -0.69865 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.45623806441891307\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 265        |\n",
            "|    time_elapsed    | 34         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -721       |\n",
            "|    critic_loss     | 4.32e+04   |\n",
            "|    learning_rate   | 0.0005     |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -147.99236 |\n",
            "-----------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.22851620770012307\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 233       |\n",
            "|    time_elapsed    | 39        |\n",
            "|    total_timesteps | 9240      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 555       |\n",
            "|    critic_loss     | 2.7e+04   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9139      |\n",
            "|    reward          | -99.50984 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  -0.22906828260285342\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 186        |\n",
            "|    time_elapsed    | 49         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 3.58e+03   |\n",
            "|    critic_loss     | 8.76e+04   |\n",
            "|    ent_coef        | 0.0862     |\n",
            "|    ent_coef_loss   | -14.2      |\n",
            "|    learning_rate   | 0.0003     |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -145.30376 |\n",
            "-----------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  -0.2293103974478461\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 857        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -14.398424 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 805         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007242193 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000481    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 372         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00991    |\n",
            "|    reward               | -17.189938  |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 857         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 785         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008450095 |\n",
            "|    clip_fraction        | 0.0542      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000446   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 275         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00567    |\n",
            "|    reward               | -17.437323  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 590         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 780         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011105386 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00206    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 430         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00881    |\n",
            "|    reward               | 4.102968    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 959         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 778         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011813583 |\n",
            "|    clip_fraction        | 0.0992      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0025     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.26e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00798    |\n",
            "|    reward               | -0.23636451 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.18e+03    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  -0.3204717817257729\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 600       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0881   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 54.5      |\n",
            "|    reward             | 1.4936749 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 36.7      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 632         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.0277      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | -62.2       |\n",
            "|    reward             | -0.34066078 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 38.3        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 633        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.00314   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 968        |\n",
            "|    reward             | -28.650696 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 7.28e+03   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 636      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.0367   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 136      |\n",
            "|    reward             | 9.514794 |\n",
            "|    std                | 0.993    |\n",
            "|    value_loss         | 158      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 642       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.6     |\n",
            "|    explained_variance | -0.223    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 24.2      |\n",
            "|    reward             | 0.7875787 |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 4.75      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 651         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.6       |\n",
            "|    explained_variance | -0.008      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 0.138       |\n",
            "|    reward             | -0.37946367 |\n",
            "|    std                | 0.986       |\n",
            "|    value_loss         | 0.336       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 657        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.6      |\n",
            "|    explained_variance | -0.0375    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -5.86      |\n",
            "|    reward             | -1.5846922 |\n",
            "|    std                | 0.986      |\n",
            "|    value_loss         | 0.574      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 659       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.000187  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -14.9     |\n",
            "|    reward             | 4.5112424 |\n",
            "|    std                | 0.988     |\n",
            "|    value_loss         | 3.41      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 662        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.0366     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -9.16      |\n",
            "|    reward             | 0.65219367 |\n",
            "|    std                | 0.989      |\n",
            "|    value_loss         | 1.77       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 664        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.972     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -4.35      |\n",
            "|    reward             | 0.74242216 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 0.577      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 667        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.325     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 32.8       |\n",
            "|    reward             | -2.1250813 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 8.88       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 670       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0692    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 37.3      |\n",
            "|    reward             | -1.163905 |\n",
            "|    std                | 0.989     |\n",
            "|    value_loss         | 13.8      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 672      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0584  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 67.5     |\n",
            "|    reward             | 0.8839   |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 26.3     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 674       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -26.4     |\n",
            "|    reward             | -2.328768 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 4.28      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 675        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 3.4        |\n",
            "|    reward             | -0.7566541 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.158      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 676        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.0068    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 1.14e+03   |\n",
            "|    reward             | -24.132826 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.47e+04   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 678        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 587        |\n",
            "|    reward             | -23.587656 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.86e+03   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 679       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 1.14e+03  |\n",
            "|    reward             | -87.75003 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 2.57e+04  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 679        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.228      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 28.4       |\n",
            "|    reward             | 0.24933034 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 31.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 680       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0565    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 276       |\n",
            "|    reward             | 10.838779 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 427       |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.12512821690218728\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 258      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -69.1    |\n",
            "|    critic_loss     | 3.93e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 6.436212 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.09618669541272504\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 232      |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 684      |\n",
            "|    critic_loss     | 2.7e+04  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 9.790937 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.09636865004551791\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 172        |\n",
            "|    time_elapsed    | 54         |\n",
            "|    total_timesteps | 9492       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | -1.51e+03  |\n",
            "|    critic_loss     | 1.87e+04   |\n",
            "|    ent_coef        | 0.0633     |\n",
            "|    ent_coef_loss   | -38.3      |\n",
            "|    learning_rate   | 0.0003     |\n",
            "|    n_updates       | 9391       |\n",
            "|    reward          | -1.9664218 |\n",
            "-----------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.32186169244882457\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 759        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.5544497 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 736          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066562146 |\n",
            "|    clip_fraction        | 0.0622       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000266     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 359          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00714     |\n",
            "|    reward               | -1.1099768   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.01e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 731         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006714006 |\n",
            "|    clip_fraction        | 0.068       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00475    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 585         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00637    |\n",
            "|    reward               | 1.791783    |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.07e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 721         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009793982 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00648    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 168         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 18.053413   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 323         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008195062 |\n",
            "|    clip_fraction        | 0.0471      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.9       |\n",
            "|    explained_variance   | 0.0012      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.12e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.007      |\n",
            "|    reward               | -136.8599   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 4.71e+03    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.22102690157444654\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 564      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | -0.0348  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 169      |\n",
            "|    reward             | 5.959512 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 401      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 580       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | -545      |\n",
            "|    reward             | -4.97271  |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 2.87e+03  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 601        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 3.42e+03   |\n",
            "|    reward             | -51.261547 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 9.29e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 609       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 245       |\n",
            "|    reward             | 19.143417 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 515       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 613        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -2.25      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 4.68       |\n",
            "|    reward             | -0.8647153 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.459      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 619         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.889       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 1.85        |\n",
            "|    reward             | -0.30527353 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0405      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 625         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.0888      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 5.15        |\n",
            "|    reward             | -0.07872413 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.184       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 629         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.548       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 4.4         |\n",
            "|    reward             | -0.39711836 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.197       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 627        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0984     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 2.12       |\n",
            "|    reward             | -1.9174914 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.896      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 627        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.166     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 6.62       |\n",
            "|    reward             | 0.15699455 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.632      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 628          |\n",
            "|    iterations         | 1100         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 5500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | -1.67        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1099         |\n",
            "|    policy_loss        | 4.19         |\n",
            "|    reward             | -0.023779728 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.233        |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 627      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.146   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -6.3     |\n",
            "|    reward             | 0.571871 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.489    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 626        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.307      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -7.08      |\n",
            "|    reward             | 0.26056325 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.01       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 628        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0663    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 2.49       |\n",
            "|    reward             | -0.7530234 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.795      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 629         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.016      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | 27          |\n",
            "|    reward             | -0.35290247 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.11        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 629      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.022   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 55.9     |\n",
            "|    reward             | 7.747676 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 38.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 625       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -2.67     |\n",
            "|    reward             | 14.354415 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.109     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 624       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0067    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 759       |\n",
            "|    reward             | 19.155518 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.02e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 623       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.282    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -8.89     |\n",
            "|    reward             | -2.168158 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 16.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 622        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -1.99      |\n",
            "|    reward             | -2.0366185 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.361      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.26548470531593044\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 241       |\n",
            "|    time_elapsed    | 40        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.45e+03 |\n",
            "|    critic_loss     | 1.1e+05   |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 0.611416  |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.1911756286926983\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 221       |\n",
            "|    time_elapsed    | 43        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 44.4      |\n",
            "|    critic_loss     | 1.23e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 189.19356 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.48227287574015576\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 172      |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.99e+03 |\n",
            "|    critic_loss     | 1.06e+04 |\n",
            "|    ent_coef        | 1.68     |\n",
            "|    ent_coef_loss   | -44.1    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 0.653491 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.09810349638659628\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 798        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.0956328 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 762          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070617115 |\n",
            "|    clip_fraction        | 0.108        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000141    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 91.5         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00758     |\n",
            "|    reward               | 3.396214     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 242          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 762         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006256909 |\n",
            "|    clip_fraction        | 0.0801      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00201     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 165         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00633    |\n",
            "|    reward               | 0.070672944 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 292         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 756         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009525372 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00498    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 57.1        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0094     |\n",
            "|    reward               | -0.2375975  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 115         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 740         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008162697 |\n",
            "|    clip_fraction        | 0.0989      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00315    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.8        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00799    |\n",
            "|    reward               | 0.35844046  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 82          |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.05439727979917402\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  22.246397165457406  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2015-2025_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 972\n",
            "[INFO] Moved trained_models to 2015-2025_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_crypto/\n",
            "[INFO] Moved results to 2015-2025_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "\n",
        "ensemble_agent_1 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_1,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_1,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_1,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 741         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.59       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -2.06       |\n",
            "|    reward             | -0.12829468 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0976      |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 781      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.0761   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 12.3     |\n",
            "|    reward             | 0.250399 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.05     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 792       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0683    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 40.5      |\n",
            "|    reward             | -1.256325 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 13.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 794       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -86.3     |\n",
            "|    reward             | -0.043514 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 68.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 789      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.0673   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 7.64     |\n",
            "|    reward             | 0.071235 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.839    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 789       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0262    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 2.58      |\n",
            "|    reward             | 1.9801607 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.876     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 793      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0123  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -25.6    |\n",
            "|    reward             | 0.716963 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 796          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -0.92        |\n",
            "|    reward             | -0.090849854 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.00974      |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0295   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -4.39     |\n",
            "|    reward             | 0.6742902 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.305     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 798        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0231     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 15.1       |\n",
            "|    reward             | -0.1924222 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.83       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 795       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 12        |\n",
            "|    reward             | 2.8639667 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 3.53      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 794         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 2.83        |\n",
            "|    reward             | 0.115374096 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.107       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 796       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.529    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 7.46      |\n",
            "|    reward             | -0.151204 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.522     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -3.03e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 9.99      |\n",
            "|    reward             | 0.419101  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.07      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 801      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -17.6    |\n",
            "|    reward             | 1.310669 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 8.68     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 799         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 2.36        |\n",
            "|    reward             | -0.30231038 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0449      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 798        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 5.64       |\n",
            "|    reward             | 0.21329604 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.532      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 797       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 14.4      |\n",
            "|    reward             | -0.479638 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.36      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 799       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 42.5      |\n",
            "|    reward             | -0.214364 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 17.3      |\n",
            "-------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1706775.79\n",
            "total_reward: 706775.79\n",
            "total_cost: 1705.70\n",
            "total_trades: 7656\n",
            "Sharpe: 0.452\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 800        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -4.95      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 22.2       |\n",
            "|    reward             | -1.2335354 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.8        |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.18178122973364025\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1678355.15\n",
            "total_reward: 678355.15\n",
            "total_cost: 1668.82\n",
            "total_trades: 6041\n",
            "Sharpe: 0.483\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 263       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 27.6      |\n",
            "|    critic_loss     | 1.6e+03   |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.432622 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.13208641086380432\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1343946.29\n",
            "total_reward: 343946.29\n",
            "total_cost: 998.99\n",
            "total_trades: 9970\n",
            "Sharpe: 0.361\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 64.4     |\n",
            "|    critic_loss     | 1.22e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 0.075336 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.1313291552587612\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1390910.17\n",
            "total_reward: 390910.17\n",
            "total_cost: 998.98\n",
            "total_trades: 7976\n",
            "Sharpe: 0.354\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 183      |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.78e+03 |\n",
            "|    critic_loss     | 1.13e+03 |\n",
            "|    ent_coef        | 1.04     |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | -0.25299 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.10152730535528129\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1197207.88\n",
            "total_reward: 197207.88\n",
            "total_cost: 127537.63\n",
            "total_trades: 15429\n",
            "Sharpe: 0.238\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 987         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 2           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.023435915 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 956          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058059357 |\n",
            "|    clip_fraction        | 0.0704       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00477     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.53         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00814     |\n",
            "|    reward               | -0.009264314 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.3          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 947         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.00664063  |\n",
            "|    clip_fraction        | 0.0484      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00433     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.69        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00643    |\n",
            "|    reward               | -0.19984797 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.94        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 948         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006512302 |\n",
            "|    clip_fraction        | 0.0714      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0847     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.21        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00843    |\n",
            "|    reward               | -0.85548234 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.35        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 949         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008307827 |\n",
            "|    clip_fraction        | 0.0961      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00566     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.73        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0113     |\n",
            "|    reward               | 0.8576339   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.33        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.026697224837735657\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 795         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.417      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 2.07        |\n",
            "|    reward             | -0.29587156 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0584      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 799        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.385     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 6.95       |\n",
            "|    reward             | 0.13959879 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.495      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 795        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | -18.7      |\n",
            "|    reward             | -1.6882672 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.87       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 797       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.000527  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 29.9      |\n",
            "|    reward             | 0.1511001 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 8.23      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 797       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.23     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 6.65      |\n",
            "|    reward             | 0.6142693 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.527     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 800       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.134    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 3.98      |\n",
            "|    reward             | -0.081218 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.258     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 801         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0777      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -8.1        |\n",
            "|    reward             | -0.54881865 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.789       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 800        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -24.6      |\n",
            "|    reward             | 0.49347854 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 7.01       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 800         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -7.74       |\n",
            "|    reward             | 0.013998473 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.747       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 799         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-05   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -1.78       |\n",
            "|    reward             | 0.016196504 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0306      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 801         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 0.459       |\n",
            "|    reward             | 0.039250907 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.00615     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 802        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -2.46      |\n",
            "|    reward             | 0.31271952 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0699     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 802        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 0.378      |\n",
            "|    reward             | -0.2807806 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.00514    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 801          |\n",
            "|    iterations         | 1400         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 7000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0.734        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1399         |\n",
            "|    policy_loss        | -0.937       |\n",
            "|    reward             | -0.054464422 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.00873      |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 801       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -2.34     |\n",
            "|    reward             | -0.130833 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.0504    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 802      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 8.41e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 1.76     |\n",
            "|    reward             | 0.06172  |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.0257   |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 802        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.000327   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -0.0316    |\n",
            "|    reward             | 0.18903542 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.0181     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 803        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -2.07      |\n",
            "|    reward             | -0.1394428 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.104      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 803        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -6.15      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -1.6       |\n",
            "|    reward             | 0.11035438 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.0723     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 803        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 2.53       |\n",
            "|    reward             | 0.21010341 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.393      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.18161723470670207\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 256      |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.3    |\n",
            "|    critic_loss     | 15.1     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | -0.28812 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.1531670804103301\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 226      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 53.1     |\n",
            "|    critic_loss     | 73.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.924048 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.22350301918295257\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 174      |\n",
            "|    time_elapsed    | 47       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.85e+03 |\n",
            "|    critic_loss     | 1.87e+04 |\n",
            "|    ent_coef        | 1.07     |\n",
            "|    ent_coef_loss   | -4.94    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.316432 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.05765951633305641\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 929        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.3941674 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 867         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009461443 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00674    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.54        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    reward               | 0.2581087   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.9         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 852         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009045154 |\n",
            "|    clip_fraction        | 0.0696      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0178      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.92        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00518    |\n",
            "|    reward               | -0.25517976 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.2         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 847         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008662391 |\n",
            "|    clip_fraction        | 0.073       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00274     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.88        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0072     |\n",
            "|    reward               | 1.231356    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 852         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008758492 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00374    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.12        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    reward               | -0.22432345 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.21866199440157336\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 778         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.726      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.26       |\n",
            "|    reward             | -0.31552598 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.175       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 777          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | -0.814       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 9.56         |\n",
            "|    reward             | -0.024085695 |\n",
            "|    std                | 0.994        |\n",
            "|    value_loss         | 1.17         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0894     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 37.6       |\n",
            "|    reward             | -1.0153874 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 17.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 780       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.165    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 21.1      |\n",
            "|    reward             | 2.2638762 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 4.54      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 779         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.000137   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 16.6        |\n",
            "|    reward             | -0.46368316 |\n",
            "|    std                | 0.991       |\n",
            "|    value_loss         | 1.74        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 781         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.683      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -4.64       |\n",
            "|    reward             | -0.09765824 |\n",
            "|    std                | 0.994       |\n",
            "|    value_loss         | 0.369       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 782        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.251      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 15.7       |\n",
            "|    reward             | 0.09935816 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 1.86       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 784       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0677    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -22.1     |\n",
            "|    reward             | 1.0781324 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 3.48      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 785        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.0705    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 2.42       |\n",
            "|    reward             | 0.37634858 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 1.18       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 786       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -3.78     |\n",
            "|    reward             | -4.245134 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.308     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 784        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -11.1      |\n",
            "|    reward             | -2.2210128 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 1.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.0089   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -47.7     |\n",
            "|    reward             | -2.923812 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 25.1      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 785         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -3.17       |\n",
            "|    reward             | -0.31031725 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.312       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 786        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -1.15      |\n",
            "|    reward             | 0.10401301 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.0398     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 787       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -17       |\n",
            "|    reward             | 1.6682653 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 2.88      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 788        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.119     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -6.11      |\n",
            "|    reward             | 0.88174623 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.966      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 788         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -0.767      |\n",
            "|    reward             | 0.024619332 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 0.00725     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 788         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.127       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 11.7        |\n",
            "|    reward             | 0.064498946 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.34        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 789        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0382     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -4.87      |\n",
            "|    reward             | 0.17330645 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.539      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 790        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.114      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 28.5       |\n",
            "|    reward             | 0.41328198 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 9.64       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.43569922483597096\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 262       |\n",
            "|    time_elapsed    | 32        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 24.7      |\n",
            "|    critic_loss     | 304       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.013811 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.5518842575346623\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 231       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 45.3      |\n",
            "|    critic_loss     | 412       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.144375 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.3913135072668222\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 186       |\n",
            "|    time_elapsed    | 45        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.36e+03  |\n",
            "|    critic_loss     | 888       |\n",
            "|    ent_coef        | 1.22      |\n",
            "|    ent_coef_loss   | -24.3     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -0.304504 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.4043390320094273\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 988        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.3864589 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 933          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052529396 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0137       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.6          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00405     |\n",
            "|    reward               | -1.8915325   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.81         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 920          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0112765655 |\n",
            "|    clip_fraction        | 0.15         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00989      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.24         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0138      |\n",
            "|    reward               | -1.6915729   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 4.05         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 919         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007740025 |\n",
            "|    clip_fraction        | 0.0602      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00531     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.71        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00935    |\n",
            "|    reward               | -0.783581   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 4.99        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 918        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 11         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00553664 |\n",
            "|    clip_fraction        | 0.0449     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.0257    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.34       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.00633   |\n",
            "|    reward               | -0.9546781 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 6.82       |\n",
            "----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.640065956939381\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 781         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -8.42       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.37       |\n",
            "|    reward             | -0.39878854 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.233       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 790        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.112     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 12.8       |\n",
            "|    reward             | 0.20315038 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.65       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 792        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.171      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 44         |\n",
            "|    reward             | -1.2465007 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 16.3       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 781      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 27.8     |\n",
            "|    reward             | 1.474847 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 7.05     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 779        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 6.51       |\n",
            "|    reward             | -0.4386431 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.337      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.188      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -9.48      |\n",
            "|    reward             | -1.0459772 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.909      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 783       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 14.5      |\n",
            "|    reward             | 1.1724614 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.97      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 785       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -29       |\n",
            "|    reward             | 2.5715594 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 12.5      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 786        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -2.88      |\n",
            "|    reward             | -1.0225759 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.234      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 787        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -2.78      |\n",
            "|    reward             | 0.35840562 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.135      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 788        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.318      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -1.31      |\n",
            "|    reward             | -1.5250739 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.196      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 789        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.184     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -2.32      |\n",
            "|    reward             | -1.0193999 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.66       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 790      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 12.3     |\n",
            "|    reward             | 0.440439 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 3.16     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 790        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0147     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -20.9      |\n",
            "|    reward             | 0.84779584 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.08       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 791      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -31.7    |\n",
            "|    reward             | 0.210786 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 9.94     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 791       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0146    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 15        |\n",
            "|    reward             | -1.114284 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.96      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 791        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0353    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 1.92       |\n",
            "|    reward             | 0.47269788 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 3.43       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 791        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0891    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -30.2      |\n",
            "|    reward             | 0.13760743 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 9.48       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 791        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.087     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -40.7      |\n",
            "|    reward             | 0.51887804 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 14.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 791       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.27     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -16.1     |\n",
            "|    reward             | 1.2273428 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.23      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.2688259217211804\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 276      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -381     |\n",
            "|    critic_loss     | 3.56     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.479958 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.42134788285441355\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 246      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 90.1     |\n",
            "|    critic_loss     | 3.37e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.621419 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.3812810883970185\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 187      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.63e+03 |\n",
            "|    critic_loss     | 328      |\n",
            "|    ent_coef        | 1.3      |\n",
            "|    ent_coef_loss   | -19.5    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.931565 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.43303495036599854\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 938        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.3419273 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 847         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008907914 |\n",
            "|    clip_fraction        | 0.0997      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0101     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.48        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00884    |\n",
            "|    reward               | -0.55325013 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 10.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 858         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009061715 |\n",
            "|    clip_fraction        | 0.0868      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0114      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.18        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00944    |\n",
            "|    reward               | -1.0040214  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.78        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 867          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060293884 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.000138    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.39         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00515     |\n",
            "|    reward               | 0.29220295   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.9          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 873         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009020124 |\n",
            "|    clip_fraction        | 0.0827      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0199     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.77        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00938    |\n",
            "|    reward               | 0.11936658  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 12.6        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.40749877526873807\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 775       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -5.54     |\n",
            "|    reward             | -0.150594 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.31      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 780      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 2.53     |\n",
            "|    reward             | 0.915564 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 0.523    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 28.2      |\n",
            "|    reward             | -1.440227 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 7.12      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 770      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 29.4     |\n",
            "|    reward             | 0.402917 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 7.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 772      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 5.22     |\n",
            "|    reward             | 1.005244 |\n",
            "|    std                | 0.997    |\n",
            "|    value_loss         | 0.968    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 774       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 19.8      |\n",
            "|    reward             | -1.276276 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 3.1       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 777       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0646    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 16.9      |\n",
            "|    reward             | -1.188884 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 2.74      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 778       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -25.4     |\n",
            "|    reward             | -3.309428 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 8.99      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 778         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.934      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -47.4       |\n",
            "|    reward             | 0.054508664 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 27.6        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 779      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.00155 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 1.13     |\n",
            "|    reward             | 0.568095 |\n",
            "|    std                | 0.994    |\n",
            "|    value_loss         | 0.216    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 780      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 3.54     |\n",
            "|    reward             | 0.69906  |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 0.237    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 781      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 11.7     |\n",
            "|    reward             | -0.66812 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 781       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 28.4      |\n",
            "|    reward             | 0.537715  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.61      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.634     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 16.7       |\n",
            "|    reward             | 0.70540285 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.11       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 774      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -19.7    |\n",
            "|    reward             | 0.9692   |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 3.1      |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 775          |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | -19.8        |\n",
            "|    reward             | -0.017777827 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 2.84         |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 775      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 11.3     |\n",
            "|    reward             | -2.13744 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 775         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -2.48       |\n",
            "|    reward             | -0.05155756 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0556      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 776       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.000128  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 18.4      |\n",
            "|    reward             | -0.440975 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.6       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 776       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 7.38      |\n",
            "|    reward             | 0.2632087 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.73      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.5789640734370055\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 269       |\n",
            "|    time_elapsed    | 33        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -66.3     |\n",
            "|    critic_loss     | 6.18      |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.360644 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.19134389426680035\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 239       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 145       |\n",
            "|    critic_loss     | 938       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.477217 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.3039331641364327\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 177       |\n",
            "|    time_elapsed    | 50        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 423       |\n",
            "|    critic_loss     | 1.18e+04  |\n",
            "|    ent_coef        | 0.112     |\n",
            "|    ent_coef_loss   | -11.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.100835 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.28956040711032227\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 955        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.7044606 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 920         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008836739 |\n",
            "|    clip_fraction        | 0.0993      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0019      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.6         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00907    |\n",
            "|    reward               | 0.29716727  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.37        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 903         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007916907 |\n",
            "|    clip_fraction        | 0.0838      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00362     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.56        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00924    |\n",
            "|    reward               | 0.55659115  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.14        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 900          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0084759435 |\n",
            "|    clip_fraction        | 0.0959       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00828      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.66         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00899     |\n",
            "|    reward               | 0.899184     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 6.31         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 898         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011211596 |\n",
            "|    clip_fraction        | 0.0855      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0367     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.18        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00976    |\n",
            "|    reward               | 0.30659676  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.33        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.31286399517997976\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 404        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0844     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -4.68      |\n",
            "|    reward             | -0.2686696 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.228      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 503         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 10.3        |\n",
            "|    reward             | 0.030240258 |\n",
            "|    std                | 0.996       |\n",
            "|    value_loss         | 1.08        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 568         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.0545     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 13.1        |\n",
            "|    reward             | -0.24262257 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 1.96        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 608       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.527    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 16.2      |\n",
            "|    reward             | 1.2885635 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.82      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 633       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.185     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -3.26     |\n",
            "|    reward             | 0.8677601 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.16      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 652         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.28       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -3.26       |\n",
            "|    reward             | -0.09195027 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.141       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 667        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.604     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 5.99       |\n",
            "|    reward             | 0.16775578 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.824      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 678        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.76      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 3.35       |\n",
            "|    reward             | 0.20691165 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.732      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 688       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0492    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -3.32     |\n",
            "|    reward             | -0.398071 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.58      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 695        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 2.52       |\n",
            "|    reward             | 0.05832407 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.117      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 701        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.3       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -8.7       |\n",
            "|    reward             | 0.87254757 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.578      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 707       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.112     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 19.3      |\n",
            "|    reward             | 1.2709119 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 4.69      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 711       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0203    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -30.3     |\n",
            "|    reward             | 1.2203752 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 9.39      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 715        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.319      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -8.41      |\n",
            "|    reward             | 0.07147449 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.482      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 718         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -1.44       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -5.1        |\n",
            "|    reward             | -0.51559824 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.443       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 8.94      |\n",
            "|    reward             | 0.9998136 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.993     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 724        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.157     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 13.6       |\n",
            "|    reward             | -0.1512742 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 2.16       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 726         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.413      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 5.39        |\n",
            "|    reward             | -0.02515848 |\n",
            "|    std                | 0.993       |\n",
            "|    value_loss         | 0.315       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.244      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -5.38      |\n",
            "|    reward             | 0.56490725 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 0.315      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 730         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -4.09       |\n",
            "|    reward             | -0.69802415 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.457       |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.05521405642482035\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 275      |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.93    |\n",
            "|    critic_loss     | 125      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.038164 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.0017370710398163694\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 246      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 524      |\n",
            "|    critic_loss     | 2.42e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.382614 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.03636141130788735\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 192      |\n",
            "|    time_elapsed    | 48       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.02e+03 |\n",
            "|    critic_loss     | 345      |\n",
            "|    ent_coef        | 1.51     |\n",
            "|    ent_coef_loss   | -49.4    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.088159 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.06770174521846646\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 937        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.1818621 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 902         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005072085 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.00355    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.88        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00812    |\n",
            "|    reward               | -3.0546176  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.15        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 884         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010374621 |\n",
            "|    clip_fraction        | 0.0955      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00357    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.75        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    reward               | -1.3984522  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.8         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 881          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064781117 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0105      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 6.72         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00877     |\n",
            "|    reward               | -0.20136589  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 880         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007534981 |\n",
            "|    clip_fraction        | 0.0597      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0148     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.4         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00583    |\n",
            "|    reward               | -0.1846232  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.21443000319373973\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 728         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0397      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -3.74       |\n",
            "|    reward             | -0.17950718 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.128       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 737          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0.365        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 3.59         |\n",
            "|    reward             | -0.005256768 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.118        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 744        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.147      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 3.76       |\n",
            "|    reward             | 0.03803657 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.347      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.15     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 18.1      |\n",
            "|    reward             | 1.4720666 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.24      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 747         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.312      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -3.34       |\n",
            "|    reward             | -0.16166568 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.512       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 748        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.18       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -0.398     |\n",
            "|    reward             | -0.2630138 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.133      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 749         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.247      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -0.888      |\n",
            "|    reward             | -0.99511087 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.16        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 750       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -10.3     |\n",
            "|    reward             | 3.1792004 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.99      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 751      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.498   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 9.65     |\n",
            "|    reward             | 0.681505 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 750        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.903     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -3.55      |\n",
            "|    reward             | 0.65043986 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.659      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 751       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.268     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 9.94      |\n",
            "|    reward             | -0.765494 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.914     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 751        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.926     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 5.5        |\n",
            "|    reward             | 0.13902737 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.275      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 724         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.312       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -7.47       |\n",
            "|    reward             | -0.17670959 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.437       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 726         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -0.186      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 3.15        |\n",
            "|    reward             | 0.002381012 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.06        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 728      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | -0.577   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    reward             | 0.105523 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.00201  |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 730         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 4.62        |\n",
            "|    reward             | -0.22044593 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.156       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 731       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0.00298   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 7.03      |\n",
            "|    reward             | -0.409716 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 0.469     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 733         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -3.04       |\n",
            "|    reward             | -0.15891445 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.278       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 734        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0.577      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -3.11      |\n",
            "|    reward             | 0.16068979 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.123      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 735       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 0.481     |\n",
            "|    reward             | -0.047801 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.00747   |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.04711867995054647\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 264      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11       |\n",
            "|    critic_loss     | 10.8     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.2347   |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.16078021373581985\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 239       |\n",
            "|    time_elapsed    | 39        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 226       |\n",
            "|    critic_loss     | 3.27e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.130358 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.1875972396055131\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 187       |\n",
            "|    time_elapsed    | 50        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.46e+03  |\n",
            "|    critic_loss     | 1.85e+04  |\n",
            "|    ent_coef        | 1.66      |\n",
            "|    ent_coef_loss   | -49.3     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.593437 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.39284312071708294\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 923        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.5808227 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 877         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006625887 |\n",
            "|    clip_fraction        | 0.0997      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0243     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.81        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00929    |\n",
            "|    reward               | -0.9207543  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.46        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 864          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071586543 |\n",
            "|    clip_fraction        | 0.0673       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00252     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.02         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00697     |\n",
            "|    reward               | -0.2185257   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.78         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 863         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010323122 |\n",
            "|    clip_fraction        | 0.0952      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0113     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.45        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00684    |\n",
            "|    reward               | -0.07952543 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 9.43        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 859         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008221286 |\n",
            "|    clip_fraction        | 0.0887      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0187      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.21        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    reward               | 0.57623595  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 6.65        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.26661362190056975\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 684         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -2.26       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -7.49       |\n",
            "|    reward             | -0.11178985 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.646       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 716        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -0.1       |\n",
            "|    reward             | 0.09280832 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.00345    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 728         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.137       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | -5.65       |\n",
            "|    reward             | 0.096873395 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.376       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 736      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 6.92     |\n",
            "|    reward             | 0.52338  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.442    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 736        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -3.4       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -17        |\n",
            "|    reward             | -0.7183467 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.97       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.158      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -0.45      |\n",
            "|    reward             | 0.16405024 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0183     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 741          |\n",
            "|    iterations         | 700          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 3500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0.515        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 699          |\n",
            "|    policy_loss        | -1.99        |\n",
            "|    reward             | -0.013334268 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.0444       |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.00693   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 4.16      |\n",
            "|    reward             | -0.005942 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.153     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 744         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.0946     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -0.87       |\n",
            "|    reward             | -0.46280658 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.129       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 744        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.3        |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 6.66       |\n",
            "|    reward             | 0.21711107 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.588      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 745          |\n",
            "|    iterations         | 1100         |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 5500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -0.643       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1099         |\n",
            "|    policy_loss        | 0.869        |\n",
            "|    reward             | -0.070666194 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.155        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.000521 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -12.9     |\n",
            "|    reward             | 0.8878247 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.46      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 747      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0.0642   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -21      |\n",
            "|    reward             | 1.911771 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 747      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 34.8     |\n",
            "|    reward             | 1.158122 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 747         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.139       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | 12.4        |\n",
            "|    reward             | -0.24112804 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.34        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 1.49e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 2.34      |\n",
            "|    reward             | -0.659354 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.151     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 4.21     |\n",
            "|    reward             | 0.410276 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 0.263    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | -0.0507  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 15.1     |\n",
            "|    reward             | 1.094695 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 748      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 37.8     |\n",
            "|    reward             | 0.690336 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.19     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 23.3      |\n",
            "|    reward             | -1.696764 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.46      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.0366933674410298\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 261      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 142      |\n",
            "|    critic_loss     | 5.49e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.42269  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.06237230016544798\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 244      |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 112      |\n",
            "|    critic_loss     | 3.21e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.653864 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.009262996002011808\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 183      |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 76.7     |\n",
            "|    critic_loss     | 57.8     |\n",
            "|    ent_coef        | 0.027    |\n",
            "|    ent_coef_loss   | -21.1    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.134641 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.0035123051400042958\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 906        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -1.8369157 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 840         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007604314 |\n",
            "|    clip_fraction        | 0.0889      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0108     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.95        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00985    |\n",
            "|    reward               | -0.12813902 |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 6.01        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 834        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 7          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00794181 |\n",
            "|    clip_fraction        | 0.0929     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.3      |\n",
            "|    explained_variance   | -0.0159    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.3        |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0102    |\n",
            "|    reward               | -1.3098234 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 7.29       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 829         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007171178 |\n",
            "|    clip_fraction        | 0.0581      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.044      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.78        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00907    |\n",
            "|    reward               | 0.19120635  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.21        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 825          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067881662 |\n",
            "|    clip_fraction        | 0.0522       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00543     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.65         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00776     |\n",
            "|    reward               | 0.19380024   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 12           |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.20317232889746792\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  21.265182832876842  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Merging trade action files...\n",
            "[INFO] Merged trade actions saved to: 2015-2025_no_crypto/merged_trade_actions.csv\n",
            "[INFO] Total trades executed: 1545\n",
            "[INFO] Moved trained_models to 2015-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_no_crypto/\n",
            "[INFO] Moved results to 2015-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_2 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_2,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_2,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_2,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_no_crypto.csv\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
