{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
        "\n",
        "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
        "\n",
        "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
        "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
        "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
        "* **Pytorch Version**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGunVt8oLCVS"
      },
      "source": [
        "# Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOzAKQ-SLGX6"
      },
      "source": [
        "* [1. Problem Definition](#0)\n",
        "* [2. Getting Started - Load Python packages](#1)\n",
        "    * [2.1. Install Packages](#1.1)    \n",
        "    * [2.2. Check Additional Packages](#1.2)\n",
        "    * [2.3. Import Packages](#1.3)\n",
        "    * [2.4. Create Folders](#1.4)\n",
        "* [3. Download Data](#2)\n",
        "* [4. Preprocess Data](#3)        \n",
        "    * [4.1. Technical Indicators](#3.1)\n",
        "    * [4.2. Perform Feature Engineering](#3.2)\n",
        "* [5.Build Environment](#4)  \n",
        "    * [5.1. Training & Trade Data Split](#4.1)\n",
        "    * [5.2. User-defined Environment](#4.2)   \n",
        "    * [5.3. Initialize Environment](#4.3)    \n",
        "* [6.Implement DRL Algorithms](#5)  \n",
        "* [7.Backtesting Performance](#6)  \n",
        "    * [7.1. BackTestStats](#6.1)\n",
        "    * [7.2. BackTestPlot](#6.2)   \n",
        "    * [7.3. Baseline Stats](#6.3)   \n",
        "    * [7.3. Compare to Stock Market Index](#6.4)             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "<a id='0'></a>\n",
        "# Part 1. Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
        "\n",
        "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
        "\n",
        "\n",
        "* Action: The action space describes the allowed actions that the agent interacts with the\n",
        "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
        "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
        "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
        "values at state s′ and s, respectively\n",
        "\n",
        "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
        "our trading agent observes many different features to better learn in an interactive environment.\n",
        "\n",
        "* Environment: Dow 30 consituents\n",
        "\n",
        "\n",
        "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "<a id='1'></a>\n",
        "# Part 2. Getting Started- Load Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5_PTmOh1hj"
      },
      "source": [
        "<a id='1.1'></a>\n",
        "## 2.1. Install all the packages through FinRL library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPT0ipYE28wL",
        "outputId": "912ac487-d3c8-467f-ef2a-968fa655b206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrds in /opt/anaconda3/lib/python3.12/site-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.2.3)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from wrds) (2.0.34)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.3.0)\n",
            "zsh:1: command not found: apt-get\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-m8nirk84\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-req-build-m8nirk84\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 69776b349ee4e63efe3826f318aef8e5c5f59648\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-mvzs6yt7/elegantrl_340a7c0e899a4e28b0aa1ab4786666b7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/ks/bjl76g8d4zxgw0m5p8z2pd9r0000gn/T/pip-install-mvzs6yt7/elegantrl_340a7c0e899a4e28b0aa1ab4786666b7\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.8)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.5.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.4)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.10.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.1)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (75.1.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.0)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (1.11.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.34)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (8.27.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.9.2)\n",
            "Requirement already satisfied: pytz>=2014.10 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2024.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /opt/anaconda3/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.11)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.25.3)\n",
            "Requirement already satisfied: aiosignal in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: frozenlist in /opt/anaconda3/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.14.1)\n",
            "Requirement already satisfied: smart-open in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (5.2.1)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (19.0.1)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2024.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.5.0)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (4.11.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.66.5)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.7.1)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.4.0)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (0.21.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /opt/anaconda3/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.12.3)\n",
            "Requirement already satisfied: th in /opt/anaconda3/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.7)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (8.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.10.6)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.2.0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.1.3)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.8)\n",
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# ## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install pandas_market_calendars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBHhVysOEzi"
      },
      "source": [
        "\n",
        "<a id='1.2'></a>\n",
        "## 2.2. Check if the additional packages needed are present, if not install them.\n",
        "* Yahoo Finance API\n",
        "* pandas\n",
        "* numpy\n",
        "* matplotlib\n",
        "* stockstats\n",
        "* OpenAI gym\n",
        "* stable-baselines\n",
        "* tensorflow\n",
        "* pyfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Suppress Warnings\n",
        "# ===========================\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ===========================\n",
        "# Standard Libraries\n",
        "# ===========================\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('Agg')  \n",
        "\n",
        "# ===========================\n",
        "# Enable Inline Plotting (Jupyter)\n",
        "# ===========================\n",
        "%matplotlib inline\n",
        "\n",
        "# ===========================\n",
        "# FinRL Imports\n",
        "# ===========================\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "# ===========================\n",
        "# Create Necessary Directories\n",
        "# ===========================\n",
        "check_and_make_directories([\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR\n",
        "])\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Custom Imports (model.py)\n",
        "# ===========================\n",
        "sys.path.append(os.path.abspath(\".\"))  \n",
        "from models import DRLEnsembleAgent\n",
        "\n",
        "sys.path.append(\"../FinRL-Library\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "<a id='2'></a>\n",
        "# Part 3. Download Data\n",
        "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
        "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
        "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeQ7iS-LoMm"
      },
      "source": [
        "\n",
        "\n",
        "-----\n",
        "class YahooDownloader:\n",
        "    Provides methods for retrieving daily stock data from\n",
        "    Yahoo Finance API\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        start_date : str\n",
        "            start date of the data (modified from config.py)\n",
        "        end_date : str\n",
        "            end date of the data (modified from config.py)\n",
        "        ticker_list : list\n",
        "            a list of stock tickers (modified from config.py)\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    fetch_data()\n",
        "        Fetches data from yahoo API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Part 4: Preprocess Data\n",
        "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
        "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
        "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_csv_to_features(csv_path):\n",
        "    # Step 1: Load Data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Step 2: Identify 5-day and 7-day tickers\n",
        "    day_values_per_tic = df.groupby('tic')['day'].apply(lambda x: sorted(x.unique())).reset_index()\n",
        "    day_values_per_tic.columns = ['tic', 'unique_days']\n",
        "\n",
        "    tics_5day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(5)))]['tic']\n",
        "    tics_7day = day_values_per_tic[day_values_per_tic['unique_days'].apply(lambda x: x == list(range(7)))]['tic']\n",
        "\n",
        "    df_5day_full = df[df['tic'].isin(tics_5day)]\n",
        "    df_7day_full = df[df['tic'].isin(tics_7day)]\n",
        "\n",
        "    # Step 3: Apply Technical Indicators\n",
        "    fe_ti = FeatureEngineer(\n",
        "        use_technical_indicator=True,\n",
        "        use_turbulence=False,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    df_5day_full = fe_ti.preprocess_data(df_5day_full)\n",
        "    if not df_7day_full.empty:\n",
        "        df_7day_full = fe_ti.preprocess_data(df_7day_full)\n",
        "    else:\n",
        "        print(\"[Info] df_7day_full is empty. Skipping technical indicators.\")\n",
        "\n",
        "    # Step 4: Combine and Clean Index\n",
        "    combined_df = pd.concat([df_5day_full, df_7day_full], ignore_index=False)\n",
        "    combined_df.index = range(len(combined_df))\n",
        "\n",
        "    # Step 5: Remove dates with only one ticker\n",
        "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "    combined_df = combined_df[combined_df.groupby('date')['date'].transform('count') > 1]\n",
        "    combined_df = combined_df.sort_values(['date', 'tic']).reset_index(drop=True)\n",
        "\n",
        "    # Step 6: Apply Turbulence Feature\n",
        "    fe_turb = FeatureEngineer(\n",
        "        use_technical_indicator=False,\n",
        "        use_turbulence=True,\n",
        "        user_defined_feature=False\n",
        "    )\n",
        "    processed = fe_turb.preprocess_data(combined_df)\n",
        "\n",
        "    # Step 7: Final Cleaning\n",
        "    processed = processed.copy()\n",
        "    processed = processed.fillna(0)\n",
        "    processed = processed.replace(np.inf, 0)\n",
        "\n",
        "    return processed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5. Design Environment\n",
        "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
        "\n",
        "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_drl_ensemble_agent(processed_df,  \n",
        "                              indicators, \n",
        "                              train_start_date, \n",
        "                              train_end_date,\n",
        "                              trade_start_date, \n",
        "                              trade_end_date, \n",
        "                              rebalance_window=63, \n",
        "                              validation_window=63, \n",
        "                              initial_amount=1_000_000,\n",
        "                              transaction_cost=0.001,\n",
        "                              hmax=100,\n",
        "                              reward_scaling=1e-4,\n",
        "                              print_verbosity=5):\n",
        "    \"\"\"\n",
        "    Setup DRLEnsembleAgent with flexible date and parameter configuration.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate dynamic parameters\n",
        "    stock_dimension = len(processed_df.tic.unique())\n",
        "    state_space = 1 + 2 * stock_dimension + len(indicators) * stock_dimension\n",
        "\n",
        "    # 2. Environment configuration\n",
        "    env_kwargs = {\n",
        "        \"hmax\": hmax,\n",
        "        \"initial_amount\": initial_amount,\n",
        "        \"buy_cost_pct\": transaction_cost,\n",
        "        \"sell_cost_pct\": transaction_cost,\n",
        "        \"state_space\": state_space,\n",
        "        \"stock_dim\": stock_dimension,\n",
        "        \"tech_indicator_list\": indicators,\n",
        "        \"action_space\": stock_dimension,\n",
        "        \"reward_scaling\": reward_scaling,\n",
        "        \"print_verbosity\": print_verbosity\n",
        "    }\n",
        "\n",
        "    # 3. Initialize DRLEnsembleAgent\n",
        "    agent = DRLEnsembleAgent(\n",
        "        df=processed_df,\n",
        "        train_period=(train_start_date, train_end_date),\n",
        "        val_test_period=(trade_start_date, trade_end_date),\n",
        "        rebalance_window=rebalance_window,\n",
        "        validation_window=validation_window,\n",
        "        **env_kwargs\n",
        "    )\n",
        "\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "<a id='5'></a>\n",
        "# Part 6: Implement DRL Algorithms\n",
        "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
        "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
        "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
        "design their own DRL algorithms by adapting these DRL algorithms.\n",
        "\n",
        "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.005,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\":0.01,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.00025,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
        "                      \"buffer_size\": 10_000,\n",
        "                      \"learning_rate\": 0.0005,\n",
        "                      \"batch_size\": 64\n",
        "                    }\n",
        "\n",
        "SAC_model_kwargs = {\n",
        "                      \"batch_size\": 128,\n",
        "                      \"buffer_size\": 100000,\n",
        "                      \"learning_rate\": 0.0003,\n",
        "                      \"learning_starts\": 100,\n",
        "                      \"ent_coef\": \"auto_0.1\",\n",
        "                    }\n",
        "\n",
        "TD3_model_kwargs = {\n",
        "                      \"batch_size\": 100,\n",
        "                      \"buffer_size\": 1000000,\n",
        "                      \"learning_rate\": 0.001\n",
        "                   }\n",
        "\n",
        "\n",
        "\n",
        "timesteps_dict = {'a2c' : 10_000,\n",
        "                 'ppo' : 10_000,\n",
        "                 'ddpg' : 10_000,\n",
        "                  'sac' : 10_000,\n",
        "                 'td3' : 10_000,\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='6'></a>\n",
        "# Part 7: Backtest Our Strategy\n",
        "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_ensemble_and_generate_daily_return(ensemble_agent, \n",
        "                                            A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, \n",
        "                                            timesteps_dict, \n",
        "                                            processed_df, \n",
        "                                            trade_start_date, trade_end_date, \n",
        "                                            rebalance_window, validation_window, \n",
        "                                            output_csv_name=\"df_daily_return.csv\",\n",
        "                                            initial_fund=1_000_000,\n",
        "                                            original_csv_path=\"data.csv\"):\n",
        "    \"\"\"\n",
        "    Runs DRL Ensemble Strategy, tracks continuous portfolio value, \n",
        "    calculates daily returns, saves outputs, and organizes files into a folder.\n",
        "    \"\"\"\n",
        "    from finrl.main import check_and_make_directories\n",
        "    from finrl.config import (\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR,\n",
        "     )\n",
        "\n",
        "    # ===========================\n",
        "    # Create Necessary Directories\n",
        "    # ===========================\n",
        "    check_and_make_directories([\n",
        "        TRAINED_MODEL_DIR,\n",
        "        TENSORBOARD_LOG_DIR,\n",
        "        RESULTS_DIR\n",
        "    ])\n",
        "\n",
        "\n",
        "    # === Step 1: Create Folder Based on CSV Name ===\n",
        "    base_name = os.path.splitext(os.path.basename(original_csv_path))[0]\n",
        "    target_folder = f\"{base_name}\"\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "        print(f\"[INFO] Created folder: {target_folder}\")\n",
        "\n",
        "    # === Step 2: Run Ensemble Strategy ===\n",
        "    print(\"[INFO] Running Ensemble Strategy...\")\n",
        "    df_summary = ensemble_agent.run_ensemble_strategy(\n",
        "        A2C_kwargs, PPO_kwargs, DDPG_kwargs, SAC_kwargs, TD3_kwargs, timesteps_dict\n",
        "    )\n",
        "\n",
        "    # === Step 3: Prepare Trade Dates ===\n",
        "    unique_trade_date = processed_df[\n",
        "        (processed_df.date >= trade_start_date) & (processed_df.date <= trade_end_date)\n",
        "    ].date.unique()\n",
        "\n",
        "    current_value = initial_fund\n",
        "    portfolio_tracking = []\n",
        "    is_first_file = True\n",
        "\n",
        "    rebalance_points = list(range(rebalance_window + validation_window, len(unique_trade_date) + 1, rebalance_window))\n",
        "\n",
        "    # === Step 4: Track Portfolio Value Across Rebalances ===\n",
        "    for i in rebalance_points:\n",
        "        file_path = f'results/account_value_trade_ensemble_{i}.csv'\n",
        "        if os.path.exists(file_path):\n",
        "            temp = pd.read_csv(file_path)\n",
        "\n",
        "            if is_first_file:\n",
        "                first_date = temp.loc[0, 'date']\n",
        "                original_value = temp.loc[0, 'account_value']\n",
        "                portfolio_tracking.append({\n",
        "                    'date': first_date,\n",
        "                    'portfolio_value': current_value,\n",
        "                    'original_account_value': original_value\n",
        "                })\n",
        "                start_idx = 1\n",
        "                is_first_file = False\n",
        "            else:\n",
        "                start_idx = 1\n",
        "\n",
        "            for idx in range(start_idx, len(temp)):\n",
        "                daily_return = temp.loc[idx, 'daily_return']\n",
        "                date = temp.loc[idx, 'date']\n",
        "                original_value = temp.loc[idx, 'account_value']\n",
        "                if pd.notna(daily_return):\n",
        "                    current_value *= (1 + daily_return)\n",
        "                    portfolio_tracking.append({\n",
        "                        'date': date,\n",
        "                        'portfolio_value': current_value,\n",
        "                        'original_account_value': original_value\n",
        "                    })\n",
        "        else:\n",
        "            print(f\"[Warning] File does not exist: {file_path}\")\n",
        "\n",
        "    df_portfolio = pd.DataFrame(portfolio_tracking)\n",
        "\n",
        "    # === Step 5: Plot Portfolio Value ===\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['portfolio_value'], label='Continuous Portfolio Value')\n",
        "    plt.plot(pd.to_datetime(df_portfolio['date']), df_portfolio['original_account_value'], label='Original (Resetting) Account Value', linestyle='--')\n",
        "    plt.title('Portfolio Value: Continuous vs Original')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Portfolio Value')\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(target_folder, \"portfolio_value_plot.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"[INFO] Portfolio value plot saved to: {plot_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # === Step 6: Calculate Daily Returns ===\n",
        "    df_daily_return = df_portfolio.copy()\n",
        "    df_daily_return[\"daily_return\"] = df_daily_return[\"portfolio_value\"].pct_change()\n",
        "    df_daily_return = df_daily_return.infer_objects(copy=False)\n",
        "    df_daily_return.loc[0, \"daily_return\"] = 0.0\n",
        "    df_daily_return = df_daily_return[[\"date\", \"daily_return\"]]\n",
        "\n",
        "    # === Step 7: Save Daily Return CSV into Folder ===\n",
        "    csv_full_path = os.path.join(target_folder, output_csv_name)\n",
        "    df_daily_return.to_csv(csv_full_path, index=False)\n",
        "    print(f\"[INFO] Daily return saved to: {csv_full_path}\")\n",
        "\n",
        "    # === Step 8: Move Directories into the Folder ===\n",
        "    dirs_to_move = [TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR]\n",
        "\n",
        "    for dir_path in dirs_to_move:\n",
        "        if os.path.exists(dir_path):\n",
        "            dest_path = os.path.join(target_folder, os.path.basename(dir_path))\n",
        "            if os.path.exists(dest_path):\n",
        "                shutil.rmtree(dest_path)  # Clean if already exists\n",
        "            shutil.move(dir_path, target_folder)\n",
        "            print(f\"[INFO] Moved {dir_path} to {target_folder}/\")\n",
        "        else:\n",
        "            print(f\"[Warning] Directory not found: {dir_path}\")\n",
        "\n",
        "    return df_daily_return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_2\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 557       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.437    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 4.96      |\n",
            "|    reward             | -0.960131 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.517     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 552         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.00606    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 0.883       |\n",
            "|    reward             | 0.043912873 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.757       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 553        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.229     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 15         |\n",
            "|    reward             | -0.5398014 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.95       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 558        |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -17.4      |\n",
            "|    reward             | -0.3017099 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.04       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 560       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0813   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 1.12      |\n",
            "|    reward             | 0.9951041 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.88      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 562        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.278     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -5.91      |\n",
            "|    reward             | 0.22362092 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.582      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 556        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.01      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -27.4      |\n",
            "|    reward             | 0.10950244 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.16       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 536         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -21         |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -10.3       |\n",
            "|    reward             | -0.14633526 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.7         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 503       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.376    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 10.1      |\n",
            "|    reward             | 0.8438851 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.14      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 508       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.257     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 16.7      |\n",
            "|    reward             | 0.2629732 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.93      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 510        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.341     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 24.4       |\n",
            "|    reward             | -1.5233546 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 6.09       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 498      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.534   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 7.72     |\n",
            "|    reward             | 1.056121 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 0.8      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 495        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.582     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -0.99      |\n",
            "|    reward             | 0.40898797 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.136      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 496      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0.132    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 6.57     |\n",
            "|    reward             | 2.000766 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 496       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.093    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -5.59     |\n",
            "|    reward             | 1.2605854 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.59      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 486        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0264     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 10.3       |\n",
            "|    reward             | 0.98238647 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.36       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 480        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.0308    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -13        |\n",
            "|    reward             | 0.28395343 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 1.49       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 477        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.284     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -23.5      |\n",
            "|    reward             | -0.6423594 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 5.39       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 477        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.401     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 9.04       |\n",
            "|    reward             | 0.44873184 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 0.666      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 471          |\n",
            "|    iterations         | 2000         |\n",
            "|    time_elapsed       | 21           |\n",
            "|    total_timesteps    | 10000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | 5.96e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1999         |\n",
            "|    policy_loss        | -13.4        |\n",
            "|    reward             | -0.026253324 |\n",
            "|    std                | 0.998        |\n",
            "|    value_loss         | 1.26         |\n",
            "----------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.12854110524740403\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_2\n",
            "day: 3925, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1557633.07\n",
            "total_reward: 557633.07\n",
            "total_cost: 998.99\n",
            "total_trades: 19625\n",
            "Sharpe: 0.360\n",
            "=================================\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2429831650007682\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_2\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.07968116556192399\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 3925, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1584784.27\n",
            "total_reward: 584784.27\n",
            "total_cost: 6091.59\n",
            "total_trades: 8612\n",
            "Sharpe: 0.243\n",
            "=================================\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.02526159458194491\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 648         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.31580883 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 631          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075237844 |\n",
            "|    clip_fraction        | 0.0883       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0394       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.12         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00907     |\n",
            "|    reward               | -0.12546763  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 2.53         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 629         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009793317 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.039       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.24        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -0.74824446 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.01        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 626         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009411968 |\n",
            "|    clip_fraction        | 0.0951      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.00217    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.415       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00815    |\n",
            "|    reward               | -2.0710433  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.38        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 621          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074282335 |\n",
            "|    clip_fraction        | 0.0511       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00396      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.96         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00704     |\n",
            "|    reward               | 0.762229     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.07609387711100718\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 554        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0596    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 12.7       |\n",
            "|    reward             | -1.5585659 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.89       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 559        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -2.38e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 0.892      |\n",
            "|    reward             | 0.37556207 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.436      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 560         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.269      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | -2.81       |\n",
            "|    reward             | -0.43758604 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.206       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 562          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0.121        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -16.5        |\n",
            "|    reward             | -0.001702581 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 2.79         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 562       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.089    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -12.1     |\n",
            "|    reward             | 0.7839972 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 562        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.00298   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 6.84       |\n",
            "|    reward             | 0.14775701 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.938      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 562          |\n",
            "|    iterations         | 700          |\n",
            "|    time_elapsed       | 6            |\n",
            "|    total_timesteps    | 3500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0.152        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 699          |\n",
            "|    policy_loss        | 11.7         |\n",
            "|    reward             | -0.029428765 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 2.33         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 561        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0469    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -1.71      |\n",
            "|    reward             | -0.1097896 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0322     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 561         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0279      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 10.5        |\n",
            "|    reward             | -0.26758456 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.51        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 561         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.139       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 12.7        |\n",
            "|    reward             | -0.04600169 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.48        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 562       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.653    |\n",
            "|    reward             | 1.8596776 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.07      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 562         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.903       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -9.2        |\n",
            "|    reward             | 0.015486788 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.529       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 562        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.446      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 1.34       |\n",
            "|    reward             | 0.36826393 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.031      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 562        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.814     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -0.983     |\n",
            "|    reward             | -0.3018207 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 561       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.549    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -16.6     |\n",
            "|    reward             | -2.769706 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.59      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 560        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.21       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -5.8       |\n",
            "|    reward             | 0.16842233 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.404      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 560         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.351       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -14.3       |\n",
            "|    reward             | 0.045419686 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.82        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 561        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0154     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -6.84      |\n",
            "|    reward             | 0.75411105 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.49       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 561        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0497     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -11.6      |\n",
            "|    reward             | -0.2182294 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.2        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 561         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0502      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -19.9       |\n",
            "|    reward             | -0.26743865 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 3.54        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.22530519081945263\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "day: 3988, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1826160.74\n",
            "total_reward: 826160.74\n",
            "total_cost: 1282.07\n",
            "total_trades: 27752\n",
            "Sharpe: 0.352\n",
            "=================================\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.14781124592942035\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.008817198009606479\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "day: 3988, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2257026.89\n",
            "total_reward: 1257026.89\n",
            "total_cost: 5944.12\n",
            "total_trades: 12533\n",
            "Sharpe: 0.456\n",
            "=================================\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.15851771292468125\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 616         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.14200068 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 592         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007249276 |\n",
            "|    clip_fraction        | 0.0606      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0264     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.799       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00903    |\n",
            "|    reward               | -0.8127902  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.72        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 598         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006560078 |\n",
            "|    clip_fraction        | 0.0494      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0627     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.18        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00569    |\n",
            "|    reward               | 0.7363344   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 9.06        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 605          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071547194 |\n",
            "|    clip_fraction        | 0.0583       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0118       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.04         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00595     |\n",
            "|    reward               | -0.05633922  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.94         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 607        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 16         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00701057 |\n",
            "|    clip_fraction        | 0.0686     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | 0.00891    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 7.12       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0078    |\n",
            "|    reward               | 0.8946437  |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 20.1       |\n",
            "----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.008807015901322962\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 537        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.316     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 7.65       |\n",
            "|    reward             | -1.3021692 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1.84       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 545        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.217      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 1.99       |\n",
            "|    reward             | 0.22924294 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 0.151      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 535        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 0.409      |\n",
            "|    reward             | -0.5331572 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.358      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 536          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | 0.155        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -15.9        |\n",
            "|    reward             | -0.081581675 |\n",
            "|    std                | 0.996        |\n",
            "|    value_loss         | 2.92         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.469     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 2.67       |\n",
            "|    reward             | 0.55549085 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 0.781      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -3.58e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 6.45       |\n",
            "|    reward             | -0.2692534 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.901      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 542       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -5.53     |\n",
            "|    reward             | -0.042421 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 1.58      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 545      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 5.57     |\n",
            "|    reward             | 0.149093 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 0.883    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 545       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -28.8     |\n",
            "|    reward             | 2.1249614 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 8.26      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0635     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 9.59       |\n",
            "|    reward             | -0.6765461 |\n",
            "|    std                | 0.99       |\n",
            "|    value_loss         | 1.63       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 537       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.157     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -21.2     |\n",
            "|    reward             | 0.5936644 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 3.77      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 537         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 2.38e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -2.57       |\n",
            "|    reward             | -0.41248366 |\n",
            "|    std                | 0.989       |\n",
            "|    value_loss         | 0.421       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 15.5     |\n",
            "|    reward             | 1.020599 |\n",
            "|    std                | 0.99     |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0.0642   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -5       |\n",
            "|    reward             | 1.705995 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 0.671    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0.0154   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 30.5     |\n",
            "|    reward             | 0.068884 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 13.4     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 534        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -7.05      |\n",
            "|    reward             | -1.5200765 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 2.84       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 532         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 21.2        |\n",
            "|    reward             | 0.019661361 |\n",
            "|    std                | 0.99        |\n",
            "|    value_loss         | 6.45        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.411     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 3.34       |\n",
            "|    reward             | 0.36365518 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 0.107      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 532         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 17          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 3.76        |\n",
            "|    reward             | -0.27065057 |\n",
            "|    std                | 0.994       |\n",
            "|    value_loss         | 0.177       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 533        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 4.19       |\n",
            "|    reward             | -1.5575241 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 0.261      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3734119779381929\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "day: 4051, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2195320.63\n",
            "total_reward: 1195320.63\n",
            "total_cost: 998.98\n",
            "total_trades: 16204\n",
            "Sharpe: 0.375\n",
            "=================================\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.4019396636877343\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.38235018212829414\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "day: 4051, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2258572.69\n",
            "total_reward: 1258572.69\n",
            "total_cost: 6746.05\n",
            "total_trades: 16635\n",
            "Sharpe: 0.458\n",
            "=================================\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.37830260063611276\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 606         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.34329975 |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 587        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0090636  |\n",
            "|    clip_fraction        | 0.0697     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.054     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.78       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.00725   |\n",
            "|    reward               | -0.6399072 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 2.2        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 585         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007197747 |\n",
            "|    clip_fraction        | 0.0639      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0117     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.16        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00773    |\n",
            "|    reward               | -1.1737723  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.24        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 585         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006893134 |\n",
            "|    clip_fraction        | 0.0595      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00546     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.58        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00988    |\n",
            "|    reward               | 0.27764076  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.27        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 580         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009730036 |\n",
            "|    clip_fraction        | 0.0769      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0258     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.9        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00811    |\n",
            "|    reward               | -0.11133813 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 18.9        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.462216656353345\n",
            "======Best Model Retraining from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 517        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.11       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 0.294      |\n",
            "|    reward             | -0.4484039 |\n",
            "|    std                | 0.992      |\n",
            "|    value_loss         | 0.114      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 4.66      |\n",
            "|    reward             | 0.0555882 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 2.83      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -12.7     |\n",
            "|    reward             | -0.55953  |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.89      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 531       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0249    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -1.58     |\n",
            "|    reward             | 1.0956507 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 2.07      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 533      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.00389 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -5.29    |\n",
            "|    reward             | 0.249828 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.951    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 535        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 2.51       |\n",
            "|    reward             | -0.5223803 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.075      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 19.4      |\n",
            "|    reward             | -0.476944 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 5.78      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 536       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -8.21     |\n",
            "|    reward             | -2.089546 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 3.53      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 538         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -18.2       |\n",
            "|    reward             | -0.62278706 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 7.28        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.373   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -7.03    |\n",
            "|    reward             | 1.570014 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 0.593    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 539        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.148     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 15.1       |\n",
            "|    reward             | 0.58164287 |\n",
            "|    std                | 0.994      |\n",
            "|    value_loss         | 2.04       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 539      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 11.6     |\n",
            "|    reward             | 0.040548 |\n",
            "|    std                | 0.991    |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.419      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -4.67      |\n",
            "|    reward             | 0.03421905 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.232      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.0207    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 11.6      |\n",
            "|    reward             | -0.149559 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 1.95      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -3.27     |\n",
            "|    reward             | -3.223475 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 7.6       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 20.1      |\n",
            "|    reward             | -1.631151 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 9.79      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -0.213    |\n",
            "|    reward             | -0.574099 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.185     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 542        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.122      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 24.2       |\n",
            "|    reward             | -0.4999551 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 4.09       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 542       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 3.59      |\n",
            "|    reward             | 0.9257031 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 0.585     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 543       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.00025   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 8.33      |\n",
            "|    reward             | -0.209199 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.576     |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.4623635486987295\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "day: 4114, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1592255.10\n",
            "total_reward: 592255.10\n",
            "total_cost: 999.00\n",
            "total_trades: 20570\n",
            "Sharpe: 0.290\n",
            "=================================\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.4377005068110703\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.4151685633016971\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "day: 4114, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2045512.50\n",
            "total_reward: 1045512.50\n",
            "total_cost: 6192.66\n",
            "total_trades: 11512\n",
            "Sharpe: 0.335\n",
            "=================================\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.38655438510359547\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 639        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.4060876 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 617         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006514079 |\n",
            "|    clip_fraction        | 0.0821      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0075     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.6         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00834    |\n",
            "|    reward               | 0.06699722  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 3.54        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 611         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008380117 |\n",
            "|    clip_fraction        | 0.0874      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0024     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.78        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00686    |\n",
            "|    reward               | -0.13112289 |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 7.44        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 606          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050897878 |\n",
            "|    clip_fraction        | 0.04         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.0149       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.83         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00765     |\n",
            "|    reward               | -0.5587955   |\n",
            "|    std                  | 0.994        |\n",
            "|    value_loss           | 3.23         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 601         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009051347 |\n",
            "|    clip_fraction        | 0.0764      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0294     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.33        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00805    |\n",
            "|    reward               | 0.37653807  |\n",
            "|    std                  | 0.996       |\n",
            "|    value_loss           | 8.77        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.2941353587254633\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 13         |\n",
            "|    reward             | -0.9496711 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.48       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 539       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 6.47      |\n",
            "|    reward             | 0.196118  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.36      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 524       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 11.9      |\n",
            "|    reward             | -0.957895 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.25      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 526       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 9.42e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -18.7     |\n",
            "|    reward             | -0.525677 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.51      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 528       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -0.848    |\n",
            "|    reward             | 1.531961  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.86      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 530      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -2.59    |\n",
            "|    reward             | 0.585747 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.88     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 532       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -44.4     |\n",
            "|    reward             | -0.043867 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 14.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 6.79      |\n",
            "|    reward             | -0.120264 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 522      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -34.1    |\n",
            "|    reward             | 0.659595 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 5.51     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 525       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 8.7       |\n",
            "|    reward             | 1.3343297 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.53      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 528      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 2.04     |\n",
            "|    reward             | 0.13779  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.546    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 531      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 31.1     |\n",
            "|    reward             | 0.24496  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 8.21     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 533       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 2.26      |\n",
            "|    reward             | 0.19903   |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.216     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 534      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 8.15     |\n",
            "|    reward             | -2.49553 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 536      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 36.8     |\n",
            "|    reward             | -1.8372  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 14.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 537      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 123      |\n",
            "|    reward             | 1.50038  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 132      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 538        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -24        |\n",
            "|    reward             | 0.47601575 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.72       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 540        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 4.59       |\n",
            "|    reward             | 0.54253596 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.407      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 541       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0665   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 10.2      |\n",
            "|    reward             | -1.615308 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.13      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 542       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.018    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -10.6     |\n",
            "|    reward             | -0.320713 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.13      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.43744118529077664\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "day: 4177, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1696516.44\n",
            "total_reward: 696516.44\n",
            "total_cost: 999.00\n",
            "total_trades: 20885\n",
            "Sharpe: 0.384\n",
            "=================================\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.38168789093422406\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.00977530169969494\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "day: 4177, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2470574.79\n",
            "total_reward: 1470574.79\n",
            "total_cost: 4714.08\n",
            "total_trades: 21092\n",
            "Sharpe: 0.434\n",
            "=================================\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.47912957922979843\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 582         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.25171387 |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 576        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 7          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00798858 |\n",
            "|    clip_fraction        | 0.0631     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.00481   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.63       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.00899   |\n",
            "|    reward               | 0.06152781 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 2.71       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 573        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00850741 |\n",
            "|    clip_fraction        | 0.0748     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.0155    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.41       |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.00684   |\n",
            "|    reward               | 0.1938199  |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 3.76       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 574          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.00853684   |\n",
            "|    clip_fraction        | 0.0764       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.124       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.468        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0108      |\n",
            "|    reward               | -0.005371111 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.25         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 569          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068526138 |\n",
            "|    clip_fraction        | 0.0657       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00761     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.288        |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00913     |\n",
            "|    reward               | 0.2789608    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.15         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.30111265011306154\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 465         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0744     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -0.102      |\n",
            "|    reward             | -0.32799995 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.158       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 489        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0316     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -4.46      |\n",
            "|    reward             | 0.06310541 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.89       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 501      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.225   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -13      |\n",
            "|    reward             | -0.56877 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 508       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.228    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -4.32     |\n",
            "|    reward             | 0.9213443 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.74      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 512        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0632     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 3.07       |\n",
            "|    reward             | 0.43442798 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.194      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 515          |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | -0.275       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | -11.2        |\n",
            "|    reward             | -0.005296789 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.55         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 516        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.191      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -4.43      |\n",
            "|    reward             | 0.18346839 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.04       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 2.64       |\n",
            "|    reward             | -1.2299702 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.431      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0498     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -15.2      |\n",
            "|    reward             | 0.74417377 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0537   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 15.9      |\n",
            "|    reward             | 1.1875367 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.35      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 520        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.384      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -7.04      |\n",
            "|    reward             | 0.05061638 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.66       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 520         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.29       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -2.14       |\n",
            "|    reward             | -0.16786832 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0956      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 521          |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -0.537       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | 0.756        |\n",
            "|    reward             | -0.031683505 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.207        |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 522         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0335      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 0.372       |\n",
            "|    reward             | -0.14191742 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.0783      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -0.169    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 1.73      |\n",
            "|    reward             | 0.6538032 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.64      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 522        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.0308     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -20.9      |\n",
            "|    reward             | -1.0559675 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 9.25       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -1.07     |\n",
            "|    reward             | 0.0597328 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.00845   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 522        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.0755    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -28.1      |\n",
            "|    reward             | -1.3922062 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 5.37       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 523        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 18         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.222      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -5.92      |\n",
            "|    reward             | 0.48813745 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.353      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 523       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.00333   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -2.58     |\n",
            "|    reward             | 1.0207553 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.64      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.013355465911799374\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "day: 4240, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1979680.04\n",
            "total_reward: 979680.04\n",
            "total_cost: 1787.66\n",
            "total_trades: 8567\n",
            "Sharpe: 0.329\n",
            "=================================\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.06042995509686995\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.4670903941932387\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "day: 4240, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4681499.60\n",
            "total_reward: 3681499.60\n",
            "total_cost: 6852.76\n",
            "total_trades: 18257\n",
            "Sharpe: 0.554\n",
            "=================================\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.06376378686999808\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 563         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.55926716 |\n",
            "------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 568          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.01013454   |\n",
            "|    clip_fraction        | 0.116        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00237     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.17         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0102      |\n",
            "|    reward               | -0.046834864 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.67         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 507          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071966997 |\n",
            "|    clip_fraction        | 0.0491       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00551     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 12.4         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00415     |\n",
            "|    reward               | 0.29063335   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 16.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 521         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008616567 |\n",
            "|    clip_fraction        | 0.0794      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0283     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.11        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | -1.6961457  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.83        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 532          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075198044 |\n",
            "|    clip_fraction        | 0.0774       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00421      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 6.51         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00682     |\n",
            "|    reward               | -0.32878327  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 16.5         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.06416156933249392\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 515        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0175    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 12.2       |\n",
            "|    reward             | -1.5343903 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.68       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 516        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.858     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 4.91       |\n",
            "|    reward             | 0.21649848 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.826      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 522       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.13     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -7.5      |\n",
            "|    reward             | -0.553678 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.22      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 527         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.121      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -8.89       |\n",
            "|    reward             | -0.12001306 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 1.22        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 529       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -9.14     |\n",
            "|    reward             | 0.6677927 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 0.915     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 531        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 7.37       |\n",
            "|    reward             | 0.08832915 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.814      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 531        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 2.75       |\n",
            "|    reward             | 0.13139978 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.58       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 523         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -11.6       |\n",
            "|    reward             | -0.85717195 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 1.13        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 521         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -6.87       |\n",
            "|    reward             | -0.57295483 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.5         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 520       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 15.3      |\n",
            "|    reward             | 0.2933518 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.29      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 518         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -4.7        |\n",
            "|    reward             | -0.32151332 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 0.544       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 517         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -1.05       |\n",
            "|    reward             | -0.10645197 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 0.253       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 518       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -2.45     |\n",
            "|    reward             | 2.0912514 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 1.99      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 518      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -69.8    |\n",
            "|    reward             | 2.467911 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 42       |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 519       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.324    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 13.9      |\n",
            "|    reward             | 1.3417051 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 519        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -47.9      |\n",
            "|    reward             | -1.5185864 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 18.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 518       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0139    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 7.97      |\n",
            "|    reward             | 1.7436292 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.826     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 518        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.00434    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -11.8      |\n",
            "|    reward             | -2.2332866 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.03       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 518         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.00192    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 7.28        |\n",
            "|    reward             | -0.47666642 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.46        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 519        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.115      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 4.61       |\n",
            "|    reward             | 0.28526646 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.233      |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.31364991388580543\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "day: 4303, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2503148.24\n",
            "total_reward: 1503148.24\n",
            "total_cost: 999.00\n",
            "total_trades: 14655\n",
            "Sharpe: 0.349\n",
            "=================================\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2019894509187507\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.31334680438860796\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "day: 4303, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1738774.66\n",
            "total_reward: 738774.66\n",
            "total_cost: 142844.77\n",
            "total_trades: 28899\n",
            "Sharpe: 0.314\n",
            "=================================\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.3287239023306424\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 605        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 3          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.2540323 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 592         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008028291 |\n",
            "|    clip_fraction        | 0.0896      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0129      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.709       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | -0.16430414 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.76        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 587         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007988666 |\n",
            "|    clip_fraction        | 0.0736      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00483     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.74        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00926    |\n",
            "|    reward               | -0.41017333 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 18.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 582         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012221414 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0321     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.5         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00953    |\n",
            "|    reward               | 0.67925155  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.61        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 575          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083939005 |\n",
            "|    clip_fraction        | 0.0627       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00838     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.31         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0073      |\n",
            "|    reward               | 0.32015985   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 7.85         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.2740633881993897\n",
            "======Best Model Retraining from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  64.15084845791188\n",
            "======Model training from:  2007-06-01 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 507        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.0876    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 16.5       |\n",
            "|    reward             | -0.9845532 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.82       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 507         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 0.749       |\n",
            "|    reward             | 0.030684436 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.289       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 506         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 5.19        |\n",
            "|    reward             | -0.50268185 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.942       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 506         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0641     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -10.3       |\n",
            "|    reward             | -0.31036708 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.89        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 503        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 6.31       |\n",
            "|    reward             | 0.66227037 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.841      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 500        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -16.5      |\n",
            "|    reward             | 0.17444237 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.94       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 502       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0339    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -5.2      |\n",
            "|    reward             | 0.2162939 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.03      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 505        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.421      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 0.65       |\n",
            "|    reward             | -0.9227723 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.153      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 506        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 15.7       |\n",
            "|    reward             | 0.24612454 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.63       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 507         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.369       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 8.07        |\n",
            "|    reward             | -0.48580462 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.707       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 508      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0287  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -33.7    |\n",
            "|    reward             | 3.43087  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 509      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.0541  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 25.4     |\n",
            "|    reward             | 0.572885 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 6.06     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 510       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -17       |\n",
            "|    reward             | 0.5943329 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.05      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 511         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.186       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | 11.1        |\n",
            "|    reward             | -0.18084551 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.853       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 510        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 8.45       |\n",
            "|    reward             | -0.7960272 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.788      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 506       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.154    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -14.4     |\n",
            "|    reward             | 0.8245418 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 1.99      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 505        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -2.53      |\n",
            "|    reward             | -0.3812592 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 1.24       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 505        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.469     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -7.55      |\n",
            "|    reward             | -1.1062795 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 1.3        |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 507          |\n",
            "|    iterations         | 1900         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 9500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.3        |\n",
            "|    explained_variance | 5.96e-08     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1899         |\n",
            "|    policy_loss        | -7.19        |\n",
            "|    reward             | -0.007239658 |\n",
            "|    std                | 0.996        |\n",
            "|    value_loss         | 1.24         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 508         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 19          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -9.79       |\n",
            "|    reward             | -0.43279395 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 1.22        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3090556397040913\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "day: 4366, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3770997.48\n",
            "total_reward: 2770997.48\n",
            "total_cost: 998.98\n",
            "total_trades: 8732\n",
            "Sharpe: 0.609\n",
            "=================================\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.07008560058677021\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.06499327745653968\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "day: 4366, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2478049.59\n",
            "total_reward: 1478049.59\n",
            "total_cost: 6530.61\n",
            "total_trades: 27655\n",
            "Sharpe: 0.372\n",
            "=================================\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.013362721178195579\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 572         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 3           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.18399318 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006914639 |\n",
            "|    clip_fraction        | 0.0852      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0466     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.25        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00689    |\n",
            "|    reward               | 0.005362075 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.95        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 589          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058662747 |\n",
            "|    clip_fraction        | 0.0615       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00456      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.18         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0048      |\n",
            "|    reward               | -0.15460208  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 7.54         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 591        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 13         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00515138 |\n",
            "|    clip_fraction        | 0.0516     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.4      |\n",
            "|    explained_variance   | -0.00156   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.64       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.00571   |\n",
            "|    reward               | 3.971336   |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 4.48       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 592          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054629147 |\n",
            "|    clip_fraction        | 0.0454       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0286      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.77         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    reward               | -0.5179742   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 19.1         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.1277749595041351\n",
            "======Best Model Retraining from:  2007-06-01 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  25.289164717992147  minutes\n",
            "[INFO] Portfolio value plot saved to: 2007-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2007-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2007-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2007-2025_no_crypto/\n",
            "[INFO] Moved results to 2007-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_0 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_0,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2007-06-01',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_0,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_0,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2007-2025_no_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "Successfully added technical indicators\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 756         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.377       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -1.81       |\n",
            "|    reward             | -0.17797141 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0282      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 762         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 12.1        |\n",
            "|    reward             | 0.029903462 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.33        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 767        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0139     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 78.9       |\n",
            "|    reward             | -2.6021614 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 49.7       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 767         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.202       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | -67         |\n",
            "|    reward             | -0.08699477 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 23.9        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 765         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.0408      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 10.4        |\n",
            "|    reward             | 0.123103976 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.897       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.094     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -2.03     |\n",
            "|    reward             | 0.5210909 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.108     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 761        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.18      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 4.11       |\n",
            "|    reward             | 0.30931392 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.11       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -1.81      |\n",
            "|    reward             | 0.18810019 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0399     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 742         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -1.18       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 0.953       |\n",
            "|    reward             | -0.16400997 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0307      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 745         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -1.02       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 5.49        |\n",
            "|    reward             | -0.17895092 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.214       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.0301    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.385    |\n",
            "|    reward             | 0.9064174 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.14      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 748         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 20.5        |\n",
            "|    reward             | 0.111108854 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.31        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 747         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.356      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 9.5         |\n",
            "|    reward             | 0.005289252 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.644       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 745         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0.583       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | -14.4       |\n",
            "|    reward             | -0.37775484 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.22        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 745        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.0265    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -9.74      |\n",
            "|    reward             | 0.50333136 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 8.49       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -2.36      |\n",
            "|    reward             | 0.93884486 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.118      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 739       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.218     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 34.8      |\n",
            "|    reward             | 0.1088886 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 8.05      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 738      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.101    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 26.4     |\n",
            "|    reward             | -2.25751 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 14.1     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 737       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.00746   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 1.37e+03  |\n",
            "|    reward             | 25.750715 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.32e+04  |\n",
            "-------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6759170.19\n",
            "total_reward: 5759170.19\n",
            "total_cost: 1760580.47\n",
            "total_trades: 10911\n",
            "Sharpe: 0.848\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 737         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 16.9        |\n",
            "|    reward             | 0.089328066 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.93        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.5040329517619582\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 25976116.85\n",
            "total_reward: 24976116.85\n",
            "total_cost: 998.91\n",
            "total_trades: 9955\n",
            "Sharpe: 0.966\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 268       |\n",
            "|    time_elapsed    | 29        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 601       |\n",
            "|    critic_loss     | 8.89e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | 10.461132 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.44507412540064983\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1289567.70\n",
            "total_reward: 289567.70\n",
            "total_cost: 999.00\n",
            "total_trades: 9970\n",
            "Sharpe: 0.280\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 253       |\n",
            "|    time_elapsed    | 31        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 115       |\n",
            "|    critic_loss     | 212       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.313064 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.09798209835003081\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 23540752.23\n",
            "total_reward: 22540752.23\n",
            "total_cost: 1335.77\n",
            "total_trades: 9984\n",
            "Sharpe: 0.951\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 170      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 7980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.46e+03 |\n",
            "|    critic_loss     | 1.78e+04 |\n",
            "|    ent_coef        | 0.975    |\n",
            "|    ent_coef_loss   | 2.18     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7879     |\n",
            "|    reward          | 9.01653  |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  0.4415476844643299\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 11931738.09\n",
            "total_reward: 10931738.09\n",
            "total_cost: 1590251.20\n",
            "total_trades: 17260\n",
            "Sharpe: 0.977\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 953        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.09826931 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 917          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0078047956 |\n",
            "|    clip_fraction        | 0.0848       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00028      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.53e+03     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00841     |\n",
            "|    reward               | 0.98252225   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.5e+03      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 909         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010173177 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00283     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 459         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00998    |\n",
            "|    reward               | -0.93710685 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1e+03       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 902         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005831225 |\n",
            "|    clip_fraction        | 0.0741      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000897    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.22e+03    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00476    |\n",
            "|    reward               | -0.05199646 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.08e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 901         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009957199 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.9       |\n",
            "|    explained_variance   | -0.0327     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.5        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    reward               | 0.7685071   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 45.2        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.42419800739862973\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 748         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.000223    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -6.62       |\n",
            "|    reward             | -0.22989872 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.346       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 760         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.0072      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 8.21        |\n",
            "|    reward             | -0.22773914 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 0.963       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 69.7       |\n",
            "|    reward             | -2.2653816 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 40.5       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 766      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 25       |\n",
            "|    reward             | 2.200497 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 5.14     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 766         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.979      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -4.4        |\n",
            "|    reward             | -0.99046236 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.196       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 767       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -20.9     |\n",
            "|    reward             | -1.708785 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.87      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 768       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 0.635     |\n",
            "|    reward             | 1.8857774 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.423     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.0461   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -69.1     |\n",
            "|    reward             | 1.5920433 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 27.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 769        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.0848    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -0.802     |\n",
            "|    reward             | -0.3206851 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 0.203      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 769       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.239    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -9.56     |\n",
            "|    reward             | 0.3348059 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.756     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 769         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.232       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 0.106       |\n",
            "|    reward             | 0.024337862 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.000984    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 770       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.382    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 6.06      |\n",
            "|    reward             | 0.3312507 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.307     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 770         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.106      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -1.1        |\n",
            "|    reward             | -0.11947851 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.302       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -3.29      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 26.4       |\n",
            "|    reward             | 0.30969238 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.67       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 771       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0604    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -34.2     |\n",
            "|    reward             | 3.2763054 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 13.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 771       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0169    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -56.5     |\n",
            "|    reward             | 1.0255526 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 26.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 770       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.00506  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 27.3      |\n",
            "|    reward             | 0.8767259 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 8         |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 770        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0541     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 3.82       |\n",
            "|    reward             | -0.7728955 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 771         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.139       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 0.246       |\n",
            "|    reward             | -0.05845037 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.46        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 771       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.0869   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 54.1      |\n",
            "|    reward             | 2.0396206 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 21        |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.3186358317200639\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 276       |\n",
            "|    time_elapsed    | 29        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 609       |\n",
            "|    critic_loss     | 6.69e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | 47.546185 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.09461815913161091\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 229      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 8232     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 278      |\n",
            "|    critic_loss     | 4.87e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8131     |\n",
            "|    reward          | 0.261    |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.1441712928399153\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 193       |\n",
            "|    time_elapsed    | 42        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.69e+03  |\n",
            "|    critic_loss     | 8.15e+03  |\n",
            "|    ent_coef        | 1.07      |\n",
            "|    ent_coef_loss   | -5.39     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | 43.940083 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.09577554051052524\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 896       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -2.096058 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 876         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007924167 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000757   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 55.4        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00868    |\n",
            "|    reward               | 0.25484812  |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 102         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008690916 |\n",
            "|    clip_fraction        | 0.0726      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.7       |\n",
            "|    explained_variance   | -0.000164   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 861         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00799    |\n",
            "|    reward               | -2.3173482  |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 1.57e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 847         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011167362 |\n",
            "|    clip_fraction        | 0.0897      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00242    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 201         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00775    |\n",
            "|    reward               | 0.19077379  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 372         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 844         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007614629 |\n",
            "|    clip_fraction        | 0.099       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00659    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.3        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    reward               | -0.46959952 |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 25.7        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.09531338544301658\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 731         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -1.57       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -2.03       |\n",
            "|    reward             | -0.07575925 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0607      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 730         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.425      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 3.01        |\n",
            "|    reward             | 0.008942552 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.107       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.106     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 41.2       |\n",
            "|    reward             | -1.6079067 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 13.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 733       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 17        |\n",
            "|    reward             | 1.836698  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.21      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.0142    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 0.394      |\n",
            "|    reward             | 0.32114527 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0176     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 738         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -5.13       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 4.46        |\n",
            "|    reward             | -0.71572363 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.45        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 21.4       |\n",
            "|    reward             | -1.2492353 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.57       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 740      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.0355  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -34.7    |\n",
            "|    reward             | 1.290295 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 7.61     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 740        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 0.414      |\n",
            "|    reward             | -0.0408361 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.171      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 742        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -3.38      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -3.44      |\n",
            "|    reward             | -1.5828433 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.208      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 743        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0.388      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -15.3      |\n",
            "|    reward             | -1.5269878 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.48       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 744         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13         |\n",
            "|    explained_variance | -0.016      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -62.1       |\n",
            "|    reward             | 0.005918519 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 22.7        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 744       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.0264    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -22.5     |\n",
            "|    reward             | -9.669995 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 7.14      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0.1       |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 152       |\n",
            "|    reward             | 25.893578 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 153       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0.0127    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 790       |\n",
            "|    reward             | 3.4407713 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.88e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 745       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0.000748  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 8.18e+03  |\n",
            "|    reward             | -95.98399 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.67e+05  |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 746         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -13         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 4.1         |\n",
            "|    reward             | -0.17619075 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.457       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0.00448  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -89.6    |\n",
            "|    reward             | 4.601235 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 85.8     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 747        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0.0248     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -138       |\n",
            "|    reward             | -2.8693717 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 147        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 747       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -13       |\n",
            "|    explained_variance | 0.000601  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 5.99e+03  |\n",
            "|    reward             | 14.638208 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.61e+05  |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.3791382720647386\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 279       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 110       |\n",
            "|    critic_loss     | 745       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.048237 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.3984246560566943\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 225        |\n",
            "|    time_elapsed    | 37         |\n",
            "|    total_timesteps | 8484       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 494        |\n",
            "|    critic_loss     | 1.12e+04   |\n",
            "|    learning_rate   | 0.001      |\n",
            "|    n_updates       | 8383       |\n",
            "|    reward          | -72.941765 |\n",
            "-----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.14092858392122765\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 179       |\n",
            "|    time_elapsed    | 47        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.93e+03  |\n",
            "|    critic_loss     | 1.18e+03  |\n",
            "|    ent_coef        | 1.04      |\n",
            "|    ent_coef_loss   | -2.95     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -0.794494 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.521978038673806\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    fps             | 890      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "| train/             |          |\n",
            "|    reward          | -9.26369 |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 857         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006408216 |\n",
            "|    clip_fraction        | 0.0773      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00219    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 216         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00611    |\n",
            "|    reward               | -7.5544643  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 411         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 845         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008627659 |\n",
            "|    clip_fraction        | 0.0597      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.000492   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.53e+03    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00846    |\n",
            "|    reward               | 0.33442664  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.11e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 842         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008561941 |\n",
            "|    clip_fraction        | 0.0672      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000363    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.73e+03    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00727    |\n",
            "|    reward               | -0.90777445 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.15e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 841         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007821141 |\n",
            "|    clip_fraction        | 0.0808      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000339    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.46e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00832    |\n",
            "|    reward               | -0.7185701  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.34e+03    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.3134512245048947\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 631         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.653      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -9.18       |\n",
            "|    reward             | -0.33739245 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.429       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 678          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.8        |\n",
            "|    explained_variance | 0.0389       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 16.1         |\n",
            "|    reward             | -0.010919532 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 2.02         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 697       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0591    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 63.5      |\n",
            "|    reward             | -1.442257 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 36.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 706       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 33.6      |\n",
            "|    reward             | 3.2169826 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.71      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 710         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 10.4        |\n",
            "|    reward             | -0.47484905 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.648       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 714        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -15.7      |\n",
            "|    reward             | -0.8149576 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.4        |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 717      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.0343  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 28.6     |\n",
            "|    reward             | 1.275114 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 7.81     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 719       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.0383    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -31.3     |\n",
            "|    reward             | 3.3636346 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 9.3       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 720      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.5     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -7.44    |\n",
            "|    reward             | 1.54742  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 720       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.058     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | 551       |\n",
            "|    reward             | 18.148113 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.82e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.00116   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -1.01e+03 |\n",
            "|    reward             | 76.158424 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.33e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -5.52e+03 |\n",
            "|    reward             | 521.33575 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.21e+05  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.00485   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | -1.45e+03  |\n",
            "|    reward             | -170.70729 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.44e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 723       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0131   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 134       |\n",
            "|    reward             | 2.3335433 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 139       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0336   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -344      |\n",
            "|    reward             | 10.538395 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 706       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 724       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.00124  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 1.89e+03  |\n",
            "|    reward             | 20.772636 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.36e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 724      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.00014 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -415     |\n",
            "|    reward             | 119.5855 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.93e+03 |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 725         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.0246     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -12.3       |\n",
            "|    reward             | -0.15137242 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 5.77        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 725        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.000268  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -5.2e+03   |\n",
            "|    reward             | -108.35532 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.5e+05    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 726       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.00471  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 81.4      |\n",
            "|    reward             | 13.248472 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.8e+03   |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.24048642829922784\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 261      |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 605      |\n",
            "|    critic_loss     | 5.05e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 40.66811 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.525690999901955\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 223      |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 699      |\n",
            "|    critic_loss     | 1.87e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 50.81268 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.523228605208474\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 183       |\n",
            "|    time_elapsed    | 47        |\n",
            "|    total_timesteps | 8736      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.02e+03  |\n",
            "|    critic_loss     | 5.57e+04  |\n",
            "|    ent_coef        | 1.3       |\n",
            "|    ent_coef_loss   | -22.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8635      |\n",
            "|    reward          | 106.38751 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.5208266453413336\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 876        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -11.554981 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 846          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076247826 |\n",
            "|    clip_fraction        | 0.105        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000189     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 362          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00691     |\n",
            "|    reward               | -2.0397635   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 668          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 838         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007063357 |\n",
            "|    clip_fraction        | 0.0592      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.000503    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.54e+03    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    reward               | -34.576984  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.13e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 834          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069683446 |\n",
            "|    clip_fraction        | 0.042        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000601    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.32e+04     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00536     |\n",
            "|    reward               | 258.13376    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.76e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 824          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029030433 |\n",
            "|    clip_fraction        | 0.00474      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.000149    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.51e+04     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.000999    |\n",
            "|    reward               | 13.065504    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 9.02e+04     |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.27205644540141916\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 709       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | -0.167    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 50.7      |\n",
            "|    reward             | 1.0298321 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 26.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 717        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.00205   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -221       |\n",
            "|    reward             | -1.8771073 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 548        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 715        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -0.000607  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 5.22e+03   |\n",
            "|    reward             | -141.87051 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.13e+05   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 717       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0967    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 334       |\n",
            "|    reward             | 26.031021 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 750       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 716         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | -0.0544     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -6.24       |\n",
            "|    reward             | -0.58178985 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 12.3        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 718       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.000832 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -2.36e+03 |\n",
            "|    reward             | 4.7857738 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.34e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 720      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.00358  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 622      |\n",
            "|    reward             | 5.927092 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 6.91e+03 |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 720        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 2.65e-05   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -4.28e+03  |\n",
            "|    reward             | -109.56475 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.51e+05   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 719         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0.00375     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 2.06e+03    |\n",
            "|    reward             | 0.109179586 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 4.54e+04    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 720        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -0.349     |\n",
            "|    reward             | 0.80193937 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.202      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 721      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.00634  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -0.783   |\n",
            "|    reward             | 0.79808  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.194    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 0.0382     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 7.16       |\n",
            "|    reward             | -1.4472389 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.24       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 722       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0186   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 65.2      |\n",
            "|    reward             | 3.8043122 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 35.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.172    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 22        |\n",
            "|    reward             | 0.5016889 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.16      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 721       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.0978    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -24.7     |\n",
            "|    reward             | 1.7977173 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.89      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 722      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -103     |\n",
            "|    reward             | 1.094004 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 57.9     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 722        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -20.6      |\n",
            "|    reward             | -1.8202223 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 5.58       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 721        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -13        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -5.14      |\n",
            "|    reward             | 0.21173196 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.172      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 720      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 87.5     |\n",
            "|    reward             | 0.414909 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 78       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 719      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -13      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 8.42     |\n",
            "|    reward             | 37.63015 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 73.7     |\n",
            "------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.2550449725688527\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 264       |\n",
            "|    time_elapsed    | 34        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -300      |\n",
            "|    critic_loss     | 1.87e+04  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 226.21294 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3871136771781304\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 237       |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 126       |\n",
            "|    critic_loss     | 1.79e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | 146.19746 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.3900737382416362\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 171       |\n",
            "|    time_elapsed    | 52        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 600       |\n",
            "|    critic_loss     | 1.18e+03  |\n",
            "|    ent_coef        | 0.0432    |\n",
            "|    ent_coef_loss   | -15.9     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.528841 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.3792383255701847\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 808         |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 2           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.49878135 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 802         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009792926 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00326     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 36.6        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00887    |\n",
            "|    reward               | 0.13146392  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 92.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 804          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066352417 |\n",
            "|    clip_fraction        | 0.0715       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.0173       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 52.7         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00741     |\n",
            "|    reward               | 1.9669516    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 92.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 804          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060479613 |\n",
            "|    clip_fraction        | 0.0521       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00502      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 71.2         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00514     |\n",
            "|    reward               | 7.473453     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 138          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 805          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058832243 |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00504      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 850          |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00528     |\n",
            "|    reward               | -7.218342    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.78e+03     |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.36032111039842407\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 706         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.522      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -7.19       |\n",
            "|    reward             | -0.18795511 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.674       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 711         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.059       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 12.5        |\n",
            "|    reward             | 0.009405455 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 1.53        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 704       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0.0215    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 0.442     |\n",
            "|    reward             | 1.1647627 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 1.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 707       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.244    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 24.8      |\n",
            "|    reward             | 3.0003705 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 5.34      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 707      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.025    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -2.63    |\n",
            "|    reward             | 1.322701 |\n",
            "|    std                | 0.992    |\n",
            "|    value_loss         | 0.101    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 709       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.74     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -8.66     |\n",
            "|    reward             | -0.803556 |\n",
            "|    std                | 0.992     |\n",
            "|    value_loss         | 0.621     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 711        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -1.37      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 3.85       |\n",
            "|    reward             | 0.23748218 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.141      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 711       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -0.00656  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -7.98     |\n",
            "|    reward             | 0.3177687 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 1.06      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 711         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | 14.8        |\n",
            "|    reward             | -0.58815175 |\n",
            "|    std                | 0.99        |\n",
            "|    value_loss         | 1.49        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 711         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -1.49       |\n",
            "|    reward             | -0.44182068 |\n",
            "|    std                | 0.991       |\n",
            "|    value_loss         | 0.383       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 711       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0.0548    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -10.2     |\n",
            "|    reward             | 1.5594593 |\n",
            "|    std                | 0.991     |\n",
            "|    value_loss         | 0.934     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 712       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -6.69     |\n",
            "|    reward             | 0.8470547 |\n",
            "|    std                | 0.99      |\n",
            "|    value_loss         | 0.544     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 712       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -24.6     |\n",
            "|    reward             | 1.0550058 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 4.7       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 712        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | 0.34       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -13.5      |\n",
            "|    reward             | -0.4523384 |\n",
            "|    std                | 0.991      |\n",
            "|    value_loss         | 1.56       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 713         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | -0.0248     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -9.68       |\n",
            "|    reward             | -0.69960004 |\n",
            "|    std                | 0.997       |\n",
            "|    value_loss         | 1.17        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 713      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.217   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 11.2     |\n",
            "|    reward             | 1.319327 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 0.938    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 713         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | 6.03        |\n",
            "|    reward             | 0.012446496 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.813       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 712      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 10.6     |\n",
            "|    reward             | 0.138577 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 711      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | -0.0205  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -20.1    |\n",
            "|    reward             | 1.252002 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 711         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -11.1       |\n",
            "|    reward             | -0.29972142 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.45        |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  -0.1990868767422171\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 257        |\n",
            "|    time_elapsed    | 35         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 111        |\n",
            "|    critic_loss     | 3.53e+04   |\n",
            "|    learning_rate   | 0.0005     |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -140.49573 |\n",
            "-----------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.22783479416581204\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 236      |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 102      |\n",
            "|    critic_loss     | 2.62e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 0.405076 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.07425212675896002\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    episodes        | 4          |\n",
            "|    fps             | 180        |\n",
            "|    time_elapsed    | 51         |\n",
            "|    total_timesteps | 9240       |\n",
            "| train/             |            |\n",
            "|    actor_loss      | 3.05e+03   |\n",
            "|    critic_loss     | 9.28e+03   |\n",
            "|    ent_coef        | 1.33       |\n",
            "|    ent_coef_loss   | -24.4      |\n",
            "|    learning_rate   | 0.0003     |\n",
            "|    n_updates       | 9139       |\n",
            "|    reward          | -85.527405 |\n",
            "-----------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  -0.22328598149516352\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 848       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -7.638381 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 792         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009321595 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.0064     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.5        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    reward               | -3.4014206  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 85.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 792         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009977137 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 0.00183     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 87.9        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0101     |\n",
            "|    reward               | -2.4597054  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 158         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 792         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009201856 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00379    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00789    |\n",
            "|    reward               | 1.4323655   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 123         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 790         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009439923 |\n",
            "|    clip_fraction        | 0.0963      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.9       |\n",
            "|    explained_variance   | -0.00279    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 238         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00737    |\n",
            "|    reward               | -0.05495167 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 399         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  -0.10092138802989305\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 668         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.547      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -9.11       |\n",
            "|    reward             | -0.33214232 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.574       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 680        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -1.92      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 13.8       |\n",
            "|    reward             | 0.14951561 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 1.88       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 663         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.469      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 24.3        |\n",
            "|    reward             | -0.16156696 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 4.76        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 669       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 14.6      |\n",
            "|    reward             | 0.8910369 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.64      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 674         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | 0.271       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | -6.51       |\n",
            "|    reward             | -0.10253951 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.924       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 675        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.286      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 1.74       |\n",
            "|    reward             | 0.20090014 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0843     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 678         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.185      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -21.7       |\n",
            "|    reward             | -0.65495586 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.86        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 681       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.00268  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 3.3       |\n",
            "|    reward             | 2.7468593 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.931     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 683      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.9    |\n",
            "|    explained_variance | 0.113    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -14.4    |\n",
            "|    reward             | 0.647445 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 684        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.818     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 3.23       |\n",
            "|    reward             | 0.14021388 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.97       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 684        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.24       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 13         |\n",
            "|    reward             | -1.3622075 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.9        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 674        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | -0.0238    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 20         |\n",
            "|    reward             | -2.5581212 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.76       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 672       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -0.0312   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 1.42      |\n",
            "|    reward             | -2.199488 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.77      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 671        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 15.6       |\n",
            "|    reward             | -1.2815479 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.06       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 672        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | -1.06      |\n",
            "|    reward             | 0.83062106 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.044      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 667       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.018     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 16.3      |\n",
            "|    reward             | -0.220742 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.92      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 668        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 15.9       |\n",
            "|    reward             | -0.9948699 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.18       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 664        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.9      |\n",
            "|    explained_variance | 0.0131     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 6.61       |\n",
            "|    reward             | -1.7038151 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.21       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 666       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | -7.53     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -60       |\n",
            "|    reward             | 0.3681971 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 54.1      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 667         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.9       |\n",
            "|    explained_variance | 0.153       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -5.95       |\n",
            "|    reward             | 0.002319642 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.425       |\n",
            "---------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.11068057982853198\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 236      |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 600      |\n",
            "|    critic_loss     | 1.34e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.275984 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3543695142861412\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 208      |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 117      |\n",
            "|    critic_loss     | 4.21e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 8.18223  |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.09760326687546775\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 180      |\n",
            "|    time_elapsed    | 52       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 556      |\n",
            "|    critic_loss     | 9.07e+04 |\n",
            "|    ent_coef        | 0.0243   |\n",
            "|    ent_coef_loss   | -39.9    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 19.92131 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.09360088062992279\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 823        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -13.584081 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 799         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007911196 |\n",
            "|    clip_fraction        | 0.0789      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -0.00233    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 312         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00753    |\n",
            "|    reward               | -62.45635   |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 618         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 787          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057032583 |\n",
            "|    clip_fraction        | 0.0572       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.000114     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.64e+03     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00734     |\n",
            "|    reward               | 9.055056     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.99e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 785          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072275987 |\n",
            "|    clip_fraction        | 0.0496       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 6.49e-05     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 7.46e+03     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0054      |\n",
            "|    reward               | 11.929547    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.54e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 784         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004994434 |\n",
            "|    clip_fraction        | 0.0124      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | -1.65e-05   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.95e+04    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00387    |\n",
            "|    reward               | -116.55817  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.28e+05    |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.24138372718085163\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.26612245502895\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 670         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -8.69       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -3.02       |\n",
            "|    reward             | -0.36415604 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.103       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 681       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.9     |\n",
            "|    explained_variance | 0.155     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 6.87      |\n",
            "|    reward             | -0.007759 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.677     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 668        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0014     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 45.6       |\n",
            "|    reward             | -1.1248217 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 21.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 674       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 19.7      |\n",
            "|    reward             | 2.4213617 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.22      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 676        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | -4.33      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -5.49      |\n",
            "|    reward             | -0.8498263 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.333      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 677         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -0.297      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 7.96        |\n",
            "|    reward             | -0.64239246 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 0.692       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 680        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -11.5      |\n",
            "|    reward             | -0.9093698 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.988      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 682       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 8.41      |\n",
            "|    reward             | -1.496612 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.606     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 684         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.8       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -16.5       |\n",
            "|    reward             | -0.81126463 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 3.22        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 684        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.7      |\n",
            "|    explained_variance | -0.313     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 3          |\n",
            "|    reward             | 0.21000129 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 0.804      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 685       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 1.51      |\n",
            "|    reward             | -0.083445 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 0.0662    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 686      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0.0136   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -14.8    |\n",
            "|    reward             | 0.836    |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 686      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -27.7    |\n",
            "|    reward             | 1.786045 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 5.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 687      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | -0.052   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 41.8     |\n",
            "|    reward             | 1.058102 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 8.98     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 686         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.7       |\n",
            "|    explained_variance | 0.188       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | 10          |\n",
            "|    reward             | -0.31338128 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 0.794       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 687       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -0.81     |\n",
            "|    reward             | -0.642923 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.0659    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 688       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 2.22      |\n",
            "|    reward             | 0.384689  |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.159     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 688      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 13.2     |\n",
            "|    reward             | 1.100264 |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 688      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 34.8     |\n",
            "|    reward             | 0.535378 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 9.79     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 688        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.8      |\n",
            "|    explained_variance | 0.0871     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 22.5       |\n",
            "|    reward             | -1.7261928 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 3.85       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.17722536703465183\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 263      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.27     |\n",
            "|    critic_loss     | 13.7     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.778185 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.09559286847569312\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 220       |\n",
            "|    time_elapsed    | 44        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 354       |\n",
            "|    critic_loss     | 6.71e+04  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 247.50491 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.4832663299537755\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 180       |\n",
            "|    time_elapsed    | 54        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 120       |\n",
            "|    critic_loss     | 1.22e+03  |\n",
            "|    ent_coef        | 0.0387    |\n",
            "|    ent_coef_loss   | -39.1     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | 1.6942811 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  0.4759185579527504\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 812       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -18.66837 |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 785          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067812055 |\n",
            "|    clip_fraction        | 0.0896       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -7.52e-05    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 385          |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00835     |\n",
            "|    reward               | 29.26827     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 636          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 774         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005690974 |\n",
            "|    clip_fraction        | 0.0188      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12.8       |\n",
            "|    explained_variance   | 5.38e-05    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.95e+03    |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00337    |\n",
            "|    reward               | 7.9259834   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.07e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 756          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076740743 |\n",
            "|    clip_fraction        | 0.072        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | -0.0011      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.62e+03     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00481     |\n",
            "|    reward               | -0.018897856 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.07e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 753          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054104384 |\n",
            "|    clip_fraction        | 0.0369       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -12.8        |\n",
            "|    explained_variance   | 0.00187      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.2e+03      |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    reward               | 0.24126327   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.51e+04     |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.26846338337495557\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  21.980212946732838  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2015-2025_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_crypto/\n",
            "[INFO] Moved results to 2015-2025_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "\n",
        "ensemble_agent_1 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_1,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_1,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_1,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully added technical indicators\n",
            "[Info] df_7day_full is empty. Skipping technical indicators.\n",
            "Successfully added turbulence index\n",
            "[INIT] Unique trade dates from 2023-01-04 to 2025-04-11\n",
            "[INIT] Total trading days: 570\n",
            "[INIT] First 5 dates: <DatetimeArray>\n",
            "['2023-01-04 00:00:00', '2023-01-05 00:00:00', '2023-01-06 00:00:00',\n",
            " '2023-01-09 00:00:00', '2023-01-10 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "[INIT] Last 5 dates: <DatetimeArray>\n",
            "['2025-04-07 00:00:00', '2025-04-08 00:00:00', '2025-04-09 00:00:00',\n",
            " '2025-04-10 00:00:00', '2025-04-11 00:00:00']\n",
            "Length: 5, dtype: datetime64[ns]\n",
            "\n",
            "[INIT PREVIEW] ===== Rebalancing Schedule Summary =====\n",
            "Iter     Train End    Val Start    Val End      Trade Start  Trade End   \n",
            "126      2023-01-04 00:00:00 2023-01-04 00:00:00 2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00\n",
            "189      2023-04-05 00:00:00 2023-04-05 00:00:00 2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00\n",
            "252      2023-07-07 00:00:00 2023-07-07 00:00:00 2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00\n",
            "315      2023-10-05 00:00:00 2023-10-05 00:00:00 2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00\n",
            "378      2024-01-05 00:00:00 2024-01-05 00:00:00 2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00\n",
            "441      2024-04-08 00:00:00 2024-04-08 00:00:00 2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00\n",
            "504      2024-07-09 00:00:00 2024-07-09 00:00:00 2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00\n",
            "567      2024-10-07 00:00:00 2024-10-07 00:00:00 2025-01-07 00:00:00 2025-01-07 00:00:00 2025-04-09 00:00:00\n",
            "[INFO] Running Ensemble Strategy...\n",
            "============Start Ensemble Strategy============\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-01-04 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_126_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 761         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | 0.0683      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.45       |\n",
            "|    reward             | -0.29848948 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.206       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 786          |\n",
            "|    iterations         | 200          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 1000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0.0651       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 199          |\n",
            "|    policy_loss        | 13           |\n",
            "|    reward             | -0.017307136 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 1.42         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 797         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0433      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 16.9        |\n",
            "|    reward             | 0.046518434 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.62        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 799          |\n",
            "|    iterations         | 400          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 2000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.4        |\n",
            "|    explained_variance | 0.27         |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 399          |\n",
            "|    policy_loss        | -25.3        |\n",
            "|    reward             | -0.043420453 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 4.32         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 803       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.299    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -1.12     |\n",
            "|    reward             | 0.4858169 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.278     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 808        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.244      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -1.21      |\n",
            "|    reward             | 0.75460786 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.132      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 812       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0499   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 6.91      |\n",
            "|    reward             | 0.4034552 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.597     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 814         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -3.38       |\n",
            "|    reward             | -0.11275472 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0965      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 816       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.72      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 3.78      |\n",
            "|    reward             | 0.4804537 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.197     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 817         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0275     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 23.1        |\n",
            "|    reward             | -0.70572895 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 5.91        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 818       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0297    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 13.4      |\n",
            "|    reward             | 2.3405423 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.5       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 818         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | -2          |\n",
            "|    reward             | 0.001943641 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0404      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 819        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 2.3        |\n",
            "|    reward             | 0.39852998 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.262      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 820         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.445      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | -5.31       |\n",
            "|    reward             | -0.31668246 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.217       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 821       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.123     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 6.02      |\n",
            "|    reward             | 1.2497816 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.637     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 821        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -0.507     |\n",
            "|    reward             | -0.1413691 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.00578    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 822        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.602     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 2.33       |\n",
            "|    reward             | 0.30129763 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0665     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 823       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 0.773     |\n",
            "|    reward             | -0.016356 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.053     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 823       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0.722     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 5.15      |\n",
            "|    reward             | -0.666631 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.214     |\n",
            "-------------------------------------\n",
            "day: 1994, episode: 5\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 898521.91\n",
            "total_reward: -101478.09\n",
            "total_cost: 26153.09\n",
            "total_trades: 10879\n",
            "Sharpe: -0.030\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 823        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -1.88      |\n",
            "|    reward             | -0.2552256 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.0225     |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.5832082389071588\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
            "day: 1994, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1352141.74\n",
            "total_reward: 352141.74\n",
            "total_cost: 999.00\n",
            "total_trades: 11945\n",
            "Sharpe: 0.355\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 269       |\n",
            "|    time_elapsed    | 29        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.38      |\n",
            "|    critic_loss     | 609       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.097946 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.06852036624106059\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_126_1\n",
            "day: 1994, episode: 15\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1477704.76\n",
            "total_reward: 477704.76\n",
            "total_cost: 999.00\n",
            "total_trades: 9970\n",
            "Sharpe: 0.443\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 246       |\n",
            "|    time_elapsed    | 32        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 129       |\n",
            "|    critic_loss     | 1.79e+03  |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.038786 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.20644756751924745\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_126_1\n",
            "day: 1994, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1502235.40\n",
            "total_reward: 502235.40\n",
            "total_cost: 998.92\n",
            "total_trades: 1994\n",
            "Sharpe: 0.347\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 191       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 7980      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.58e+03  |\n",
            "|    critic_loss     | 375       |\n",
            "|    ent_coef        | 1.03      |\n",
            "|    ent_coef_loss   | -2.47     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 7879      |\n",
            "|    reward          | -0.856024 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.054017329044657544\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_126_1\n",
            "day: 1994, episode: 25\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 962213.66\n",
            "total_reward: -37786.34\n",
            "total_cost: 127506.53\n",
            "total_trades: 15349\n",
            "Sharpe: 0.001\n",
            "=================================\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 994        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | 0.08029089 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 948         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010391332 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0818     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.52        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    reward               | -0.22064056 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.02        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 953          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058317743 |\n",
            "|    clip_fraction        | 0.0483       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0222      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.38         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00677     |\n",
            "|    reward               | -0.6505638   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.26         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 936         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006765048 |\n",
            "|    clip_fraction        | 0.0721      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0083      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.29        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00948    |\n",
            "|    reward               | -0.71216595 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.52        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 943          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0090058055 |\n",
            "|    clip_fraction        | 0.0903       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00492      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.87         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0124      |\n",
            "|    reward               | -0.09116718  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.67         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2023-01-04 00:00:00 to  2023-04-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.16215492018142905\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======Trading from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-04-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_189_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 754        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.495     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -6.53      |\n",
            "|    reward             | -0.1911984 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.242      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 777        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.302     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 15.6       |\n",
            "|    reward             | 0.21648127 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.28       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 783        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.08      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 44         |\n",
            "|    reward             | -1.3839101 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 21         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 783         |\n",
            "|    iterations         | 400         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 2000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.45       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 399         |\n",
            "|    policy_loss        | 30          |\n",
            "|    reward             | 0.032025423 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 10          |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 774        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.222     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 6          |\n",
            "|    reward             | 0.64346015 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.265      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 781        |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.92      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | -1.93      |\n",
            "|    reward             | 0.05276627 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.14       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 786       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.553    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -0.899577 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.71      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 789       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -33.2     |\n",
            "|    reward             | 0.8013655 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 11.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 789        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -0.905     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 4.5        |\n",
            "|    reward             | 0.42218786 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.817      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 785         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -4.07       |\n",
            "|    reward             | -0.49498406 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.189       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 790       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 48.6      |\n",
            "|    reward             | 1.3034757 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 21.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 793       |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.021    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | -19.1     |\n",
            "|    reward             | 1.3911749 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.01      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 795        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -0.0827    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 1.06       |\n",
            "|    reward             | -1.7124089 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.641      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 797        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 3.24       |\n",
            "|    reward             | -0.4430789 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.206      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 800       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -17.1     |\n",
            "|    reward             | -0.625295 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.83      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -37.5    |\n",
            "|    reward             | 0.517384 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 15.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 804       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | 0.0069    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 18.5      |\n",
            "|    reward             | 0.4651189 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.59      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 806        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 1.71       |\n",
            "|    reward             | -0.7662846 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.79       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 808        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0127     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 4.5        |\n",
            "|    reward             | 0.15863857 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.3        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 810       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0261   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 37        |\n",
            "|    reward             | 1.0748016 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 16.7      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.169474642936047\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 265       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -70.6     |\n",
            "|    critic_loss     | 7.47e+03  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -1.119908 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.23689383078090726\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 258       |\n",
            "|    time_elapsed    | 31        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -52.3     |\n",
            "|    critic_loss     | 5.62      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -2.592765 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.16446642080015653\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_189_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 8232      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.58e+03  |\n",
            "|    critic_loss     | 202       |\n",
            "|    ent_coef        | 1.13      |\n",
            "|    ent_coef_loss   | -9.17     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8131      |\n",
            "|    reward          | -1.094159 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "sac Sharpe Ratio:  0.22679552131337383\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_189_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 1022       |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.1514812 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 982          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073824683 |\n",
            "|    clip_fraction        | 0.082        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.0139      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.31         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0121      |\n",
            "|    reward               | 0.018890234  |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.35         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 948          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008416838  |\n",
            "|    clip_fraction        | 0.0822       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0458      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.35         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00678     |\n",
            "|    reward               | -0.029537741 |\n",
            "|    std                  | 0.997        |\n",
            "|    value_loss           | 5.24         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 939         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007290085 |\n",
            "|    clip_fraction        | 0.0557      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.00123    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.05        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00776    |\n",
            "|    reward               | 0.87470883  |\n",
            "|    std                  | 0.996       |\n",
            "|    value_loss           | 4.95        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 928         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009725724 |\n",
            "|    clip_fraction        | 0.0959      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.00529     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.42        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -0.336889   |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 8.1         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-04-05 00:00:00 to  2023-07-07 00:00:00\n",
            "ppo Sharpe Ratio:  0.2004878841861889\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======Trading from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-07-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_252_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 771         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.2        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -9.25       |\n",
            "|    reward             | -0.37301737 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.74        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 679         |\n",
            "|    iterations         | 200         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 1000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.00867     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 199         |\n",
            "|    policy_loss        | 8.41        |\n",
            "|    reward             | 0.028715467 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.818       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 685         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.415      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 31.2        |\n",
            "|    reward             | -0.95633805 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 12.9        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 703       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.027    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 20.8      |\n",
            "|    reward             | 1.6336548 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.25      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 715       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 1.1       |\n",
            "|    reward             | 0.5499838 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0557    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 722         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -0.749      |\n",
            "|    reward             | -0.64451826 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0555      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 729        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.447      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 8.6        |\n",
            "|    reward             | -0.7328613 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.811      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 739         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.531      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -2          |\n",
            "|    reward             | -0.24278809 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.288       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 6         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.331    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 5.78      |\n",
            "|    reward             | 0.3515361 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.406     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 746        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 5.36e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -4.57      |\n",
            "|    reward             | -1.7308164 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.438      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 744        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.807     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 1.09       |\n",
            "|    reward             | -0.7082889 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.089      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 747        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.13      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -26.3      |\n",
            "|    reward             | -0.7215337 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.81       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 744         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.634      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | 2.34        |\n",
            "|    reward             | -0.24663599 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.112       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 746        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -2.07      |\n",
            "|    reward             | 0.18963614 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0728     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 746       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.46      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -12.1     |\n",
            "|    reward             | 1.4149636 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.32      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 750        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.000114  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -6.49      |\n",
            "|    reward             | 0.40393046 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.429      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 754         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -0.681      |\n",
            "|    reward             | 0.037417382 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.00879     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 756         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.0369      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | 7.11        |\n",
            "|    reward             | 0.028863339 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.484       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 759         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.0286     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -5.41       |\n",
            "|    reward             | 0.064550065 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.262       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 761        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0902    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 31.4       |\n",
            "|    reward             | 0.58325046 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 10.9       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "a2c Sharpe Ratio:  -0.5442813571482181\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 278       |\n",
            "|    time_elapsed    | 30        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 510       |\n",
            "|    critic_loss     | 304       |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.195106 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.3906591510284237\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 243       |\n",
            "|    time_elapsed    | 34        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -62       |\n",
            "|    critic_loss     | 913       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.039131 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "td3 Sharpe Ratio:  -0.48269891152517663\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_252_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 184       |\n",
            "|    time_elapsed    | 45        |\n",
            "|    total_timesteps | 8484      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 116       |\n",
            "|    critic_loss     | 1.75e+03  |\n",
            "|    ent_coef        | 0.0946    |\n",
            "|    ent_coef_loss   | -7.09     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 8383      |\n",
            "|    reward          | -1.228834 |\n",
            "----------------------------------\n",
            "======sac Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "sac Sharpe Ratio:  -0.3708598445933421\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_252_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 908        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.7602585 |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 894          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074304976 |\n",
            "|    clip_fraction        | 0.0926       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0115       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.53         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.0112      |\n",
            "|    reward               | -2.6392608   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.66         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 875         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009624266 |\n",
            "|    clip_fraction        | 0.0812      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0316     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.71        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00823    |\n",
            "|    reward               | -1.0961194  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.74        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 862        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 9          |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00704486 |\n",
            "|    clip_fraction        | 0.0509     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.3      |\n",
            "|    explained_variance   | -0.053     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.628      |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.00938   |\n",
            "|    reward               | -0.6470383 |\n",
            "|    std                  | 0.998      |\n",
            "|    value_loss           | 1.79       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 870         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007385088 |\n",
            "|    clip_fraction        | 0.0873      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.0304      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.724       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00777    |\n",
            "|    reward               | -0.935997   |\n",
            "|    std                  | 0.997       |\n",
            "|    value_loss           | 1.94        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-07-07 00:00:00 to  2023-10-05 00:00:00\n",
            "ppo Sharpe Ratio:  -0.4041504723028163\n",
            "======Best Model Retraining from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======Trading from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2023-10-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_315_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 753         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.924      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | 1.35        |\n",
            "|    reward             | -0.16050884 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.04        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 765        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.387     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 8.84       |\n",
            "|    reward             | 0.20387338 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.698      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 734        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.0475    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 42.1       |\n",
            "|    reward             | -1.4511364 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 15.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 743       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.0609    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 18        |\n",
            "|    reward             | 1.4240606 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.47      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 747         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 8.22        |\n",
            "|    reward             | -0.71847206 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.554       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 754         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.225      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | -17.2       |\n",
            "|    reward             | -0.80224186 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.81        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 759       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.29     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 9.86      |\n",
            "|    reward             | 0.5686009 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.95      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0314     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -30.4      |\n",
            "|    reward             | 0.14786747 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 12.3       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 764         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -2.66       |\n",
            "|    reward             | -0.56456035 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0869      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 766        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 2.7        |\n",
            "|    reward             | 0.17634562 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.133      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 762        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -4.78      |\n",
            "|    reward             | -1.3093892 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.267      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.661     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -23.5      |\n",
            "|    reward             | -0.8953983 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.1        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 762        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.104     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 5.43       |\n",
            "|    reward             | 0.03817859 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.307      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 762        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -8.31      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -2.53      |\n",
            "|    reward             | -0.4141244 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0705     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 761         |\n",
            "|    iterations         | 1500        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 7500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1499        |\n",
            "|    policy_loss        | -0.321      |\n",
            "|    reward             | 0.022968484 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.00164     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 762        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0.0337     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 2.97       |\n",
            "|    reward             | -0.5610167 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.137      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 764          |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | 1.37         |\n",
            "|    reward             | -0.088662624 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.111        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 765          |\n",
            "|    iterations         | 1800         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 9000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -0.356       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1799         |\n",
            "|    policy_loss        | -9.54        |\n",
            "|    reward             | -0.018070787 |\n",
            "|    std                | 1.02         |\n",
            "|    value_loss         | 0.943        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 766        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | 0.553      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -0.288     |\n",
            "|    reward             | 0.00204413 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.00723    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 767        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 0.209      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -1.71      |\n",
            "|    reward             | 0.08969032 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.0319     |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "a2c Sharpe Ratio:  0.5061262302531282\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 267      |\n",
            "|    time_elapsed    | 32       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 253      |\n",
            "|    critic_loss     | 561      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.23255  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ddpg Sharpe Ratio:  0.3564047448843993\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 224      |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.06    |\n",
            "|    critic_loss     | 27.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.913314 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "td3 Sharpe Ratio:  0.45159721713521617\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_315_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 175      |\n",
            "|    time_elapsed    | 49       |\n",
            "|    total_timesteps | 8736     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.6e+03  |\n",
            "|    critic_loss     | 279      |\n",
            "|    ent_coef        | 1.15     |\n",
            "|    ent_coef_loss   | -10.7    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8635     |\n",
            "|    reward          | 0.993695 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "sac Sharpe Ratio:  0.332977146811218\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_315_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 919        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.1072762 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 891         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006990457 |\n",
            "|    clip_fraction        | 0.0721      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0507     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.92        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00596    |\n",
            "|    reward               | -0.55809265 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.86        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 888          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069613517 |\n",
            "|    clip_fraction        | 0.0824       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0277      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.13         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00854     |\n",
            "|    reward               | -0.20755391  |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 2.69         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 873         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009195718 |\n",
            "|    clip_fraction        | 0.0903      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0027     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.51        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | -0.8273245  |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 3.84        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 870         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009963023 |\n",
            "|    clip_fraction        | 0.0916      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.00491    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.38        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 0.24540137  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.6         |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2023-10-05 00:00:00 to  2024-01-05 00:00:00\n",
            "ppo Sharpe Ratio:  0.29919537697008197\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======Trading from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-01-05 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_378_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 750         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -2.51       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -6.75       |\n",
            "|    reward             | -0.11654618 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.428       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 746        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.09       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 9.83       |\n",
            "|    reward             | 0.16014437 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.04       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 746      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.074   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 39.3     |\n",
            "|    reward             | -1.16728 |\n",
            "|    std                | 0.999    |\n",
            "|    value_loss         | 17.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 748       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0246   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 13        |\n",
            "|    reward             | 2.1875157 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.49      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 750        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.0168     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -1.64      |\n",
            "|    reward             | 0.55751663 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.71       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 742         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.234      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 18.6        |\n",
            "|    reward             | -0.87649673 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 2.35        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 739        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.215      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | 15.4       |\n",
            "|    reward             | -1.2171457 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 737        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.321     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -23.7      |\n",
            "|    reward             | -3.1906543 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 11.7       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 737         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | -0.119      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -84.7       |\n",
            "|    reward             | 0.109291844 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 71.6        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 735        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.364     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 5.75       |\n",
            "|    reward             | 0.70199966 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.799      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0538    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -2.08      |\n",
            "|    reward             | 0.95299566 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.526      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.107     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | 5.5        |\n",
            "|    reward             | -0.3213225 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.544      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 727       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0479   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 40.3      |\n",
            "|    reward             | 2.3723495 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 17.3      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0619    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | -2.17      |\n",
            "|    reward             | -0.9926866 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.771      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 728       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0651   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -15.6     |\n",
            "|    reward             | 2.3116817 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.29      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 729       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.264    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -38.3     |\n",
            "|    reward             | 0.6091775 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 731        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.052     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | -41.5      |\n",
            "|    reward             | -0.7658843 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 15.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 728        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -1.66      |\n",
            "|    reward             | -0.0470637 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.0398     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 730        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.197     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 4.07       |\n",
            "|    reward             | 0.44724655 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.203      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 731        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.246      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 13.9       |\n",
            "|    reward             | 0.12890811 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.81       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "a2c Sharpe Ratio:  0.35737294693678123\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 243       |\n",
            "|    time_elapsed    | 36        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -20.2     |\n",
            "|    critic_loss     | 1.25      |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.299747 |\n",
            "----------------------------------\n",
            "======ddpg Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ddpg Sharpe Ratio:  0.5545375665490655\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 228       |\n",
            "|    time_elapsed    | 39        |\n",
            "|    total_timesteps | 8988      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 22.5      |\n",
            "|    critic_loss     | 437       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 8887      |\n",
            "|    reward          | -0.190249 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "td3 Sharpe Ratio:  0.17124474713317756\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_378_1\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 189         |\n",
            "|    time_elapsed    | 47          |\n",
            "|    total_timesteps | 8988        |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 97.9        |\n",
            "|    critic_loss     | 1.06e+03    |\n",
            "|    ent_coef        | 0.0359      |\n",
            "|    ent_coef_loss   | -37         |\n",
            "|    learning_rate   | 0.0003      |\n",
            "|    n_updates       | 8887        |\n",
            "|    reward          | -0.14566903 |\n",
            "------------------------------------\n",
            "======sac Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "sac Sharpe Ratio:  0.34047126719519427\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_378_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 962       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -0.946928 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 923         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005048267 |\n",
            "|    clip_fraction        | 0.0632      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0446     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.26        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0079     |\n",
            "|    reward               | 0.03201306  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.11        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 911          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067418693 |\n",
            "|    clip_fraction        | 0.0627       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0171       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.62         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00894     |\n",
            "|    reward               | 0.9117035    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.01         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 889         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008197576 |\n",
            "|    clip_fraction        | 0.0733      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0349      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.69        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00877    |\n",
            "|    reward               | 1.0986354   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 5.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 890         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008349579 |\n",
            "|    clip_fraction        | 0.0642      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00515     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.83        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00738    |\n",
            "|    reward               | 0.13675301  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 8.59        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-01-05 00:00:00 to  2024-04-08 00:00:00\n",
            "ppo Sharpe Ratio:  0.34939455122674506\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======Trading from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-04-08 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_441_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 559        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -4.33      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -2.95      |\n",
            "|    reward             | -0.3599732 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 0.199      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 573        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.034     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 12.5       |\n",
            "|    reward             | 0.10569645 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.48       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 614        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.14       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 14.6       |\n",
            "|    reward             | 0.35462543 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 2.75       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 648      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.3    |\n",
            "|    explained_variance | -0.085   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 20.4     |\n",
            "|    reward             | 3.502912 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 671       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | 0.068     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -3.97     |\n",
            "|    reward             | 1.3850356 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.252     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 687       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -4.09     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -4.9      |\n",
            "|    reward             | -0.267163 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 0.204     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 693       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.07     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 8.08      |\n",
            "|    reward             | 0.2824717 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 0.967     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 695       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.3     |\n",
            "|    explained_variance | -0.498    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 2.23      |\n",
            "|    reward             | 0.1657413 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 0.425     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 701        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -0.549     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -3.32      |\n",
            "|    reward             | -0.5779747 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.74       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 699         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -3.34       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | -1.91       |\n",
            "|    reward             | 0.027980251 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 0.0636      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 689         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -189        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | -1.7        |\n",
            "|    reward             | 0.061831627 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.0511      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 684         |\n",
            "|    iterations         | 1200        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 6000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -2.03       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1199        |\n",
            "|    policy_loss        | 0.0514      |\n",
            "|    reward             | 0.017449705 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.000468    |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 672      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -3.4     |\n",
            "|    reward             | 0.220566 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 0.129    |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 660         |\n",
            "|    iterations         | 1400        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 7000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.5       |\n",
            "|    explained_variance | -0.403      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1399        |\n",
            "|    policy_loss        | -27.8       |\n",
            "|    reward             | 0.025962565 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 5.83        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 663       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.375    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 0.806     |\n",
            "|    reward             | -0.107857 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.103     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 669      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 9.67     |\n",
            "|    reward             | 1.360614 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.779    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 664       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.019    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 5.18      |\n",
            "|    reward             | -0.000869 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.43      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 661        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0121    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 15.5       |\n",
            "|    reward             | 0.20135583 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.65       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 666      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.025   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -25.5    |\n",
            "|    reward             | 1.715011 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 6.1      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 661       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -13.5     |\n",
            "|    reward             | -0.465844 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.66      |\n",
            "-------------------------------------\n",
            "======a2c Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "a2c Sharpe Ratio:  0.3737478144722796\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 253      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -269     |\n",
            "|    critic_loss     | 1.08e+03 |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.77944  |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ddpg Sharpe Ratio:  0.25060661488002717\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 245      |\n",
            "|    time_elapsed    | 37       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 29.3     |\n",
            "|    critic_loss     | 24       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 1.657872 |\n",
            "---------------------------------\n",
            "======td3 Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "td3 Sharpe Ratio:  0.021039468605255927\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_441_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 192      |\n",
            "|    time_elapsed    | 47       |\n",
            "|    total_timesteps | 9240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.97e+03 |\n",
            "|    critic_loss     | 1.29e+03 |\n",
            "|    ent_coef        | 1.45     |\n",
            "|    ent_coef_loss   | -27.7    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9139     |\n",
            "|    reward          | 0.955282 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "sac Sharpe Ratio:  0.057294188418895386\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_441_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 935       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -2.855682 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 901         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010455113 |\n",
            "|    clip_fraction        | 0.111       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0221      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.87        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    reward               | -3.083436   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.11        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 892          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064454423 |\n",
            "|    clip_fraction        | 0.0552       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | -0.00508     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.13         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00608     |\n",
            "|    reward               | -1.4594098   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.32         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 887         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008331833 |\n",
            "|    clip_fraction        | 0.0437      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0136     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.86        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00715    |\n",
            "|    reward               | 0.13792528  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 884         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009677999 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00287     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.19        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -0.01056728 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.32        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-04-08 00:00:00 to  2024-07-09 00:00:00\n",
            "ppo Sharpe Ratio:  0.15760170182109015\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======Trading from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-07-09 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_504_1\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 756        |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.168      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -0.756     |\n",
            "|    reward             | -0.6628135 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.139      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 762       |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.472    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 9.52      |\n",
            "|    reward             | 0.1541977 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.51      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.0359    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 49.3       |\n",
            "|    reward             | -1.2777771 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 18.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0.191     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 30.8      |\n",
            "|    reward             | 0.6696037 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.239     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -5.28      |\n",
            "|    reward             | -0.1089195 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.64       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 764         |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 3           |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.567       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 8.87        |\n",
            "|    reward             | 0.053506427 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.762       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 764         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -0.618      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | 3.84        |\n",
            "|    reward             | -0.84410155 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.226       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.31     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -2.54     |\n",
            "|    reward             | 1.9358674 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.72      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.00544  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 7.6       |\n",
            "|    reward             | 0.6504425 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.47      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 763        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.272     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 3.92       |\n",
            "|    reward             | 0.23182356 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.764      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 764        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -0.467     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | 22.5       |\n",
            "|    reward             | -1.3316969 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.52       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 764      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0.15     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 10.6     |\n",
            "|    reward             | -2.85779 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 0.91     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.388    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 27.7      |\n",
            "|    reward             | 0.1758874 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.73      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 765       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0301   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -17.1     |\n",
            "|    reward             | -1.442637 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.12      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.0819   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 4.74      |\n",
            "|    reward             | 0.9817644 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.224     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 764        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 31.2       |\n",
            "|    reward             | -0.8053866 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.68       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 765        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.303      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 18.7       |\n",
            "|    reward             | -0.8664948 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.67       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 765         |\n",
            "|    iterations         | 1800        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 9000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1799        |\n",
            "|    policy_loss        | -12.7       |\n",
            "|    reward             | -0.37623686 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 2.62        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 764       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 12        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.013    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | -15.3     |\n",
            "|    reward             | 0.1395891 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 5.06      |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 764          |\n",
            "|    iterations         | 2000         |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 10000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.5        |\n",
            "|    explained_variance | -0.938       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1999         |\n",
            "|    policy_loss        | -8.21        |\n",
            "|    reward             | -0.042842392 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 0.751        |\n",
            "----------------------------------------\n",
            "======a2c Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "a2c Sharpe Ratio:  0.283337863585579\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.51    |\n",
            "|    critic_loss     | 139      |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.387125 |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ddpg Sharpe Ratio:  0.2019894509187507\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_504_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 249       |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 9492      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 133       |\n",
            "|    critic_loss     | 55.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9391      |\n",
            "|    reward          | -0.227272 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "td3 Sharpe Ratio:  0.2184795398798295\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_504_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 193      |\n",
            "|    time_elapsed    | 48       |\n",
            "|    total_timesteps | 9492     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.65e+03 |\n",
            "|    critic_loss     | 9.66e+03 |\n",
            "|    ent_coef        | 1.56     |\n",
            "|    ent_coef_loss   | -43.6    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9391     |\n",
            "|    reward          | 0.29504  |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "sac Sharpe Ratio:  0.23289672419984328\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_504_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 922        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.1655016 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 875         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008733722 |\n",
            "|    clip_fraction        | 0.089       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.031      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.21        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | -1.0302244  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 3.64        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 870          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064260736 |\n",
            "|    clip_fraction        | 0.087        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | 0.00369      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.98         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00836     |\n",
            "|    reward               | 0.20148546   |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 5.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 858          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006030678  |\n",
            "|    clip_fraction        | 0.067        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.3        |\n",
            "|    explained_variance   | -0.0101      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.35         |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00521     |\n",
            "|    reward               | -0.047678128 |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 2.87         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 859         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006590176 |\n",
            "|    clip_fraction        | 0.0652      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.029      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.68        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | 0.27446684  |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 2.35        |\n",
            "-----------------------------------------\n",
            "======ppo Validation from:  2024-07-09 00:00:00 to  2024-10-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.023579559264053494\n",
            "======Best Model Retraining from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======Trading from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "============================================\n",
            "turbulence_threshold:  73.03914298739384\n",
            "======Model training from:  2015-02-02 to  2024-10-07 00:00:00\n",
            "======a2c Training========\n",
            "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/a2c/a2c_567_1\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 742         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.3       |\n",
            "|    explained_variance | 4.77e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -4.85       |\n",
            "|    reward             | -0.39627975 |\n",
            "|    std                | 0.995       |\n",
            "|    value_loss         | 0.24        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 751        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 9.23       |\n",
            "|    reward             | 0.14537042 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1          |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 754        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.3      |\n",
            "|    explained_variance | 0.25       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 42.5       |\n",
            "|    reward             | -1.2703701 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 16.3       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 756      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 33.8     |\n",
            "|    reward             | 1.590223 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 7.55     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 756        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0259     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 0.318      |\n",
            "|    reward             | -0.6053732 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.0215     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 758       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | -0.237    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 3.77      |\n",
            "|    reward             | -0.415737 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.424     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 759         |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -10.3       |\n",
            "|    reward             | -0.90048546 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.858       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 759        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 5          |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 9.24       |\n",
            "|    reward             | -1.5027983 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.992      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 760         |\n",
            "|    iterations         | 900         |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 4500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 899         |\n",
            "|    policy_loss        | -15.2       |\n",
            "|    reward             | -0.81602323 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.74        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 4.26     |\n",
            "|    reward             | 0.154872 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.564    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 760       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 1.37      |\n",
            "|    reward             | -0.052781 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0609    |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 761      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -14.6    |\n",
            "|    reward             | 0.815292 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -26.6    |\n",
            "|    reward             | 1.77895  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 5.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.00179 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 32.5     |\n",
            "|    reward             | 1.074798 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 9.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 760      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -1.13    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 0.635    |\n",
            "|    reward             | -0.28679 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 0.287    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 760       |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 0.635     |\n",
            "|    reward             | -0.573131 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.0744    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 760        |\n",
            "|    iterations         | 1700       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 8500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 1.79e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1699       |\n",
            "|    policy_loss        | 2.14       |\n",
            "|    reward             | 0.36913183 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.151      |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 761      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.4    |\n",
            "|    explained_variance | -0.186   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 9.04     |\n",
            "|    reward             | 1.205281 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 761      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 30.2     |\n",
            "|    reward             | 0.760816 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 10.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 761        |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | 14.9       |\n",
            "|    reward             | -1.5839579 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 2.48       |\n",
            "--------------------------------------\n",
            "======a2c Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "a2c Sharpe Ratio:  -0.07839807202607886\n",
            "======ddpg Training========\n",
            "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 264      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 73.2     |\n",
            "|    critic_loss     | 22.1     |\n",
            "|    learning_rate   | 0.0005   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.019    |\n",
            "---------------------------------\n",
            "======ddpg Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ddpg Sharpe Ratio:  -0.001603320001007641\n",
            "======td3 Training========\n",
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/td3/td3_567_1\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 234       |\n",
            "|    time_elapsed    | 41        |\n",
            "|    total_timesteps | 9744      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 435       |\n",
            "|    critic_loss     | 7.9e+03   |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 9643      |\n",
            "|    reward          | -0.200275 |\n",
            "----------------------------------\n",
            "======td3 Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "td3 Sharpe Ratio:  -0.17624985101365845\n",
            "======sac Training========\n",
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/sac/sac_567_1\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 190      |\n",
            "|    time_elapsed    | 51       |\n",
            "|    total_timesteps | 9744     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.29e+03 |\n",
            "|    critic_loss     | 106      |\n",
            "|    ent_coef        | 1.74     |\n",
            "|    ent_coef_loss   | -41.6    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9643     |\n",
            "|    reward          | 1.987924 |\n",
            "---------------------------------\n",
            "======sac Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "sac Sharpe Ratio:  -0.03358875233877692\n",
            "======ppo Training========\n",
            "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to tensorboard_log/ppo/ppo_567_1\n",
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 908        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -2.4245667 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 871         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009249937 |\n",
            "|    clip_fraction        | 0.0885      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.00125     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.24        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00859    |\n",
            "|    reward               | 0.01149663  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 4.21        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 864         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009185607 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | -0.0335     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.54        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    reward               | -0.8431773  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.48        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 852         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008185618 |\n",
            "|    clip_fraction        | 0.0633      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0249      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.11        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00734    |\n",
            "|    reward               | 0.03132801  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 2.41        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 852          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064238915 |\n",
            "|    clip_fraction        | 0.0562       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.00362      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.6          |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00719     |\n",
            "|    reward               | 0.33843774   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 4.47         |\n",
            "------------------------------------------\n",
            "======ppo Validation from:  2024-10-07 00:00:00 to  2025-01-07 00:00:00\n",
            "ppo Sharpe Ratio:  -0.09727979479390492\n",
            "======Best Model Retraining from:  2015-02-02 to  2025-01-07 00:00:00\n",
            "======Trading from:  2025-01-07 00:00:00 to  2025-04-09 00:00:00\n",
            "Ensemble Strategy took:  21.04033549626668  minutes\n",
            "[INFO] Portfolio value plot saved to: 2015-2025_no_crypto/portfolio_value_plot.png\n",
            "[INFO] Daily return saved to: 2015-2025_no_crypto/df_daily_return_ensemble.csv\n",
            "[INFO] Moved trained_models to 2015-2025_no_crypto/\n",
            "[INFO] Moved tensorboard_log to 2015-2025_no_crypto/\n",
            "[INFO] Moved results to 2015-2025_no_crypto/\n"
          ]
        }
      ],
      "source": [
        "processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')\n",
        "\n",
        "ensemble_agent_2 = setup_drl_ensemble_agent(\n",
        "    processed_df = processed_2,\n",
        "    indicators = INDICATORS,\n",
        "    train_start_date = '2015-02-02',\n",
        "    train_end_date = '2023-01-03',\n",
        "    trade_start_date = '2023-01-04',\n",
        "    trade_end_date = '2025-04-11',\n",
        "    rebalance_window = 63,\n",
        "    validation_window = 63\n",
        ")\n",
        "\n",
        "df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "    ensemble_agent=ensemble_agent_2,\n",
        "    A2C_kwargs=A2C_model_kwargs,\n",
        "    PPO_kwargs=PPO_model_kwargs,\n",
        "    DDPG_kwargs=DDPG_model_kwargs,\n",
        "    SAC_kwargs=SAC_model_kwargs,\n",
        "    TD3_kwargs=TD3_model_kwargs,\n",
        "    timesteps_dict=timesteps_dict,\n",
        "    processed_df=processed_2,\n",
        "    trade_start_date='2023-01-04',\n",
        "    trade_end_date='2025-04-11',\n",
        "    rebalance_window=63,\n",
        "    validation_window=63,\n",
        "    output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "    initial_fund=1_000_000,\n",
        "    original_csv_path=\"2015-2025_no_crypto.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "#     ensemble_agent = ensemble_agent,\n",
        "#     A2C_kwargs = A2C_model_kwargs,\n",
        "#     PPO_kwargs = PPO_model_kwargs,\n",
        "#     DDPG_kwargs = DDPG_model_kwargs,\n",
        "#     SAC_kwargs = SAC_model_kwargs,\n",
        "#     TD3_kwargs = TD3_model_kwargs,\n",
        "#     timesteps_dict = timesteps_dict,\n",
        "#     processed_df = processed,\n",
        "#     trade_start_date = '2023-01-04',\n",
        "#     trade_end_date = '2025-04-11',\n",
        "#     rebalance_window = 63,\n",
        "#     validation_window = 63,\n",
        "#     output_csv_name = \"df_daily_return_ensemble.csv\"\n",
        "# )\n",
        "\n",
        "# df_daily_return = run_ensemble_and_generate_daily_return(\n",
        "#     ensemble_agent=ensemble_agent_0,\n",
        "#     A2C_kwargs=A2C_model_kwargs,\n",
        "#     PPO_kwargs=PPO_model_kwargs,\n",
        "#     DDPG_kwargs=DDPG_model_kwargs,\n",
        "#     SAC_kwargs=SAC_model_kwargs,\n",
        "#     TD3_kwargs=TD3_model_kwargs,\n",
        "#     timesteps_dict=timesteps_dict,\n",
        "#     processed_df=processed_0,\n",
        "#     trade_start_date='2023-01-04',\n",
        "#     trade_end_date='2025-04-11',\n",
        "#     rebalance_window=63,\n",
        "#     validation_window=63,\n",
        "#     output_csv_name=\"df_daily_return_ensemble.csv\",\n",
        "#     initial_fund=1_000_000,\n",
        "#     original_csv_path=\"2007-2025_no_crypto.csv\"\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# processed_0 = process_csv_to_features('2007-2025_no_crypto.csv')\n",
        "# processed_1 = process_csv_to_features('2015-2025_crypto.csv')\n",
        "# processed_2 = process_csv_to_features('2015-2025_no_crypto.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
