{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*.\n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0vEcPxSJ8hI",
        "outputId": "01b6f59b-11a7-4eb4-821e-878373799d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: wrds in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
            "Requirement already satisfied: packaging<=24.2 in /usr/local/lib/python3.11/dist-packages (from wrds) (24.2)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from wrds) (2.2.2)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/dist-packages (from wrds) (2.9.10)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /usr/local/lib/python3.11/dist-packages (from wrds) (2.0.40)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3,>=2.2->wrds) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3,>=2.2->wrds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3,>=2.2->wrds) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.1,>=2->wrds) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.1,>=2->wrds) (4.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.17.0)\n",
            "Requirement already satisfied: pyportfolioopt in /usr/local/lib/python3.11/dist-packages (1.5.6)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (2.0.14)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (2.2.2)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt) (1.14.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt) (3.2.7.post2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyportfolioopt) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyportfolioopt) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt) (24.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt) (75.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt) (3.0.2)\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-l9b0lcvf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-l9b0lcvf\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 6a0881d9f6f0d03e5af0d486c5a2ecf9f47e3f00\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-qnlfl58x/elegantrl_bc4998a4d832459b8c940990a8a3401c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-qnlfl58x/elegantrl_bc4998a4d832459b8c940990a8a3401c\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: alpaca-py<0.38,>=0.37 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.37.0)\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (3.1.60)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (1.9.7)\n",
            "Requirement already satisfied: pyfolio<0.10,>=0.9 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.9.2)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (1.5.6)\n",
            "Requirement already satisfied: ray<3,>=2 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.44.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (1.6.1)\n",
            "Requirement already satisfied: selenium<5,>=4 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (4.31.0)\n",
            "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.5.4)\n",
            "Requirement already satisfied: webdriver-manager<5,>=4 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (4.0.2)\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (3.3.0)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.0.2)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.11.15)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (75.2.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.3)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.40)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (1.14.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.8) (0.5.5)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (5.29.4)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.5.0)\n",
            "Requirement already satisfied: aiohttp_cors in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.1.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.30.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (18.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.3.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium<5,>=4->finrl==0.3.8) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium<5,>=4->finrl==0.3.8) (4.13.1)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (1.1.0)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /usr/local/lib/python3.11/dist-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.3)\n",
            "Requirement already satisfied: th in /usr/local/lib/python3.11/dist-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.11/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.24.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.17.2)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /usr/local/lib/python3.11/dist-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.8) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "!pip install swig\n",
        "!pip install wrds\n",
        "!pip install pyportfolioopt\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt1317y2ixSS",
        "outputId": "fd2302f4-c8bd-4097-c6ce-acbb143390b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.0.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.3)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "!pip install pandas_market_calendars\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process:\n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "mFCP1YEhi6oi",
        "outputId": "9b1616cb-73c8-4da7-ef40-677ed79fd73f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tic</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>day</th>\n",
              "      <th>macd</th>\n",
              "      <th>boll_ub</th>\n",
              "      <th>boll_lb</th>\n",
              "      <th>rsi_30</th>\n",
              "      <th>cci_30</th>\n",
              "      <th>dx_30</th>\n",
              "      <th>close_30_sma</th>\n",
              "      <th>close_60_sma</th>\n",
              "      <th>vix</th>\n",
              "      <th>turbulence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-05-30</td>\n",
              "      <td>agg</td>\n",
              "      <td>99.38000</td>\n",
              "      <td>99.47000</td>\n",
              "      <td>99.27000</td>\n",
              "      <td>99.45000</td>\n",
              "      <td>977600.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.581127</td>\n",
              "      <td>98.958873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>99.38000</td>\n",
              "      <td>99.38000</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-05-30</td>\n",
              "      <td>bil</td>\n",
              "      <td>91.60000</td>\n",
              "      <td>91.60000</td>\n",
              "      <td>91.50000</td>\n",
              "      <td>91.57998</td>\n",
              "      <td>1550.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.581127</td>\n",
              "      <td>98.958873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>91.60000</td>\n",
              "      <td>91.60000</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-05-30</td>\n",
              "      <td>gld</td>\n",
              "      <td>64.72000</td>\n",
              "      <td>64.86000</td>\n",
              "      <td>64.60001</td>\n",
              "      <td>64.86000</td>\n",
              "      <td>2744800.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.581127</td>\n",
              "      <td>98.958873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>64.72000</td>\n",
              "      <td>64.72000</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-05-30</td>\n",
              "      <td>spy</td>\n",
              "      <td>153.48000</td>\n",
              "      <td>153.53999</td>\n",
              "      <td>151.34000</td>\n",
              "      <td>151.46001</td>\n",
              "      <td>131273376.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.581127</td>\n",
              "      <td>98.958873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>153.48000</td>\n",
              "      <td>153.48000</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-05-30</td>\n",
              "      <td>vb</td>\n",
              "      <td>75.28999</td>\n",
              "      <td>75.32651</td>\n",
              "      <td>74.23000</td>\n",
              "      <td>74.36000</td>\n",
              "      <td>35300.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.581127</td>\n",
              "      <td>98.958873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-66.666667</td>\n",
              "      <td>100.0</td>\n",
              "      <td>75.28999</td>\n",
              "      <td>75.28999</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  tic      close       high        low       open       volume  \\\n",
              "                                                                              \n",
              "0  2007-05-30  agg   99.38000   99.47000   99.27000   99.45000     977600.0   \n",
              "0  2007-05-30  bil   91.60000   91.60000   91.50000   91.57998       1550.0   \n",
              "0  2007-05-30  gld   64.72000   64.86000   64.60001   64.86000    2744800.0   \n",
              "0  2007-05-30  spy  153.48000  153.53999  151.34000  151.46001  131273376.0   \n",
              "0  2007-05-30   vb   75.28999   75.32651   74.23000   74.36000      35300.0   \n",
              "\n",
              "   day  macd    boll_ub    boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
              "                                                                             \n",
              "0  2.0   0.0  99.581127  98.958873     0.0 -66.666667  100.0      99.38000   \n",
              "0  2.0   0.0  99.581127  98.958873     0.0 -66.666667  100.0      91.60000   \n",
              "0  2.0   0.0  99.581127  98.958873     0.0 -66.666667  100.0      64.72000   \n",
              "0  2.0   0.0  99.581127  98.958873     0.0 -66.666667  100.0     153.48000   \n",
              "0  2.0   0.0  99.581127  98.958873     0.0 -66.666667  100.0      75.28999   \n",
              "\n",
              "   close_60_sma    vix  turbulence  \n",
              "                                    \n",
              "0      99.38000  12.83         0.0  \n",
              "0      91.60000  12.83         0.0  \n",
              "0      64.72000  12.83         0.0  \n",
              "0     153.48000  12.83         0.0  \n",
              "0      75.28999  12.83         0.0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure\n",
        "# it has the columns and index in the form that could be make into the environment.\n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "d8b5b0b5-03da-45a9-a415-380e0a33dac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 8, State Space: 81\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "e3bcb35e-cb31-4034-d3fc-cb6bf781cf06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "923b3824-cfb9-4281-e339-b80f0470d1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "6de2db0c-7f7a-4016-a307-4110811f2125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 597         |\n",
            "|    iterations         | 100         |\n",
            "|    time_elapsed       | 0           |\n",
            "|    total_timesteps    | 500         |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.4       |\n",
            "|    explained_variance | 0.109       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 99          |\n",
            "|    policy_loss        | -19.2       |\n",
            "|    reward             | -0.04413799 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.05        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 603        |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 1          |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | 0.0916     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | 6.25       |\n",
            "|    reward             | 0.21341272 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.588      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 600        |\n",
            "|    iterations         | 300        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 1500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.4      |\n",
            "|    explained_variance | -1.05      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 299        |\n",
            "|    policy_loss        | 2.39       |\n",
            "|    reward             | 0.10893681 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 0.109      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 597       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.137    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -4.9      |\n",
            "|    reward             | 0.3333478 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.363     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 603        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 4          |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.5      |\n",
            "|    explained_variance | -1.98      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | -2.25      |\n",
            "|    reward             | -0.7269862 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.0922     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 609       |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 4         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.244    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 2.37      |\n",
            "|    reward             | 0.1540909 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.072     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 613       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.5     |\n",
            "|    explained_variance | -0.0885   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -4.29     |\n",
            "|    reward             | 1.8886164 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.65      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 617         |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.6       |\n",
            "|    explained_variance | -0.623      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | 0.438       |\n",
            "|    reward             | 0.015305154 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.0691      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 620        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.6      |\n",
            "|    explained_variance | -29.4      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | -0.617     |\n",
            "|    reward             | 0.10633217 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.00636    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 623          |\n",
            "|    iterations         | 1000         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 5000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.7        |\n",
            "|    explained_variance | -0.924       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 999          |\n",
            "|    policy_loss        | 0.484        |\n",
            "|    reward             | -0.074253894 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 0.00362      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 624        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.7      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -1.34      |\n",
            "|    reward             | 0.19705838 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 0.0296     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 625          |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 9            |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.8        |\n",
            "|    explained_variance | -0.552       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | -1.95        |\n",
            "|    reward             | -0.013498796 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.0395       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 627         |\n",
            "|    iterations         | 1300        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 6500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.8       |\n",
            "|    explained_variance | -0.188      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1299        |\n",
            "|    policy_loss        | -13.8       |\n",
            "|    reward             | -0.93575084 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 3.84        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 628        |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | -0.585     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 3.96       |\n",
            "|    reward             | 0.08715285 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.336      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 629          |\n",
            "|    iterations         | 1500         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 7500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.8        |\n",
            "|    explained_variance | 0.0311       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1499         |\n",
            "|    policy_loss        | 13.4         |\n",
            "|    reward             | -0.018911943 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 2.03         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 630        |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.0863    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | 12.4       |\n",
            "|    reward             | 0.47756907 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 1.7        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 631         |\n",
            "|    iterations         | 1700        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 8500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1699        |\n",
            "|    policy_loss        | -1.76       |\n",
            "|    reward             | -0.15589386 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 1.85        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 632        |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0.185      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | 5.56       |\n",
            "|    reward             | -0.5679795 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.767      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 633       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.11     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 9.19      |\n",
            "|    reward             | 1.0636901 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.56      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 631       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.136    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 16.8      |\n",
            "|    reward             | 1.9987913 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.75      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 632        |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | 15.8       |\n",
            "|    reward             | -1.1498967 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 3.29       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 633      |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 18.9     |\n",
            "|    reward             | 0.611345 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 633       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.8     |\n",
            "|    explained_variance | -4.74     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -14.8     |\n",
            "|    reward             | -0.414989 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 2.37      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 634      |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 4.9      |\n",
            "|    reward             | 0.299442 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 0.323    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 634        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | 14.4       |\n",
            "|    reward             | -0.3100678 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 2.23       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 634        |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | 0.446      |\n",
            "|    reward             | -2.1346023 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.5        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 635       |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.004    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -19.8     |\n",
            "|    reward             | -1.429257 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.38      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 635         |\n",
            "|    iterations         | 2800        |\n",
            "|    time_elapsed       | 22          |\n",
            "|    total_timesteps    | 14000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | -0.178      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2799        |\n",
            "|    policy_loss        | 0.713       |\n",
            "|    reward             | -0.12782127 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.0286      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 636        |\n",
            "|    iterations         | 2900       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 14500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -3.34e-06  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2899       |\n",
            "|    policy_loss        | 12         |\n",
            "|    reward             | 0.49829406 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.54       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 636        |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | 8.99       |\n",
            "|    reward             | 0.78787416 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.778      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 637        |\n",
            "|    iterations         | 3100       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 15500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.00141   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3099       |\n",
            "|    policy_loss        | 0.223      |\n",
            "|    reward             | 0.18265161 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.62       |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 637          |\n",
            "|    iterations         | 3200         |\n",
            "|    time_elapsed       | 25           |\n",
            "|    total_timesteps    | 16000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.9        |\n",
            "|    explained_variance | -0.0578      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3199         |\n",
            "|    policy_loss        | 17.4         |\n",
            "|    reward             | -0.030911874 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 2.19         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 637         |\n",
            "|    iterations         | 3300        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 16500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3299        |\n",
            "|    policy_loss        | -5.12       |\n",
            "|    reward             | -0.29994732 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.649       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 638        |\n",
            "|    iterations         | 3400       |\n",
            "|    time_elapsed       | 26         |\n",
            "|    total_timesteps    | 17000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.000205  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3399       |\n",
            "|    policy_loss        | -29        |\n",
            "|    reward             | -1.4105929 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 7.78       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 638       |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 14.9      |\n",
            "|    reward             | 0.700085  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.04      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 638        |\n",
            "|    iterations         | 3600       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 18000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.8      |\n",
            "|    explained_variance | 0.011      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3599       |\n",
            "|    policy_loss        | -49.7      |\n",
            "|    reward             | -0.8348624 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 20.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 638        |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 8.82e-06   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | -2.33      |\n",
            "|    reward             | -1.4021119 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.266      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 638        |\n",
            "|    iterations         | 3800       |\n",
            "|    time_elapsed       | 29         |\n",
            "|    total_timesteps    | 19000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3799       |\n",
            "|    policy_loss        | -20.1      |\n",
            "|    reward             | -2.4661257 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 3.62       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 638       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 6.78      |\n",
            "|    reward             | 1.4192259 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.735     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 638        |\n",
            "|    iterations         | 4000       |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 20000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -11.9      |\n",
            "|    explained_variance | -0.000684  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3999       |\n",
            "|    policy_loss        | -15.5      |\n",
            "|    reward             | -1.1276855 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.52       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 638       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 10.8      |\n",
            "|    reward             | 2.3587534 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.33      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 639      |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | -0.0744  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 14.5     |\n",
            "|    reward             | 0.061324 |\n",
            "|    std                | 1.07     |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 639         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 33          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | -0.752      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | 0.688       |\n",
            "|    reward             | -0.10841335 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.432       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 639       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 34        |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | -5.42     |\n",
            "|    reward             | 0.93476   |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.51      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 639       |\n",
            "|    iterations         | 4500      |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 22500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | -0.00367  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4499      |\n",
            "|    policy_loss        | -5.05     |\n",
            "|    reward             | 0.4164106 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.1       |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 639          |\n",
            "|    iterations         | 4600         |\n",
            "|    time_elapsed       | 35           |\n",
            "|    total_timesteps    | 23000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -11.9        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4599         |\n",
            "|    policy_loss        | -0.0962      |\n",
            "|    reward             | -0.034505725 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 0.262        |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 639         |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 36          |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -2.46       |\n",
            "|    reward             | -0.31720662 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 1.16        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 639       |\n",
            "|    iterations         | 4800      |\n",
            "|    time_elapsed       | 37        |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | -14.2     |\n",
            "|    reward             | -0.092107 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 1.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 640       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 38        |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -11.9     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 5.12      |\n",
            "|    reward             | -0.254997 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.473     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 640      |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -11.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 2.04     |\n",
            "|    reward             | 0.657743 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 0.386    |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 640        |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 23.3       |\n",
            "|    reward             | -1.5171877 |\n",
            "|    std                | 1.08       |\n",
            "|    value_loss         | 5.9        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 640         |\n",
            "|    iterations         | 5200        |\n",
            "|    time_elapsed       | 40          |\n",
            "|    total_timesteps    | 26000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -11.9       |\n",
            "|    explained_variance | 0.262       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5199        |\n",
            "|    policy_loss        | 2.46        |\n",
            "|    reward             | 0.090356626 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 0.415       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 640      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | -0.0138  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 1.5      |\n",
            "|    reward             | 0.812275 |\n",
            "|    std                | 1.08     |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 640       |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 42        |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -3.61     |\n",
            "|    reward             | -0.385068 |\n",
            "|    std                | 1.08      |\n",
            "|    value_loss         | 0.808     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 640        |\n",
            "|    iterations         | 5500       |\n",
            "|    time_elapsed       | 42         |\n",
            "|    total_timesteps    | 27500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5499       |\n",
            "|    policy_loss        | 15.8       |\n",
            "|    reward             | 0.43841428 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 2.49       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 640         |\n",
            "|    iterations         | 5600        |\n",
            "|    time_elapsed       | 43          |\n",
            "|    total_timesteps    | 28000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12         |\n",
            "|    explained_variance | -0.922      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5599        |\n",
            "|    policy_loss        | 11.1        |\n",
            "|    reward             | -0.17872502 |\n",
            "|    std                | 1.09        |\n",
            "|    value_loss         | 1.14        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 640       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 44        |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12       |\n",
            "|    explained_variance | -0.106    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -18.1     |\n",
            "|    reward             | 1.0385067 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 4.11      |\n",
            "-------------------------------------\n",
            "day: 3169, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1697103.26\n",
            "total_reward: 697103.26\n",
            "total_cost: 25191.93\n",
            "total_trades: 20535\n",
            "Sharpe: 0.316\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 641      |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 45       |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12      |\n",
            "|    explained_variance | 0.049    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 21.9     |\n",
            "|    reward             | 3.768613 |\n",
            "|    std                | 1.09     |\n",
            "|    value_loss         | 4.54     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 46         |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | -0.193     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -3.25      |\n",
            "|    reward             | 0.16070975 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 0.148      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 6000        |\n",
            "|    time_elapsed       | 46          |\n",
            "|    total_timesteps    | 30000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5999        |\n",
            "|    policy_loss        | -6.85       |\n",
            "|    reward             | -0.43780953 |\n",
            "|    std                | 1.1         |\n",
            "|    value_loss         | 0.351       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 6100        |\n",
            "|    time_elapsed       | 47          |\n",
            "|    total_timesteps    | 30500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.1       |\n",
            "|    explained_variance | 0.00425     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6099        |\n",
            "|    policy_loss        | 12.8        |\n",
            "|    reward             | -0.28998318 |\n",
            "|    std                | 1.1         |\n",
            "|    value_loss         | 2.05        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 48        |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | -0.155    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | -5.31     |\n",
            "|    reward             | -2.403635 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 0.307     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 6300       |\n",
            "|    time_elapsed       | 49         |\n",
            "|    total_timesteps    | 31500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | -0.0244    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6299       |\n",
            "|    policy_loss        | 2.64       |\n",
            "|    reward             | 0.78913194 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 2.89       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 6400       |\n",
            "|    time_elapsed       | 49         |\n",
            "|    total_timesteps    | 32000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.1      |\n",
            "|    explained_variance | 0.000453   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6399       |\n",
            "|    policy_loss        | 13.1       |\n",
            "|    reward             | 0.70424014 |\n",
            "|    std                | 1.1        |\n",
            "|    value_loss         | 2.71       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 50        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.1     |\n",
            "|    explained_variance | 0.242     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 19.4      |\n",
            "|    reward             | 1.5945841 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 3.63      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 640         |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 51          |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | 4.85        |\n",
            "|    reward             | 0.099546604 |\n",
            "|    std                | 1.1         |\n",
            "|    value_loss         | 0.203       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 640         |\n",
            "|    iterations         | 6700        |\n",
            "|    time_elapsed       | 52          |\n",
            "|    total_timesteps    | 33500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6699        |\n",
            "|    policy_loss        | -5.04       |\n",
            "|    reward             | 0.026458863 |\n",
            "|    std                | 1.1         |\n",
            "|    value_loss         | 0.34        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 6800       |\n",
            "|    time_elapsed       | 53         |\n",
            "|    total_timesteps    | 34000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6799       |\n",
            "|    policy_loss        | 26.1       |\n",
            "|    reward             | 0.27765182 |\n",
            "|    std                | 1.11       |\n",
            "|    value_loss         | 6.87       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 641      |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 34500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | 15.1     |\n",
            "|    reward             | 1.183599 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 54         |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.2      |\n",
            "|    explained_variance | 2.11e-05   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | -21.8      |\n",
            "|    reward             | -0.2407073 |\n",
            "|    std                | 1.11       |\n",
            "|    value_loss         | 3.56       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 55        |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 1.61      |\n",
            "|    reward             | 0.6406286 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.465     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 7200        |\n",
            "|    time_elapsed       | 56          |\n",
            "|    total_timesteps    | 36000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.2       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7199        |\n",
            "|    policy_loss        | 5.61        |\n",
            "|    reward             | -0.10561162 |\n",
            "|    std                | 1.11        |\n",
            "|    value_loss         | 3.08        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 56        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | -5.65     |\n",
            "|    reward             | 0.8161874 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 1.45      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 57        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 2.34      |\n",
            "|    reward             | -0.835097 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 0.631     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 7500      |\n",
            "|    time_elapsed       | 58        |\n",
            "|    total_timesteps    | 37500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7499      |\n",
            "|    policy_loss        | -14.8     |\n",
            "|    reward             | 0.8311581 |\n",
            "|    std                | 1.11      |\n",
            "|    value_loss         | 1.89      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 641      |\n",
            "|    iterations         | 7600     |\n",
            "|    time_elapsed       | 59       |\n",
            "|    total_timesteps    | 38000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.2    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 9.74     |\n",
            "|    reward             | 2.193586 |\n",
            "|    std                | 1.11     |\n",
            "|    value_loss         | 0.729    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 59        |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 13.6      |\n",
            "|    reward             | 1.6054145 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 2.63      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 640       |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 60        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | -16.8     |\n",
            "|    reward             | 1.7059451 |\n",
            "|    std                | 1.12      |\n",
            "|    value_loss         | 2.39      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 7900       |\n",
            "|    time_elapsed       | 61         |\n",
            "|    total_timesteps    | 39500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 0.324      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7899       |\n",
            "|    policy_loss        | 13.1       |\n",
            "|    reward             | -0.5395523 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 1.19       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 62        |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | -9.63     |\n",
            "|    reward             | 1.9280512 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.942     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 63        |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.3     |\n",
            "|    explained_variance | 1.79e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -12.1     |\n",
            "|    reward             | -0.754896 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.962     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 8200       |\n",
            "|    time_elapsed       | 63         |\n",
            "|    total_timesteps    | 41000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8199       |\n",
            "|    policy_loss        | 9.58       |\n",
            "|    reward             | -1.0811086 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 0.691      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 64        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0.00789   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 4.42      |\n",
            "|    reward             | 0.3367231 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 1.06      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 8400       |\n",
            "|    time_elapsed       | 65         |\n",
            "|    total_timesteps    | 42000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.3      |\n",
            "|    explained_variance | 0.007      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8399       |\n",
            "|    policy_loss        | 17.6       |\n",
            "|    reward             | 0.42831323 |\n",
            "|    std                | 1.13       |\n",
            "|    value_loss         | 4.04       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 8500        |\n",
            "|    time_elapsed       | 66          |\n",
            "|    total_timesteps    | 42500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8499        |\n",
            "|    policy_loss        | 36.6        |\n",
            "|    reward             | -0.11152061 |\n",
            "|    std                | 1.13        |\n",
            "|    value_loss         | 11.7        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 641      |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 67       |\n",
            "|    total_timesteps    | 43000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | 16.5     |\n",
            "|    reward             | 0.458221 |\n",
            "|    std                | 1.13     |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 641        |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 67         |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | -0.0104    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | -42.9      |\n",
            "|    reward             | 0.35352355 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 15.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 68        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0.304     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 6.78      |\n",
            "|    reward             | -2.158807 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 0.323     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 8900        |\n",
            "|    time_elapsed       | 69          |\n",
            "|    total_timesteps    | 44500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | 0.00649     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8899        |\n",
            "|    policy_loss        | -36.5       |\n",
            "|    reward             | -0.19709027 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 9.67        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 70        |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | -0.0119   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | -6.83     |\n",
            "|    reward             | 0.5273084 |\n",
            "|    std                | 1.13      |\n",
            "|    value_loss         | 0.955     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 641         |\n",
            "|    iterations         | 9100        |\n",
            "|    time_elapsed       | 70          |\n",
            "|    total_timesteps    | 45500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | -0.00201    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9099        |\n",
            "|    policy_loss        | 37          |\n",
            "|    reward             | -0.20265701 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 12.3        |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 641          |\n",
            "|    iterations         | 9200         |\n",
            "|    time_elapsed       | 71           |\n",
            "|    total_timesteps    | 46000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -12.4        |\n",
            "|    explained_variance | -0.281       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9199         |\n",
            "|    policy_loss        | 4.83         |\n",
            "|    reward             | -0.070487194 |\n",
            "|    std                | 1.14         |\n",
            "|    value_loss         | 0.234        |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 641       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 72        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0.027     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -21.5     |\n",
            "|    reward             | 2.6811008 |\n",
            "|    std                | 1.14      |\n",
            "|    value_loss         | 3.08      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 642         |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 73          |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -12.4       |\n",
            "|    explained_variance | -0.336      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | -1.98       |\n",
            "|    reward             | -0.85492635 |\n",
            "|    std                | 1.14        |\n",
            "|    value_loss         | 0.0243      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 642        |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 73         |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | -0.0122    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | 16.2       |\n",
            "|    reward             | -0.1978247 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 2.56       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 642      |\n",
            "|    iterations         | 9600     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | -31.5    |\n",
            "|    reward             | 2.083876 |\n",
            "|    std                | 1.14     |\n",
            "|    value_loss         | 10       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 642        |\n",
            "|    iterations         | 9700       |\n",
            "|    time_elapsed       | 75         |\n",
            "|    total_timesteps    | 48500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -12.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9699       |\n",
            "|    policy_loss        | 8.37       |\n",
            "|    reward             | -1.2326444 |\n",
            "|    std                | 1.14       |\n",
            "|    value_loss         | 2.62       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 642       |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 76        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | -34.7     |\n",
            "|    reward             | 0.6152636 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 9.76      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 642       |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 77        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -12.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 7.99      |\n",
            "|    reward             | -0.167866 |\n",
            "|    std                | 1.15      |\n",
            "|    value_loss         | 1.39      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 642      |\n",
            "|    iterations         | 10000    |\n",
            "|    time_elapsed       | 77       |\n",
            "|    total_timesteps    | 50000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -12.5    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9999     |\n",
            "|    policy_loss        | 32.8     |\n",
            "|    reward             | 1.417681 |\n",
            "|    std                | 1.15     |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2YadjfnLwgt",
        "outputId": "8e60d952-db69-4a03-b6dc-3793f3fafc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCDa78rqfO_a",
        "outputId": "e4dd77e8-50ad-4bbf-fa1e-12a1069e3db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 3169, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1731073.03\n",
            "total_reward: 731073.03\n",
            "total_cost: 999.00\n",
            "total_trades: 22183\n",
            "Sharpe: 0.436\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 200       |\n",
            "|    time_elapsed    | 63        |\n",
            "|    total_timesteps | 12680     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -15.4     |\n",
            "|    critic_loss     | 9.44      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 12579     |\n",
            "|    reward          | -0.405961 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 187       |\n",
            "|    time_elapsed    | 134       |\n",
            "|    total_timesteps | 25360     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.19     |\n",
            "|    critic_loss     | 0.357     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 25259     |\n",
            "|    reward          | -0.405961 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 195       |\n",
            "|    time_elapsed    | 194       |\n",
            "|    total_timesteps | 38040     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.28     |\n",
            "|    critic_loss     | 0.737     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 37939     |\n",
            "|    reward          | -0.405961 |\n",
            "----------------------------------\n",
            "day: 3169, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1731073.03\n",
            "total_reward: 731073.03\n",
            "total_cost: 999.00\n",
            "total_trades: 22183\n",
            "Sharpe: 0.436\n",
            "=================================\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5D5PFUhMzSV",
        "outputId": "03c32fbf-d81f-4a41-d3c9-22442e8d5410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt8eIQKYM4G3",
        "outputId": "9a14a35c-fa3e-40ec-8187-608d7027d027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "| time/              |            |\n",
            "|    fps             | 744        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 2          |\n",
            "|    total_timesteps | 2048       |\n",
            "| train/             |            |\n",
            "|    reward          | -0.2536773 |\n",
            "-----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006217262 |\n",
            "|    clip_fraction        | 0.0559      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0864     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.647       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00557    |\n",
            "|    reward               | 0.31409413  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 1.74        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 710         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005749058 |\n",
            "|    clip_fraction        | 0.0625      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.0161      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.901       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00663    |\n",
            "|    reward               | -2.7898514  |\n",
            "|    std                  | 0.999       |\n",
            "|    value_loss           | 2.03        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009380408 |\n",
            "|    clip_fraction        | 0.0861      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | -0.0101     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.71        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00872    |\n",
            "|    reward               | 0.027527245 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.7         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 706          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061313435 |\n",
            "|    clip_fraction        | 0.0486       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.000628     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.93         |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00663     |\n",
            "|    reward               | 0.363671     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.18         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 707         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007271382 |\n",
            "|    clip_fraction        | 0.0954      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.3       |\n",
            "|    explained_variance   | 0.0624      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.69        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0119     |\n",
            "|    reward               | 0.043958694 |\n",
            "|    std                  | 0.998       |\n",
            "|    value_loss           | 5.88        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 705        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 20         |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00812517 |\n",
            "|    clip_fraction        | 0.0852     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.3      |\n",
            "|    explained_variance   | -0.0433    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 2.23       |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.00799   |\n",
            "|    reward               | -0.5231202 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 4.24       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008342665 |\n",
            "|    clip_fraction        | 0.0804      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0723      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.688       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00831    |\n",
            "|    reward               | 0.7166164   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 2.29        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008473547 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0998      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.9         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    reward               | -0.18019277 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 3.54        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 697         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005825094 |\n",
            "|    clip_fraction        | 0.0524      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0151      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.94        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00692    |\n",
            "|    reward               | -0.4534466  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 7.96        |\n",
            "-----------------------------------------\n",
            "day: 3169, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2214728.08\n",
            "total_reward: 1214728.08\n",
            "total_cost: 135055.37\n",
            "total_trades: 23463\n",
            "Sharpe: 0.468\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 698         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007647124 |\n",
            "|    clip_fraction        | 0.05        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0195      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.13        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00591    |\n",
            "|    reward               | 0.8213234   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 11.2        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 676          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075061293 |\n",
            "|    clip_fraction        | 0.0515       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0997       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.24         |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00594     |\n",
            "|    reward               | -0.17537704  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 675         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008010671 |\n",
            "|    clip_fraction        | 0.0864      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.0913      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.66        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0093     |\n",
            "|    reward               | -0.2642696  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 7.33        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 673         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007819599 |\n",
            "|    clip_fraction        | 0.0479      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.139       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.88        |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.00742    |\n",
            "|    reward               | 0.065698944 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 8.3         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 676         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006638443 |\n",
            "|    clip_fraction        | 0.0714      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.4       |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.97        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00889    |\n",
            "|    reward               | -0.60617334 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 8.54        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 675          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0079563055 |\n",
            "|    clip_fraction        | 0.0703       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.4        |\n",
            "|    explained_variance   | 0.0672       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.81         |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00578     |\n",
            "|    reward               | 3.4393263    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 8.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 675          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075072334 |\n",
            "|    clip_fraction        | 0.0474       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.5        |\n",
            "|    explained_variance   | 0.0653       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.96         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00616     |\n",
            "|    reward               | -0.5862188   |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 10.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 676         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 54          |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007379583 |\n",
            "|    clip_fraction        | 0.0494      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | -0.00521    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.75        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00708    |\n",
            "|    reward               | -2.2234206  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 677         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010041688 |\n",
            "|    clip_fraction        | 0.0835      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.16        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.12        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 0.08538172  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 8.05        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 679          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0080778245 |\n",
            "|    clip_fraction        | 0.0828       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.5        |\n",
            "|    explained_variance   | 0.0313       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.68         |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -0.008       |\n",
            "|    reward               | -3.2302468   |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 7.99         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 681         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 63          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011115153 |\n",
            "|    clip_fraction        | 0.0891      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0031      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.93        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.012      |\n",
            "|    reward               | -0.5002402  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 683         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007085174 |\n",
            "|    clip_fraction        | 0.063       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0665      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.74        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00863    |\n",
            "|    reward               | -0.57748276 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 11.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 685         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009528624 |\n",
            "|    clip_fraction        | 0.099       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0805      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.85        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -3.636944   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 13.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 686         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 71          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011591695 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | -0.00706    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.78        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | -2.1517217  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 9.52        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 687         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011644179 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0665      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.87        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    reward               | 1.2623717   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 686         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011297702 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0463      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.12        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00988    |\n",
            "|    reward               | 1.118365    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 11.6        |\n",
            "-----------------------------------------\n",
            "day: 3169, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2243050.68\n",
            "total_reward: 1243050.68\n",
            "total_cost: 121656.43\n",
            "total_trades: 23091\n",
            "Sharpe: 0.458\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 687          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008187864  |\n",
            "|    clip_fraction        | 0.0718       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.5        |\n",
            "|    explained_variance   | 0.0096       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.35         |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.0085      |\n",
            "|    reward               | -0.007822951 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 9.82         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 688         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012205209 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0426      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.9         |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00788    |\n",
            "|    reward               | -0.766396   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 13          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 690         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 86          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012676077 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0474      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.33        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.00644    |\n",
            "|    reward               | -0.55893415 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 690         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010174492 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | -0.0462     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.34        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.00784    |\n",
            "|    reward               | -0.06959754 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 8.68        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 691         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 91          |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010357427 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0427      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.17        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.00891    |\n",
            "|    reward               | -0.32036364 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 8.76        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 693         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011258966 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0345      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.04        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00828    |\n",
            "|    reward               | -0.17011216 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 11.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 693        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 97         |\n",
            "|    total_timesteps      | 67584      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01033139 |\n",
            "|    clip_fraction        | 0.0976     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.5      |\n",
            "|    explained_variance   | 0.0631     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.23       |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.00924   |\n",
            "|    reward               | 0.27316815 |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 11.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 694         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 100         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012315903 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.0453      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.07        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00997    |\n",
            "|    reward               | -0.7975031  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 15.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 695         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012496615 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.5       |\n",
            "|    explained_variance   | 0.000794    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.55        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.0095     |\n",
            "|    reward               | -0.13925044 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 695        |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 105        |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00954834 |\n",
            "|    clip_fraction        | 0.112      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.6      |\n",
            "|    explained_variance   | 0.0617     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.94       |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.00922   |\n",
            "|    reward               | -0.5001811 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 11         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 696         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011998712 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.0428      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.04        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00977    |\n",
            "|    reward               | 0.35230836  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 15          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 697         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 111         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011751289 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.00283     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.58        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00801    |\n",
            "|    reward               | 0.18420945  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 9.42        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 698         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 114         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017343009 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.0844      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.97        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.00691    |\n",
            "|    reward               | 0.7547932   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 12.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 698         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 117         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013266414 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.0993      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.22        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | 0.41654122  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 699         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 120         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014247449 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | -0.00635    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.71        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.00966    |\n",
            "|    reward               | -0.27138188 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 8.62        |\n",
            "-----------------------------------------\n",
            "day: 3169, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2142596.71\n",
            "total_reward: 1142596.71\n",
            "total_cost: 122701.54\n",
            "total_trades: 23099\n",
            "Sharpe: 0.405\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 122         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014904508 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.0779      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.14        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00806    |\n",
            "|    reward               | 0.7881512   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 13.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 125         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010736515 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.0967      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.39        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00982    |\n",
            "|    reward               | -0.19199014 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 13.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 701         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 128         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015058789 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.6       |\n",
            "|    explained_variance   | 0.00261     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.66        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00751    |\n",
            "|    reward               | -0.3580102  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 8.65        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 702         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 131         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013383838 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.15        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6           |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00919    |\n",
            "|    reward               | 0.27863768  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 14.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 702         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 134         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012614237 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.144       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.94        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00893    |\n",
            "|    reward               | 0.26547664  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 13.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 703         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012787366 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.0674      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.28        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0072     |\n",
            "|    reward               | -1.5199955  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 11.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 703         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 139         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011658881 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.146       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.71        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00775    |\n",
            "|    reward               | -0.03871885 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 13          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 703         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 142         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011695133 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.0348      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.8         |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.00588    |\n",
            "|    reward               | 0.56278044  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 704         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 145         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011284215 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.194       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.51        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.0083     |\n",
            "|    reward               | 2.0039546   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 704         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 148         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012646489 |\n",
            "|    clip_fraction        | 0.0793      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.169       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.78        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.00476    |\n",
            "|    reward               | 0.67529416  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 704         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 151         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011903504 |\n",
            "|    clip_fraction        | 0.099       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | -0.00151    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.59        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0057     |\n",
            "|    reward               | 0.7003737   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 153         |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012508471 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.153       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.32        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.00753    |\n",
            "|    reward               | 2.640318    |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 10.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 156         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013636721 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.7       |\n",
            "|    explained_variance   | 0.104       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.74        |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.00904    |\n",
            "|    reward               | -1.3549688  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 14.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 706        |\n",
            "|    iterations           | 55         |\n",
            "|    time_elapsed         | 159        |\n",
            "|    total_timesteps      | 112640     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01446415 |\n",
            "|    clip_fraction        | 0.175      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -11.7      |\n",
            "|    explained_variance   | -0.00574   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.12       |\n",
            "|    n_updates            | 540        |\n",
            "|    policy_gradient_loss | -0.00149   |\n",
            "|    reward               | 1.1086438  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 8.24       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 162         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.01113668  |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.2         |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.94        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.00964    |\n",
            "|    reward               | 0.054999013 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 13.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 165         |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010950673 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.17        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.00811    |\n",
            "|    reward               | -0.08089884 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "day: 3169, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2227815.74\n",
            "total_reward: 1227815.74\n",
            "total_cost: 99641.84\n",
            "total_trades: 21475\n",
            "Sharpe: 0.417\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 706          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 168          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0120302215 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.8        |\n",
            "|    explained_variance   | -0.0134      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.6          |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | -0.00902     |\n",
            "|    reward               | 0.59003395   |\n",
            "|    std                  | 1.06         |\n",
            "|    value_loss           | 8.75         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 171         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014812725 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.18        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.15        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.0122     |\n",
            "|    reward               | -1.0868056  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 15.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 174         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010015261 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.119       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.72        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.00839    |\n",
            "|    reward               | 0.40636617  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 15.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 705         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 177         |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011499606 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | -0.0221     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.34        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00744    |\n",
            "|    reward               | -1.1128014  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 8.65        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 179         |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012998916 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.104       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.81        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00912    |\n",
            "|    reward               | -0.10289445 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 13.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 182         |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013205121 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.183       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.23        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00682    |\n",
            "|    reward               | 0.3301799   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 185         |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010841729 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.8       |\n",
            "|    explained_variance   | 0.0335      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.65        |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.00609    |\n",
            "|    reward               | 0.16369635  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 13          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 706         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 188         |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016870238 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0928      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.34        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.00366    |\n",
            "|    reward               | -2.0420249  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 707         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 191         |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014839806 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | -0.00871    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.05        |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00349    |\n",
            "|    reward               | 1.6880689   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 707         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 193         |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013083514 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0851      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.63        |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | -0.31136686 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 707         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 196         |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013927806 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0558      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.45        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.00712    |\n",
            "|    reward               | 1.2675935   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 708         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 199         |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011996632 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | -0.0503     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.96        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00541    |\n",
            "|    reward               | -0.28995958 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 708         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 202         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016590409 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0633      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.01        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00685    |\n",
            "|    reward               | -0.6047372  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 708         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 205         |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013747966 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.109       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.47        |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | -0.00517    |\n",
            "|    reward               | -0.14890112 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 708          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 208          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.018611792  |\n",
            "|    clip_fraction        | 0.178        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -11.9        |\n",
            "|    explained_variance   | -0.00208     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.31         |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -0.00973     |\n",
            "|    reward               | -0.018898658 |\n",
            "|    std                  | 1.07         |\n",
            "|    value_loss           | 11.3         |\n",
            "------------------------------------------\n",
            "day: 3169, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2460058.18\n",
            "total_reward: 1460058.18\n",
            "total_cost: 77333.69\n",
            "total_trades: 20218\n",
            "Sharpe: 0.469\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 708         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 210         |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012027144 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.185       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.31        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00962    |\n",
            "|    reward               | -0.13151442 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 213         |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012399534 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.252       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.61        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.0056     |\n",
            "|    reward               | 0.50741667  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 14.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 216         |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011430108 |\n",
            "|    clip_fraction        | 0.0913      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | -0.0261     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.94        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00786    |\n",
            "|    reward               | -0.21036062 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 12.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 219         |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015143054 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.237       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.54        |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    reward               | -0.72002405 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 16          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 222         |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010545484 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.154       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.94        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00896    |\n",
            "|    reward               | 0.7408836   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 17.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 225         |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013766059 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0235      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.53        |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | -0.00606    |\n",
            "|    reward               | -1.7934365  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 709         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 227         |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010868885 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.198       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.57        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.006      |\n",
            "|    reward               | 0.10619336  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 14          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 710         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 230         |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012515441 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.181       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.86        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.0049     |\n",
            "|    reward               | -0.69461    |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 10.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 710         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 233         |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010754079 |\n",
            "|    clip_fraction        | 0.0821      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.0492      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.53        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.000891   |\n",
            "|    reward               | -0.5681     |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 710         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 236         |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011981452 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.317       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.64        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00972    |\n",
            "|    reward               | 0.10514128  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 12.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 710         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 239         |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010194149 |\n",
            "|    clip_fraction        | 0.0823      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.00906     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.88        |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.00783    |\n",
            "|    reward               | -4.939      |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 10.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 241         |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008439796 |\n",
            "|    clip_fraction        | 0.0726      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.288       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.9         |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.00614    |\n",
            "|    reward               | -0.47554788 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 9.55        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 244         |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011773864 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.24        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.53        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | -0.00429    |\n",
            "|    reward               | 2.168142    |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 13.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 247         |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010428901 |\n",
            "|    clip_fraction        | 0.0866      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | -0.0222     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.85        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00261    |\n",
            "|    reward               | 0.6016559   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 7.55        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 250         |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011747974 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -11.9       |\n",
            "|    explained_variance   | 0.384       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.65        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    reward               | 0.34057066  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 9.98        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 253         |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013114935 |\n",
            "|    clip_fraction        | 0.0848      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.307       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.76        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00363    |\n",
            "|    reward               | 2.597948    |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 14.1        |\n",
            "-----------------------------------------\n",
            "day: 3169, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2024657.27\n",
            "total_reward: 1024657.27\n",
            "total_cost: 82722.13\n",
            "total_trades: 20736\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 255         |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011336286 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.00457     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3           |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.00492    |\n",
            "|    reward               | 1.1379647   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 7.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 258         |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011171194 |\n",
            "|    clip_fraction        | 0.0991      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.339       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.97        |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | -0.00821    |\n",
            "|    reward               | 1.7396553   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 12.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 261         |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011053221 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.314       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.57        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.00353    |\n",
            "|    reward               | 1.4512792   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 13.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 264         |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011786323 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.0203      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.19        |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    reward               | 0.71540624  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 9           |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 267         |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009208353 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.373       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.93        |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | -0.00629    |\n",
            "|    reward               | 0.6855092   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 12.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 270         |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010813574 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.36        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.65        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00397    |\n",
            "|    reward               | 0.76199615  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 273         |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012569462 |\n",
            "|    clip_fraction        | 0.0987      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.256       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.27        |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    reward               | 1.2962228   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 10.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 276         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012194356 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.38        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.02        |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.00715    |\n",
            "|    reward               | -0.6488714  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 12          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 712        |\n",
            "|    iterations           | 97         |\n",
            "|    time_elapsed         | 278        |\n",
            "|    total_timesteps      | 198656     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00907894 |\n",
            "|    clip_fraction        | 0.0925     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -12        |\n",
            "|    explained_variance   | 0.212      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 5.08       |\n",
            "|    n_updates            | 960        |\n",
            "|    policy_gradient_loss | -0.00626   |\n",
            "|    reward               | 1.8408357  |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 9.69       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 712         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 281         |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010746704 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -12         |\n",
            "|    explained_variance   | 0.581       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.9         |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | -0.00483    |\n",
            "|    reward               | 0.76831776  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 10.8        |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JSAHhV4Xc-bh",
        "outputId": "8bb0cd39-8a2d-4e7e-9db0-324aa77cbf09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 3169, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1000000.00\n",
            "total_reward: 0.00\n",
            "total_cost: 0.00\n",
            "total_trades: 0\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 220      |\n",
            "|    time_elapsed    | 57       |\n",
            "|    total_timesteps | 12680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.64e+04 |\n",
            "|    critic_loss     | 3.66e+04 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 12579    |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 220      |\n",
            "|    time_elapsed    | 115      |\n",
            "|    total_timesteps | 25360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.16e+04 |\n",
            "|    critic_loss     | 5.63e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 25259    |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 219      |\n",
            "|    time_elapsed    | 173      |\n",
            "|    total_timesteps | 38040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 8.41e+03 |\n",
            "|    critic_loss     | 1.53e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 37939    |\n",
            "|    reward          | 0.0      |\n",
            "---------------------------------\n",
            "day: 3169, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1000000.00\n",
            "total_reward: 0.00\n",
            "total_cost: 0.00\n",
            "total_trades: 0\n",
            "=================================\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 139       |\n",
            "|    time_elapsed    | 90        |\n",
            "|    total_timesteps | 12680     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 605       |\n",
            "|    critic_loss     | 56.7      |\n",
            "|    ent_coef        | 0.331     |\n",
            "|    ent_coef_loss   | 40.6      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 12579     |\n",
            "|    reward          | -1.311351 |\n",
            "----------------------------------\n",
            "day: 3169, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1689088.82\n",
            "total_reward: 689088.82\n",
            "total_cost: 134560.23\n",
            "total_trades: 21640\n",
            "Sharpe: 0.369\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 140      |\n",
            "|    time_elapsed    | 180      |\n",
            "|    total_timesteps | 25360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.03e+03 |\n",
            "|    critic_loss     | 456      |\n",
            "|    ent_coef        | 0.622    |\n",
            "|    ent_coef_loss   | -5.46    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 25259    |\n",
            "|    reward          | 0.269024 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 145      |\n",
            "|    time_elapsed    | 261      |\n",
            "|    total_timesteps | 38040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 493      |\n",
            "|    critic_loss     | 202      |\n",
            "|    ent_coef        | 0.175    |\n",
            "|    ent_coef_loss   | -15.4    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 37939    |\n",
            "|    reward          | 0.268324 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 148      |\n",
            "|    time_elapsed    | 341      |\n",
            "|    total_timesteps | 50720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 284      |\n",
            "|    critic_loss     | 28.1     |\n",
            "|    ent_coef        | 0.0515   |\n",
            "|    ent_coef_loss   | -17.4    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 50619    |\n",
            "|    reward          | 0.14265  |\n",
            "---------------------------------\n",
            "day: 3169, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1481860.49\n",
            "total_reward: 481860.49\n",
            "total_cost: 2302.75\n",
            "total_trades: 21003\n",
            "Sharpe: 0.380\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 150       |\n",
            "|    time_elapsed    | 420       |\n",
            "|    total_timesteps | 63400     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 138       |\n",
            "|    critic_loss     | 16.2      |\n",
            "|    ent_coef        | 0.0158    |\n",
            "|    ent_coef_loss   | -10.5     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 63299     |\n",
            "|    reward          | -0.045167 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
